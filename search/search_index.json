{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Levridge Overview Levridge is dedicated to building modern, user-friendly business solutions for the agricultural industry. The Levridge solution enables Ag Retailers, Ag Cooperatives and Commodity Processors to run businesses in a modern, connected way. Agronomy This cloud-based solution infinitely increases efficiencies in agronomy through the elimination of paper and the need for manual double entries, better visibility into inventory and procurement and the opportunity for real-time invoicing. Using the many functions and features of customer relationship management Levridge has multiple tools to help the agronomist overcome challenges with tracking data related to the grower, employee and contractor. This includes providing access to vital historical information, addressing knowledge sharing amongst employees, application and chemical certifications, and licensing and DOT tracking. Vital features for the agronomist include Outlook integration, email tracking, visibility to sales activities and follow up. Most importantly, it provides a 360-degree view of the client in a way that is accessible to all team members who need it. Agronomy Overview Agronomist End-to-End Scenarios Agronomy Implementation Activities Grain To be documented Accounting and Retail Split billing is a unique and necessary challenge for the ag industry. Levridge includes billing split management and the ability to clearly define splits by collecting data and using grower confirmation to maintain accuracy. The ag retail functionality includes the ability for customers and vendors to prepay and the opportunity to track and make adjustments based on consumption and usage. Levridge includes a modern and integrated approach to reaching low net pricing by seamlessly handling grower rebates and pricing. By taking advantage of the commissions, rebates and trade agreement set up functions that are native to the Dynamics 365 solution. Agronomy Retail Feed Feed Overview to be documented Commodity Processing No more dealing with inflexible reporting. A modern cloud-based platform with integration to the Microsoft Office suite and the ability to work with multiple other software applications gives Levridge an enhanced commodity accounting experience. With a built-in connection to the back office and your accounting team, access to information is seamless. Scale With the fastest-speed on the market, the Levridge scale is built with real-time synchronization to see ticket information almost immediately. The Levridge scale can connect to any ERP system within minimal hardware and only one system to maintain and service. The Levridge scale is NTEP certified. Scale Overview Equity and Patronage Due to the many factors involved and difficulty tracking revolving member equity and stock allocations, this has been traditionally handled outside of the central accounting application. It is very likely stored in Excel or some other system that integrates with your accounting or agribusiness solution. In Levridge equity and stock allocations are managed within ONE solution. Equity tracking, from build-up, to current balance to pay-out is handled within the Levridge solution. Patronage Overview","title":"Home"},{"location":"#levridge","text":"","title":"Levridge"},{"location":"#overview","text":"Levridge is dedicated to building modern, user-friendly business solutions for the agricultural industry. The Levridge solution enables Ag Retailers, Ag Cooperatives and Commodity Processors to run businesses in a modern, connected way.","title":"Overview"},{"location":"#agronomy","text":"This cloud-based solution infinitely increases efficiencies in agronomy through the elimination of paper and the need for manual double entries, better visibility into inventory and procurement and the opportunity for real-time invoicing. Using the many functions and features of customer relationship management Levridge has multiple tools to help the agronomist overcome challenges with tracking data related to the grower, employee and contractor. This includes providing access to vital historical information, addressing knowledge sharing amongst employees, application and chemical certifications, and licensing and DOT tracking. Vital features for the agronomist include Outlook integration, email tracking, visibility to sales activities and follow up. Most importantly, it provides a 360-degree view of the client in a way that is accessible to all team members who need it. Agronomy Overview Agronomist End-to-End Scenarios Agronomy Implementation Activities","title":"Agronomy"},{"location":"#grain","text":"To be documented","title":"Grain"},{"location":"#accounting-and-retail","text":"Split billing is a unique and necessary challenge for the ag industry. Levridge includes billing split management and the ability to clearly define splits by collecting data and using grower confirmation to maintain accuracy. The ag retail functionality includes the ability for customers and vendors to prepay and the opportunity to track and make adjustments based on consumption and usage. Levridge includes a modern and integrated approach to reaching low net pricing by seamlessly handling grower rebates and pricing. By taking advantage of the commissions, rebates and trade agreement set up functions that are native to the Dynamics 365 solution. Agronomy Retail","title":"Accounting and Retail"},{"location":"#feed","text":"Feed Overview to be documented","title":"Feed"},{"location":"#commodity-processing","text":"No more dealing with inflexible reporting. A modern cloud-based platform with integration to the Microsoft Office suite and the ability to work with multiple other software applications gives Levridge an enhanced commodity accounting experience. With a built-in connection to the back office and your accounting team, access to information is seamless.","title":"Commodity Processing"},{"location":"#scale","text":"With the fastest-speed on the market, the Levridge scale is built with real-time synchronization to see ticket information almost immediately. The Levridge scale can connect to any ERP system within minimal hardware and only one system to maintain and service. The Levridge scale is NTEP certified. Scale Overview","title":"Scale"},{"location":"#equity-and-patronage","text":"Due to the many factors involved and difficulty tracking revolving member equity and stock allocations, this has been traditionally handled outside of the central accounting application. It is very likely stored in Excel or some other system that integrates with your accounting or agribusiness solution. In Levridge equity and stock allocations are managed within ONE solution. Equity tracking, from build-up, to current balance to pay-out is handled within the Levridge solution. Patronage Overview","title":"Equity and Patronage"},{"location":"AddAddressForDeliveryAndInvoice/","text":"Add Address for Delivery and Invoice Brief introduction of the module, component or feature being documented. This document explains ... Add Address for Delivery and Invoice Go to Accounts receivable > Customers > All customers. In the list, find and select the desired record. In the list, click the link in the selected row. In the list, click the link in the selected row. In the list, click the link in the selected row. In the list, click the link in the selected row. Click Add. In the Name or description field, type a value. In the Purpose field, enter or select a value. In the list, select row 3. In the list, click the link in the selected row. In the list, select row 7. Click Select. In the Zip/Postal code field, type a value. In the Street field, type a value. Click OK.","title":"Add Address for Delivery and Invoice"},{"location":"AddAddressForDeliveryAndInvoice/#add-address-for-delivery-and-invoice","text":"Brief introduction of the module, component or feature being documented. This document explains ...","title":"Add Address for Delivery and Invoice"},{"location":"AddAddressForDeliveryAndInvoice/#add-address-for-delivery-and-invoice_1","text":"Go to Accounts receivable > Customers > All customers. In the list, find and select the desired record. In the list, click the link in the selected row. In the list, click the link in the selected row. In the list, click the link in the selected row. In the list, click the link in the selected row. Click Add. In the Name or description field, type a value. In the Purpose field, enter or select a value. In the list, select row 3. In the list, click the link in the selected row. In the list, select row 7. Click Select. In the Zip/Postal code field, type a value. In the Street field, type a value. Click OK.","title":"Add Address for Delivery and Invoice"},{"location":"AdjustFIFOInventoryValue/","text":"Adjust FIFO Inventory Value Brief introduction of the module, component or feature being documented. This document explains ... How to Adjust FIFO Inventory Value Close the page. Go to Inventory Management > Periodic tasks > Closing and Adjustment. Click Adjustment. Click On-hand. Click Select. Expand the Records to include section. Click Filter. In the list, find and select the desired record. In the list, find and select the desired record. In the Criteria field, type a value. Click OK. Click OK. In the list, mark the selected row. In the Unit cost field, enter a number. Click Post. Click OK. Click Details. Click Voucher. Close the page. Click Details. Click Adjustments report. Click Cancel. Click Details. Click Settlements. Click to follow the link in the Item number field. Click to follow the link in the Item number field. On the Action pane, click Manage Inventory. Click On-hand inventory. Refresh the page. Click Transactions. Close the page. Go to Inventory management > Inquiries and reports > On-hand list. Apply the following filters: Enter a filter value of \"\" on the \"Item number\" field using the \"begins with\" filter operator; Enter a filter value of \"\" on the \"Site\" field using the \"begins with\" filter operator; Enter a filter value of \"\" on the \"Warehouse\" field using the \"begins with\" filter operator; Enter a filter value of \"\" on the \"Serial number\" field using the \"begins with\" filter operator; Enter a filter value of \"\" on the \"Batch number\" field using the \"begins with\" filter operator; Enter a filter value of \"\" on the \"Search name\" field using the \"begins with\" filter operator Click Intercompany on-hand. Click the On-hand tab.","title":"Adjust FIFO Inventory Value"},{"location":"AdjustFIFOInventoryValue/#adjust-fifo-inventory-value","text":"Brief introduction of the module, component or feature being documented. This document explains ...","title":"Adjust FIFO Inventory Value"},{"location":"AdjustFIFOInventoryValue/#how-to-adjust-fifo-inventory-value","text":"Close the page. Go to Inventory Management > Periodic tasks > Closing and Adjustment. Click Adjustment. Click On-hand. Click Select. Expand the Records to include section. Click Filter. In the list, find and select the desired record. In the list, find and select the desired record. In the Criteria field, type a value. Click OK. Click OK. In the list, mark the selected row. In the Unit cost field, enter a number. Click Post. Click OK. Click Details. Click Voucher. Close the page. Click Details. Click Adjustments report. Click Cancel. Click Details. Click Settlements. Click to follow the link in the Item number field. Click to follow the link in the Item number field. On the Action pane, click Manage Inventory. Click On-hand inventory. Refresh the page. Click Transactions. Close the page. Go to Inventory management > Inquiries and reports > On-hand list. Apply the following filters: Enter a filter value of \"\" on the \"Item number\" field using the \"begins with\" filter operator; Enter a filter value of \"\" on the \"Site\" field using the \"begins with\" filter operator; Enter a filter value of \"\" on the \"Warehouse\" field using the \"begins with\" filter operator; Enter a filter value of \"\" on the \"Serial number\" field using the \"begins with\" filter operator; Enter a filter value of \"\" on the \"Batch number\" field using the \"begins with\" filter operator; Enter a filter value of \"\" on the \"Search name\" field using the \"begins with\" filter operator Click Intercompany on-hand. Click the On-hand tab.","title":"How to Adjust FIFO Inventory Value"},{"location":"AgSyncEndpoint/","text":"AgSyncEndpoint Settings AgSyncEndpoint is an object in the appsettings.json file used by the Levridge Integration Framework to define the configuration for the integration between FinOps and Agsync master data. Example \"AgSyncEndpoint\": { \"MustUseWktProcessor\": true, \"UuidSource\": \"Agsync\", \"BaseFieldUri\": \"https://fields.agsync.com/api/\", \"BaseOrderUri\": \"https://orders.agsync.com/api/\", \"tokenUrl\": \"https://auth.agsync.com/core/connect/token\", \"ClientId\": \"[Agsync assigned client ID]\", \"ClientPass\": \"[Agsync assigned client password]\", \"VaultURL\": \"https://levridgeagsynckeyvault.vault.azure.net/\", \"AgSyncTokenKey\": \"AgsyncAccessToken\", \"RedirectUri\": \"[Agsync Integration Base URL]/api/AgsyncAuth\", \"IntegrationId\": \"[Agsync assigned integration Id]\" } Definition UuidSource BaseFieldUri BaseOrderUri tokenUrl ClientId ClientPass VaultURL AgSyncTokenKey RedirectUri IntegrationId","title":"AgSyncEndpoint Settings"},{"location":"AgSyncEndpoint/#agsyncendpoint-settings","text":"AgSyncEndpoint is an object in the appsettings.json file used by the Levridge Integration Framework to define the configuration for the integration between FinOps and Agsync master data.","title":"AgSyncEndpoint Settings"},{"location":"AgSyncEndpoint/#example","text":"\"AgSyncEndpoint\": { \"MustUseWktProcessor\": true, \"UuidSource\": \"Agsync\", \"BaseFieldUri\": \"https://fields.agsync.com/api/\", \"BaseOrderUri\": \"https://orders.agsync.com/api/\", \"tokenUrl\": \"https://auth.agsync.com/core/connect/token\", \"ClientId\": \"[Agsync assigned client ID]\", \"ClientPass\": \"[Agsync assigned client password]\", \"VaultURL\": \"https://levridgeagsynckeyvault.vault.azure.net/\", \"AgSyncTokenKey\": \"AgsyncAccessToken\", \"RedirectUri\": \"[Agsync Integration Base URL]/api/AgsyncAuth\", \"IntegrationId\": \"[Agsync assigned integration Id]\" }","title":"Example"},{"location":"AgSyncEndpoint/#definition","text":"","title":"Definition"},{"location":"AgSyncEndpoint/#uuidsource","text":"","title":"UuidSource"},{"location":"AgSyncEndpoint/#basefielduri","text":"","title":"BaseFieldUri"},{"location":"AgSyncEndpoint/#baseorderuri","text":"","title":"BaseOrderUri"},{"location":"AgSyncEndpoint/#tokenurl","text":"","title":"tokenUrl"},{"location":"AgSyncEndpoint/#clientid","text":"","title":"ClientId"},{"location":"AgSyncEndpoint/#clientpass","text":"","title":"ClientPass"},{"location":"AgSyncEndpoint/#vaulturl","text":"","title":"VaultURL"},{"location":"AgSyncEndpoint/#agsynctokenkey","text":"","title":"AgSyncTokenKey"},{"location":"AgSyncEndpoint/#redirecturi","text":"","title":"RedirectUri"},{"location":"AgSyncEndpoint/#integrationid","text":"","title":"IntegrationId"},{"location":"AgronomistEnd-to-EndScenarios/","text":"Agronomist End-to-End Scenarios Overview Planned to Sales Agreement - sales tool within CE used to price product and transition growers into contracts. Fertilizer Calculator - utility that allows a salesperson to determine and calculate fertilizer requirements such as what product best fits the need of the customer and how much of that product is needed to fulfill the order (some work order is still needed and is anticipated to be ready in Fall 2020). Sales Order Process - integration of all Levridge pieces, specifically split billing, and its close integration with sales orders in F&O. Process #1: Planned to Sales Agreement There are three separate methods of kicking off a planned sales agreement with CE: 1. Batch plans 2. Regular plans 3. Proposals Batch Plans Bath Plans tab Generate new Populate required fields Name: this field will auto populate when you click save Proposal Type: there are three proposal types: Customer, Field, Prospect. The batch plan will autopopulate to the Customer Type. Customer Operation Split Group Sales Period: Needed to accurately price Save Programs tab Farmable acreage field is required Preconfigured set of products in many cases on agronomist recommends the same products at the same time so instead of calculating these rates each time, you can set up a program, so it automatically calculates product and rate. Add products utilizing hte tabs across the top of the batch plan (seed, fertilizer, chemical, etc.) After adding your products, you can either: Generate a proposal Generate a sales agreement Proposal Types There are three different proposal types within a batch plan: Customer, Prospect, and Field. 1. Customer: Simplest proposal type and is the most used proposal type for a batch plan 2. Prospect: Individual the salesperson is pursuing that is not a current client and would like to generate a price for product and contract. - Generate contract - Sales period - Farmable Acreage approximation - Navigate to the seed, fertilizer, or chemical tab(s) to add product - Create proposal - Open proposal - Proposal lines tab - Run report - Print proposal - Give to potential client 3. Field: Least common proposal type used - Only benefit of the field is if we go through and make individual plans - You can add products once to all applicable fields Creating a Regular Plan Plan to sales agreement Plans Generate new plan record Populate applicable fields Account Customer Operation Customer Site: Physical piece of land that you are putting product on Split group Description: Multiple plans for a field may be created so a detailed description is important Desired Crop Growing Season: The year the product will be applied to the field Utilize the tabs (Programs and Reccomendations, seed, fertilizer, or chemical) to add applicable product(s) Programs and Recommendations: Programs: Ability to add existing or new programs Recommendations: Attachment to precision mapping tools Seed (unit): Seed calculator: Helps to easily populate the quantity field Add product line(s) for all required seed Fertilizer: Timing: When to apply product Product Rate Unit/Rate This calculation not only helps get the product pricing but also helps to know when the grower needs specific products applied. Chemical: Timing: When to apply product Product Rate Unit/Rate This calculation not only helps get the product pricing but also helps to know when the grower needs specific products applied. Plan is complete and proposal can be generated New Proposal Plan to sales agreement Proposals Generate new proposal Populate applicable fields Description Amount Customer Operation Split Group Growing Season Sales Period Manager: the client will have a workflow that sets who the Manager is so it is auto-generated. This is a customization. Associated plans Add existing plans Select all associated plans created for this specific grower Add Create lines This function goes through each plan taking all product information and the split groups behind them and generates individual product lines for them and creates pricing. Item category terms Allow syou to set different payment methods by product. Select item category (product) select payment method Save Proposal lines Discounts/incentives to support with proposal price Launch line you would like to discount Charge codes Add new proposal line change Select charge code: Charge codes are user defined data that are defined in F&O/AX and are integrated over to CE Value: dollar amount to be deducated off total product price Save Refresh Proposal line will be updated to reflect new unit price after charge code has been applied. Unit price: You can manually override this field to decrease price as well. Calculate prices Add product lines from proposal lines tab In some instances, a customer will decide to add products after you have generated the proposal. Using the add new proposal line within the proposal lines tab will allow you to add last minute products to the proposal and recalculate price. 1. (+) New proposal line 2. Select product 3. Quantity 4. save 5. Calculate prices 6. Line will be added to the proposal lines toab and price will be updated for that product from data from F&O/AX. Submit for Approval Once your proposal is priced and complete, it is ready to be submitted for approval. The submit for approval function/button ensures that margin on products are maintained throughout the sales process. 1. Submit for approval - Looks for any charge code or line item changes that were made and compares them to the defined thresholds that have been set. - If any changes rise above threshold set it will stop the process, lock down proposal, and send to the Manager that the proposal needs approval before moving forward to contract/agreement - Seed: - Patented traits owned by specific manufacturer that you must have a license from before you can purchase and use product - The system will look and see if grower has avalid agreement to buy this product and if there is configuration fin F&O/AX to either warn or stop theprocess. - With seed, you will typically use the \"warn\" function instead of full stopping the process. - Restricted used pesticides: - Based on federal and state laws, before restricted products can be picked up, we must ensure the applicator has a valid license to apply the product. This license number needs to be physically listed on the invoice. This will warn the process this item needs to be reviewed. - At this point in the process, it is important to check back in unti lthe license number has been documented. 2. Generate sales agreement - The sales agreement will be sent to F&O/AX once generated. - The process is then complete within CE. - The final step in the sales process would be to pull up the third-party integration (ex: Hello Sign) to capture a digital signature. Process #2 Fertilizer Calculator The Fertilizer Calculator is a tool used to generate blends of fertility products. 1. Planning Tools 2. Fertilizer Calculator 3. Generate New 4. Populate applicable fields - Customer - Customer Operation - Batch size: Blender capacity when generated relays how many batches will need to occur to fill order. - Fertilizer State - Dry - Liquid Save Blend lines will populate with nutrient and product Input either input value (lbs.) or ratio (percentage) Click Calculate to populate output Quantity needed Total blend amount Number of batches needed Blend percent Generate sales order and release report to blender facilities to begin blending product (this function is not currently available but will be once it is fully developed and tested.) Process #3 Sales Order Process The sales order is comprised of two pieces: the sales order and the work order. Both are tightly integrated within FinOps. The purpose of the sales order is to give users that only have access to CE the ability to enter orders for delivery and perform some of the functions that are usually completed in FinOps/AX. Generating New Sales Order in CE Sales order tab Generate new Populate applicable sales order fields Account Customer Operation: It will default to one if the customer only has one operation. Split group: The individual paying for the product Delivery address: A requirement for many of the actions in F&O Inventory site: Where the inventory is arriving from Branch: Who is responsible for the customer or in charge of handling the billing Sales period: Used to price products against Ship date: Will default if not manually if not populated. Not a requirement. Save Enter Sales Order Lines Add product Pick quantity Save After saving the estimated price, which is pulled from CE, will be populated. This may not be the price that is actually paid but is a stored list price that adjustments can be made to. Submit for Approval Checks for any products that need an applicator license Checks for any seed tech licenses that may be required Credit check ( based off the credit settings with F&O. It will either warn or deny a generating sales order. ) Generate Sales Order Once your submission has been approved, you will need to generate a sales order in F&O. You will receive a pop-up notifying you the sales order was created successfully in F&O/AX. Once order is acted upon in F&O, you can go into details within CE and will provide more information on the split group allocations so the person in CE has access to details. Work Order The work order is a type of sales order where the Ag Retailer will be performing a service. A work order requires more detail than a basic sales order. 1. Orders 2. Work Orders 3. Generate new 4. Populate applicable fields - Account - Customer Operation - Customer Site - Auto Print: Printer logic exists to automatically print a report remotely. Would be needed when a hard copy is necessary for the remote location to send with the driver. - Sales site: Party who is responsible for the sale of the product. - Application Site: Paraty performing task - Application Operation: Type of work beign done (synced with AgSync). - Application Date: Anticipated application date of the product - Application Window: For chemical product this is important because depending on stage of the crop, it could cause harm if applied outside a specific timeframe. - Crop: Crop the product will be applied to - Order Status: - Planned: Occurring in the future - Booked: Occurring in the future - Released: Moved to queue to get done and application date is set - Scheduled: Has been assigned to an applicator for a scheduled date - In-Progress: Product was administered partially and due to an unforeseen event, the job was not completed and requires further attention. - Completed Pending Review: Applicatoin is complete. - Archived - Rejected - Work Order ID: Generated number out of AgSync - Dispensing Order Number: Only comes into effect if there is a blender/Kahler integration. This number is created when the picking list/mixed ticket is dispatched out to the warehouse. - Ordering Notes: - Notes that are specific to the field - Notes that are specific to the application - Application Notes: Notes or observations the equipment operator made while in the field. - Save Enter Work Order Lines Work Order Lines tab Add new line Populate applicable fields Sales Sites Customer Product Pests: Certain states require documentation that states the application and purpose of the sprayed chemical. Application Site Quantity Submit for Approval If this is an integrated order from AgSync or a third-party dispatch, the submit for approval function would not be present. The \"Submit for Proposal\" would show up if generated in CE. If you open a work order line, you will be able to view the Work Order Completion tab which provides more information on the application and applicator. Application Worker Application License Rolling Rock: Equipment that was used to perform application Weather data: Required by law to be tracked Wind speed Wind Direction Temperature Humidity Completion date State/End Times","title":"Agronomist End-to-End Scenarios"},{"location":"AgronomistEnd-to-EndScenarios/#agronomist-end-to-end-scenarios","text":"","title":"Agronomist End-to-End Scenarios"},{"location":"AgronomistEnd-to-EndScenarios/#overview","text":"Planned to Sales Agreement - sales tool within CE used to price product and transition growers into contracts. Fertilizer Calculator - utility that allows a salesperson to determine and calculate fertilizer requirements such as what product best fits the need of the customer and how much of that product is needed to fulfill the order (some work order is still needed and is anticipated to be ready in Fall 2020). Sales Order Process - integration of all Levridge pieces, specifically split billing, and its close integration with sales orders in F&O.","title":"Overview"},{"location":"AgronomistEnd-to-EndScenarios/#process-1-planned-to-sales-agreement","text":"There are three separate methods of kicking off a planned sales agreement with CE: 1. Batch plans 2. Regular plans 3. Proposals","title":"Process #1: Planned to Sales Agreement"},{"location":"AgronomistEnd-to-EndScenarios/#batch-plans","text":"Bath Plans tab Generate new Populate required fields Name: this field will auto populate when you click save Proposal Type: there are three proposal types: Customer, Field, Prospect. The batch plan will autopopulate to the Customer Type. Customer Operation Split Group Sales Period: Needed to accurately price Save Programs tab Farmable acreage field is required Preconfigured set of products in many cases on agronomist recommends the same products at the same time so instead of calculating these rates each time, you can set up a program, so it automatically calculates product and rate. Add products utilizing hte tabs across the top of the batch plan (seed, fertilizer, chemical, etc.) After adding your products, you can either: Generate a proposal Generate a sales agreement","title":"Batch Plans"},{"location":"AgronomistEnd-to-EndScenarios/#proposal-types","text":"There are three different proposal types within a batch plan: Customer, Prospect, and Field. 1. Customer: Simplest proposal type and is the most used proposal type for a batch plan 2. Prospect: Individual the salesperson is pursuing that is not a current client and would like to generate a price for product and contract. - Generate contract - Sales period - Farmable Acreage approximation - Navigate to the seed, fertilizer, or chemical tab(s) to add product - Create proposal - Open proposal - Proposal lines tab - Run report - Print proposal - Give to potential client 3. Field: Least common proposal type used - Only benefit of the field is if we go through and make individual plans - You can add products once to all applicable fields","title":"Proposal Types"},{"location":"AgronomistEnd-to-EndScenarios/#creating-a-regular-plan","text":"Plan to sales agreement Plans Generate new plan record Populate applicable fields Account Customer Operation Customer Site: Physical piece of land that you are putting product on Split group Description: Multiple plans for a field may be created so a detailed description is important Desired Crop Growing Season: The year the product will be applied to the field Utilize the tabs (Programs and Reccomendations, seed, fertilizer, or chemical) to add applicable product(s) Programs and Recommendations: Programs: Ability to add existing or new programs Recommendations: Attachment to precision mapping tools Seed (unit): Seed calculator: Helps to easily populate the quantity field Add product line(s) for all required seed Fertilizer: Timing: When to apply product Product Rate Unit/Rate This calculation not only helps get the product pricing but also helps to know when the grower needs specific products applied. Chemical: Timing: When to apply product Product Rate Unit/Rate This calculation not only helps get the product pricing but also helps to know when the grower needs specific products applied. Plan is complete and proposal can be generated","title":"Creating a Regular Plan"},{"location":"AgronomistEnd-to-EndScenarios/#new-proposal","text":"Plan to sales agreement Proposals Generate new proposal Populate applicable fields Description Amount Customer Operation Split Group Growing Season Sales Period Manager: the client will have a workflow that sets who the Manager is so it is auto-generated. This is a customization. Associated plans Add existing plans Select all associated plans created for this specific grower Add Create lines This function goes through each plan taking all product information and the split groups behind them and generates individual product lines for them and creates pricing. Item category terms Allow syou to set different payment methods by product. Select item category (product) select payment method Save Proposal lines Discounts/incentives to support with proposal price Launch line you would like to discount Charge codes Add new proposal line change Select charge code: Charge codes are user defined data that are defined in F&O/AX and are integrated over to CE Value: dollar amount to be deducated off total product price Save Refresh Proposal line will be updated to reflect new unit price after charge code has been applied. Unit price: You can manually override this field to decrease price as well. Calculate prices","title":"New Proposal"},{"location":"AgronomistEnd-to-EndScenarios/#add-product-lines-from-proposal-lines-tab","text":"In some instances, a customer will decide to add products after you have generated the proposal. Using the add new proposal line within the proposal lines tab will allow you to add last minute products to the proposal and recalculate price. 1. (+) New proposal line 2. Select product 3. Quantity 4. save 5. Calculate prices 6. Line will be added to the proposal lines toab and price will be updated for that product from data from F&O/AX.","title":"Add product lines from proposal lines tab"},{"location":"AgronomistEnd-to-EndScenarios/#submit-for-approval","text":"Once your proposal is priced and complete, it is ready to be submitted for approval. The submit for approval function/button ensures that margin on products are maintained throughout the sales process. 1. Submit for approval - Looks for any charge code or line item changes that were made and compares them to the defined thresholds that have been set. - If any changes rise above threshold set it will stop the process, lock down proposal, and send to the Manager that the proposal needs approval before moving forward to contract/agreement - Seed: - Patented traits owned by specific manufacturer that you must have a license from before you can purchase and use product - The system will look and see if grower has avalid agreement to buy this product and if there is configuration fin F&O/AX to either warn or stop theprocess. - With seed, you will typically use the \"warn\" function instead of full stopping the process. - Restricted used pesticides: - Based on federal and state laws, before restricted products can be picked up, we must ensure the applicator has a valid license to apply the product. This license number needs to be physically listed on the invoice. This will warn the process this item needs to be reviewed. - At this point in the process, it is important to check back in unti lthe license number has been documented. 2. Generate sales agreement - The sales agreement will be sent to F&O/AX once generated. - The process is then complete within CE. - The final step in the sales process would be to pull up the third-party integration (ex: Hello Sign) to capture a digital signature.","title":"Submit for Approval"},{"location":"AgronomistEnd-to-EndScenarios/#process-2-fertilizer-calculator","text":"The Fertilizer Calculator is a tool used to generate blends of fertility products. 1. Planning Tools 2. Fertilizer Calculator 3. Generate New 4. Populate applicable fields - Customer - Customer Operation - Batch size: Blender capacity when generated relays how many batches will need to occur to fill order. - Fertilizer State - Dry - Liquid Save Blend lines will populate with nutrient and product Input either input value (lbs.) or ratio (percentage) Click Calculate to populate output Quantity needed Total blend amount Number of batches needed Blend percent Generate sales order and release report to blender facilities to begin blending product (this function is not currently available but will be once it is fully developed and tested.)","title":"Process #2 Fertilizer Calculator"},{"location":"AgronomistEnd-to-EndScenarios/#process-3-sales-order-process","text":"The sales order is comprised of two pieces: the sales order and the work order. Both are tightly integrated within FinOps. The purpose of the sales order is to give users that only have access to CE the ability to enter orders for delivery and perform some of the functions that are usually completed in FinOps/AX.","title":"Process #3 Sales Order Process"},{"location":"AgronomistEnd-to-EndScenarios/#generating-new-sales-order-in-ce","text":"Sales order tab Generate new Populate applicable sales order fields Account Customer Operation: It will default to one if the customer only has one operation. Split group: The individual paying for the product Delivery address: A requirement for many of the actions in F&O Inventory site: Where the inventory is arriving from Branch: Who is responsible for the customer or in charge of handling the billing Sales period: Used to price products against Ship date: Will default if not manually if not populated. Not a requirement. Save","title":"Generating New Sales Order in CE"},{"location":"AgronomistEnd-to-EndScenarios/#enter-sales-order-lines","text":"Add product Pick quantity Save After saving the estimated price, which is pulled from CE, will be populated. This may not be the price that is actually paid but is a stored list price that adjustments can be made to. Submit for Approval Checks for any products that need an applicator license Checks for any seed tech licenses that may be required Credit check ( based off the credit settings with F&O. It will either warn or deny a generating sales order. ) Generate Sales Order Once your submission has been approved, you will need to generate a sales order in F&O. You will receive a pop-up notifying you the sales order was created successfully in F&O/AX. Once order is acted upon in F&O, you can go into details within CE and will provide more information on the split group allocations so the person in CE has access to details.","title":"Enter Sales Order Lines"},{"location":"AgronomistEnd-to-EndScenarios/#work-order","text":"The work order is a type of sales order where the Ag Retailer will be performing a service. A work order requires more detail than a basic sales order. 1. Orders 2. Work Orders 3. Generate new 4. Populate applicable fields - Account - Customer Operation - Customer Site - Auto Print: Printer logic exists to automatically print a report remotely. Would be needed when a hard copy is necessary for the remote location to send with the driver. - Sales site: Party who is responsible for the sale of the product. - Application Site: Paraty performing task - Application Operation: Type of work beign done (synced with AgSync). - Application Date: Anticipated application date of the product - Application Window: For chemical product this is important because depending on stage of the crop, it could cause harm if applied outside a specific timeframe. - Crop: Crop the product will be applied to - Order Status: - Planned: Occurring in the future - Booked: Occurring in the future - Released: Moved to queue to get done and application date is set - Scheduled: Has been assigned to an applicator for a scheduled date - In-Progress: Product was administered partially and due to an unforeseen event, the job was not completed and requires further attention. - Completed Pending Review: Applicatoin is complete. - Archived - Rejected - Work Order ID: Generated number out of AgSync - Dispensing Order Number: Only comes into effect if there is a blender/Kahler integration. This number is created when the picking list/mixed ticket is dispatched out to the warehouse. - Ordering Notes: - Notes that are specific to the field - Notes that are specific to the application - Application Notes: Notes or observations the equipment operator made while in the field. - Save","title":"Work Order"},{"location":"AgronomistEnd-to-EndScenarios/#enter-work-order-lines","text":"Work Order Lines tab Add new line Populate applicable fields Sales Sites Customer Product Pests: Certain states require documentation that states the application and purpose of the sprayed chemical. Application Site Quantity Submit for Approval If this is an integrated order from AgSync or a third-party dispatch, the submit for approval function would not be present. The \"Submit for Proposal\" would show up if generated in CE. If you open a work order line, you will be able to view the Work Order Completion tab which provides more information on the application and applicator. Application Worker Application License Rolling Rock: Equipment that was used to perform application Weather data: Required by law to be tracked Wind speed Wind Direction Temperature Humidity Completion date State/End Times","title":"Enter Work Order Lines"},{"location":"Agronomy/","text":"Agronomy The Agronomist End-to-End Scenarios provides an overview of: - Planned to Sales Agreement - Fertilizer Calculator - Sales Order Process The Agronomy Implementation Activities provides an overview of the implementation activities necessary for: - Integrations with AgSync and Kahler to FinOps - CE Integrations to FinOps - Agronomy for CE: A stand-alone CE instances without integration to FinOps","title":"Agronomy"},{"location":"Agronomy/#agronomy","text":"The Agronomist End-to-End Scenarios provides an overview of: - Planned to Sales Agreement - Fertilizer Calculator - Sales Order Process The Agronomy Implementation Activities provides an overview of the implementation activities necessary for: - Integrations with AgSync and Kahler to FinOps - CE Integrations to FinOps - Agronomy for CE: A stand-alone CE instances without integration to FinOps","title":"Agronomy"},{"location":"AgronomyImplementationActivities/","text":"Agronomy Implementation Activities and Estimates Overview The following is an overview of the implementation activities necessary for: - Integrations with AgSync and Kahler to FinOps - CE Integrations to FinOps - Agronomy for CE: A stand-alone CE instances without integration to FinOps Integrations with AgSync and Kahler to FinOps (+ Field Reveal) Which spot does the client want to be the source of truth for customer operations and site? Can be defined in Field Reveal, or AgSync or FinOps If in Field Reveal - do they want to migrate the data to FinOps? Determine if the client wants to use the customer site - adds some overhead Recommend FinOps as the master Discovery and deployment of Kahler TM2 module - Client does this piece. Kahler specific configurations Collaboration between client and Kahler Deploy and install Kahler integration on client machines Local install Uses IIS Start preparing data for data migrations - Client does this piece. If FinOps is the source of truth, this process can be streamlined Master data setup Determine if product masters are going to be used Build blended items/product configurator products Setup dispatching account Configure Levridge flags in inventory warehouse - Automatic BOM creator, warehouse items within each product/dispatching warehouse Id Setup unit rates Work order parameters Set site specific settings - default warehouse and site for each product that will be used in a work order Setup rolling stock - needs to correspond to AgSync Determine units of measure for products Sell UOM vs Application UOM Send clean data to 3rd parties (AgSync and Field Reveal) Setup and populate CDS with the known Ids from each party (FinOps, AgSync and Field Reveal) Also needs the blended items Get Azure AppServices and ServiceBus deployed and configured Can be done in parallel with the data preparation Setup event framework in FinOps for integrations With CE integration to FinOps All of the above for integrations with AgSync and Kahler to FinOps CE environment setup Import solutions Setup security roles on users Create an application user for use by the integration and services References the App Id setup in Azure tenet Get Azure AppServices and ServiceBus deployed and configured Get connection string to configure CE to FinOps integration plug-ins Create secure and unsecure configuration records Import flex grid configurations Determine which way data entities will be moving between CE and FinOps Where will entities be mastered Split groups, customer sites Setup relationship types in FinOps Needed for the integration of data between systems Takes multiple iterations to sort out Determine how charge codes are going to be setup Charge codes determine how discounting is done in CRM on agronomy contracts Setup the event framework to include more data for integrations Import CE system data and run integration push from FinOps Timing, crops, pests, etc. Configure master data Crop units, blend configurations, fertilizer settings, product nutrients, batch sizes Determine desired UI changes or customizations This is up to the client and can take many iterations Agronomy for CE Stand-alone CE instance without integration to FinOps.","title":"Agronomy Implementation Activities and Estimates"},{"location":"AgronomyImplementationActivities/#agronomy-implementation-activities-and-estimates","text":"","title":"Agronomy Implementation Activities and Estimates"},{"location":"AgronomyImplementationActivities/#overview","text":"The following is an overview of the implementation activities necessary for: - Integrations with AgSync and Kahler to FinOps - CE Integrations to FinOps - Agronomy for CE: A stand-alone CE instances without integration to FinOps","title":"Overview"},{"location":"AgronomyImplementationActivities/#integrations-with-agsync-and-kahler-to-finops-field-reveal","text":"Which spot does the client want to be the source of truth for customer operations and site? Can be defined in Field Reveal, or AgSync or FinOps If in Field Reveal - do they want to migrate the data to FinOps? Determine if the client wants to use the customer site - adds some overhead Recommend FinOps as the master Discovery and deployment of Kahler TM2 module - Client does this piece. Kahler specific configurations Collaboration between client and Kahler Deploy and install Kahler integration on client machines Local install Uses IIS Start preparing data for data migrations - Client does this piece. If FinOps is the source of truth, this process can be streamlined Master data setup Determine if product masters are going to be used Build blended items/product configurator products Setup dispatching account Configure Levridge flags in inventory warehouse - Automatic BOM creator, warehouse items within each product/dispatching warehouse Id Setup unit rates Work order parameters Set site specific settings - default warehouse and site for each product that will be used in a work order Setup rolling stock - needs to correspond to AgSync Determine units of measure for products Sell UOM vs Application UOM Send clean data to 3rd parties (AgSync and Field Reveal) Setup and populate CDS with the known Ids from each party (FinOps, AgSync and Field Reveal) Also needs the blended items Get Azure AppServices and ServiceBus deployed and configured Can be done in parallel with the data preparation Setup event framework in FinOps for integrations","title":"Integrations with AgSync and Kahler to FinOps (+ Field Reveal)"},{"location":"AgronomyImplementationActivities/#with-ce-integration-to-finops","text":"All of the above for integrations with AgSync and Kahler to FinOps CE environment setup Import solutions Setup security roles on users Create an application user for use by the integration and services References the App Id setup in Azure tenet Get Azure AppServices and ServiceBus deployed and configured Get connection string to configure CE to FinOps integration plug-ins Create secure and unsecure configuration records Import flex grid configurations Determine which way data entities will be moving between CE and FinOps Where will entities be mastered Split groups, customer sites Setup relationship types in FinOps Needed for the integration of data between systems Takes multiple iterations to sort out Determine how charge codes are going to be setup Charge codes determine how discounting is done in CRM on agronomy contracts Setup the event framework to include more data for integrations Import CE system data and run integration push from FinOps Timing, crops, pests, etc. Configure master data Crop units, blend configurations, fertilizer settings, product nutrients, batch sizes Determine desired UI changes or customizations This is up to the client and can take many iterations","title":"With CE integration to FinOps"},{"location":"AgronomyImplementationActivities/#agronomy-for-ce","text":"Stand-alone CE instance without integration to FinOps.","title":"Agronomy for CE"},{"location":"AgronomyRetail/","text":"Agronomy Retail Overview How to Create a Split Group in F&O Manage Splits, Sales Contracts and Prepayments Agronomy Retail Implementation Activities Customer Finance Programs","title":"Agronomy Retail"},{"location":"AgronomyRetail/#agronomy-retail","text":"","title":"Agronomy Retail"},{"location":"AgronomyRetail/#overview","text":"How to Create a Split Group in F&O Manage Splits, Sales Contracts and Prepayments Agronomy Retail Implementation Activities Customer Finance Programs","title":"Overview"},{"location":"AgronomyRetailImplementationActivities/","text":"Agronomy Retail Implementation Activities Overview The following is an overview of the Agronomy Retail implementation activities. Enter setup data under Accounts Receivable > Setup > Agriculture (if not previously imported) a. Customer operation types b. Customer site types c. Growing seasons d. Sales periods e. Lines of business f. Finance programs g. Seed and technology agreement compliance h. Dispatching accounts Enter setup data under Organization administration > Global address book > Relationship types a. Need to define a relationship from customer to customer operation Enter sales agreement classifications under Accounts Receivable > Setup > Sales agreement classifications Under General Ledger > Journal setup > Journal names set the Prepayment posting profile flag to identify journals used for customer prepayments Configure parameters under Accounts Receivable > Setup > Agriculture > Agriculture parameters Under Organization administration > Number sequences > Number sequences enter or generate number sequences Enter setup data under Accounts payable > Payment: a. Terms of payment - set prepayment fields b. Cash discounts - set prepayment fields i. Within cash discounts set up cash discount schedules Configure ag specific settings in Accounts Receivable > Setup > Accounts receivable parameters: a. Credit rating b. Credit limits c. Prices Enter vendor zones under Procurement and sourcing > Setup > Prices and discounts > Vendor zones Enter master data - involves a lot of client feedback a. Customers i. Ag specific settings are: 1. Zone address 2. Dispatching account id if using a 3rd party dispatching system (like AgSync) 3. Seed and technology agreements 4. Membership fast tab for patronage 5. Taxation fast tab 6. On customer addresses set ag taxes 7. Billing notes b. Split groups c. Customer operations d. Customer sites e. Customer finance programs f. Vendors (if not the same as customers in the Ag parameters config form) i. On vendor addresses set ag taxes g. Rolling stock i. Cab configuration ii. Drive train iii. Equipment types iv. Status v. Equipment reasons vi. Equipment parameters vii. Equipment h. Contacts i. Setup applicator licenses Enter indirect ag taxes under Tax > Indirect taxes > Sales tax: a. Sales tax codes b. Sales tax groups c. Item sales tax groups d. Sales tax settlement periods e. Tax reasons Under Inventory management > Setup > Inventory breakdown setup sites and warehouse set ag tax values Prepare and enter products by setting up ag settings in: a. Product information management > Setup > Categories and attributes > Category hierarchies b. Accounts receivable > Setup > License and certification: i. Certificate types ii. Certificates compliance iii. Regulated products iv. Restricted product regional lists c. Feed and livestock > Setup > Veterinary feed directive (VFD): i. Veterinary feed directive (VFD) drug groups ii. Veterinary feed directive (VFD) compliance d. On released products, set the default order settings for each valid site and warehouse for the product i. If using Kahler with this item, under Manage inventory on Action pane > Warehouse items > Set the dispensing method e. On released products: - Assign regional lists - Assign certificates - Assign drug groups - Assign active ingredients - Assign nutrients - If using scale - set flag to recognize the product as one that will be used at scale - If using TMS - under Transportation fast tab set enable TMS and identify the default carrier service - Define ag specific settings on supplementary sales items i. Setup up substitute items Under Accounts receivable > Setup > Trade agreement journals enter trade agreement journals including ag specific settings Define transfer order charges Under Inventory management > Setup > Charge codes If using Transportation management: a. Configure TMS Parameters under Transportation management > Setup > Transportation management parameters i. In the parameters form choose to Initialize base engine data b. Configure data under Transportation management > Setup > Load building i. Load templates ii. Equipment c. Configure data under Transportation management > Setup > Carriers: i. Carrier service codes ii. Mode iii. Transportation methods d. Enter data under Transportation management > Setup > Rating i. Carrier accessorial charge ii. Break master iii. Rate master iv. Rating profile v. Rating metadata vi. Miscellaneous charges e. Enter data under Transportation management > Setup> Freight reconciliation i. Reconciliation reasons ii. Freight bill type assignments iii. Audit master f. Enter data under Transportation management > Setup > Bing map usage key (if using mapping which most clients will be doing) g. Enter data under Transportation management > Setup > Transportation standards i. NMFC codes ii. LTL classes h. Enter data under Transportation management > Setup > Engines i. Mileage engine ii. Rate engine iii. Transit time engine iv. Zone master i. Enter shipping carriers under Transportation management > Setup > Carriers > Shipping carriers i. On the carriers define services, addresses and rating profiles and assign the shipping carrier to a vendor account","title":"Agronomy Retail Implementation Activities"},{"location":"AgronomyRetailImplementationActivities/#agronomy-retail-implementation-activities","text":"","title":"Agronomy Retail Implementation Activities"},{"location":"AgronomyRetailImplementationActivities/#overview","text":"The following is an overview of the Agronomy Retail implementation activities. Enter setup data under Accounts Receivable > Setup > Agriculture (if not previously imported) a. Customer operation types b. Customer site types c. Growing seasons d. Sales periods e. Lines of business f. Finance programs g. Seed and technology agreement compliance h. Dispatching accounts Enter setup data under Organization administration > Global address book > Relationship types a. Need to define a relationship from customer to customer operation Enter sales agreement classifications under Accounts Receivable > Setup > Sales agreement classifications Under General Ledger > Journal setup > Journal names set the Prepayment posting profile flag to identify journals used for customer prepayments Configure parameters under Accounts Receivable > Setup > Agriculture > Agriculture parameters Under Organization administration > Number sequences > Number sequences enter or generate number sequences Enter setup data under Accounts payable > Payment: a. Terms of payment - set prepayment fields b. Cash discounts - set prepayment fields i. Within cash discounts set up cash discount schedules Configure ag specific settings in Accounts Receivable > Setup > Accounts receivable parameters: a. Credit rating b. Credit limits c. Prices Enter vendor zones under Procurement and sourcing > Setup > Prices and discounts > Vendor zones Enter master data - involves a lot of client feedback a. Customers i. Ag specific settings are: 1. Zone address 2. Dispatching account id if using a 3rd party dispatching system (like AgSync) 3. Seed and technology agreements 4. Membership fast tab for patronage 5. Taxation fast tab 6. On customer addresses set ag taxes 7. Billing notes b. Split groups c. Customer operations d. Customer sites e. Customer finance programs f. Vendors (if not the same as customers in the Ag parameters config form) i. On vendor addresses set ag taxes g. Rolling stock i. Cab configuration ii. Drive train iii. Equipment types iv. Status v. Equipment reasons vi. Equipment parameters vii. Equipment h. Contacts i. Setup applicator licenses Enter indirect ag taxes under Tax > Indirect taxes > Sales tax: a. Sales tax codes b. Sales tax groups c. Item sales tax groups d. Sales tax settlement periods e. Tax reasons Under Inventory management > Setup > Inventory breakdown setup sites and warehouse set ag tax values Prepare and enter products by setting up ag settings in: a. Product information management > Setup > Categories and attributes > Category hierarchies b. Accounts receivable > Setup > License and certification: i. Certificate types ii. Certificates compliance iii. Regulated products iv. Restricted product regional lists c. Feed and livestock > Setup > Veterinary feed directive (VFD): i. Veterinary feed directive (VFD) drug groups ii. Veterinary feed directive (VFD) compliance d. On released products, set the default order settings for each valid site and warehouse for the product i. If using Kahler with this item, under Manage inventory on Action pane > Warehouse items > Set the dispensing method e. On released products: - Assign regional lists - Assign certificates - Assign drug groups - Assign active ingredients - Assign nutrients - If using scale - set flag to recognize the product as one that will be used at scale - If using TMS - under Transportation fast tab set enable TMS and identify the default carrier service - Define ag specific settings on supplementary sales items i. Setup up substitute items Under Accounts receivable > Setup > Trade agreement journals enter trade agreement journals including ag specific settings Define transfer order charges Under Inventory management > Setup > Charge codes If using Transportation management: a. Configure TMS Parameters under Transportation management > Setup > Transportation management parameters i. In the parameters form choose to Initialize base engine data b. Configure data under Transportation management > Setup > Load building i. Load templates ii. Equipment c. Configure data under Transportation management > Setup > Carriers: i. Carrier service codes ii. Mode iii. Transportation methods d. Enter data under Transportation management > Setup > Rating i. Carrier accessorial charge ii. Break master iii. Rate master iv. Rating profile v. Rating metadata vi. Miscellaneous charges e. Enter data under Transportation management > Setup> Freight reconciliation i. Reconciliation reasons ii. Freight bill type assignments iii. Audit master f. Enter data under Transportation management > Setup > Bing map usage key (if using mapping which most clients will be doing) g. Enter data under Transportation management > Setup > Transportation standards i. NMFC codes ii. LTL classes h. Enter data under Transportation management > Setup > Engines i. Mileage engine ii. Rate engine iii. Transit time engine iv. Zone master i. Enter shipping carriers under Transportation management > Setup > Carriers > Shipping carriers i. On the carriers define services, addresses and rating profiles and assign the shipping carrier to a vendor account","title":"Overview"},{"location":"Agsync/","text":"Agsync AgSync is a dispatching application owned by Raven Industries. AgSync is used by ag retailers to dispatch applicators and their equipment to different fields. More specifically, within AgSync agronomists can schedule out the days for applicators and their equipment pointing them to field A, then B, then C based on the location of the fields and the machinery and products the applicator takes with them. Levridge has built an integration between D365 finance and AgSync to connect the systems together. Within the integration, there are multiple types of data moving both ways. Master data moves from D365 to AgSync telling AgSync what customers, fields and products the ag retailer has. AgSync uses work orders to schedule the work on specific fields. Once created, work orders are sent from AgSync to D365 as sales orders. It is a bidirectional integration that consists of a Topic for Master Data that goes from D365 F&O to AgSync and Work Orders that go from AgSync to D365 F&O. The Work Order integration utilize a background service running in the same application as the Webhook controller so there is no need for two integration application instances. There are a few unique aspects to the AgSync integration that is different from other integrations: - The Workorder integration service runs in the same app service as the controller and the Master Data integration service. - The Workorder integration service does all the transformation directly rather than using an EntityMapper. - The Workorder integration service makes a direct service call to D365 F&O rather than using a data source. - We are using CDS to provide lookup services for mapping entity identifiers between systems. - We utilize a filter on the event in F&O to send only the customers that need to to be sent from F&O to AgSync. This document will provide an overview of the integration framework used to move the data and then it will describe the data moving within the integration. Let's start with the technology behind the integration. Integration Description Levridge's Integration Framework can be viewed in Integration Overview . Standard Master Data Integration The standard master data integration configuration is shown below. External UUID Master Data Integration There is an alternate configuration that may be required when integrating with Field Reveal when Field Reveal is the master source for customer fields. In this scenario Field Reveal will provide the UUIDs it generates so we can provide those values to Agsync when we create the corresponding records in Agsync. This scenario is complicated and should be avoided if possible. The reason it is necessary is because when Field Reveal creates a work order in Agsync directly it uses an old Agsync API that uses a UUID as the Sync ID. Field Reveal generates the UUID and needs to provide those to Levridge so they can be used when creating the master record in Agsync. In this scenario, it is imperative that the Customer and Customer Operation is not sent from F&O until they have been updated in Field Reveal and a Field created in Field Reveal. Once a record is created in Agsync the UUID for it cannot be modified. The order of creation is as follows: 1. Customer and Customer Operation are created in FinOps. 2. Grower and Farm are created in Field Reveal and SyncIds are added in Field Reveal. 3. Field is created in Field Reveal. 1. This will cause a Field record to be sent from Field Reveal to the Levridge Field controller . 2. The Field data will be placed on a service bus topic that has two subscriptions: 1. One subscription will be serviced by the FieldToCDS integration service. 1. This will use the UUID information to create a lookup record in CDS. 2. The other subscription will be serviced by the FieldToAX integration service 1. This will create the Customer Site in FinOps. 3. When the record is created in FinOps it will trigger the event that sends the data to Agsync. 1. The creation of the Customer Site entity causes the event to be evaluated. 2. The filter that checks to make sure a Customer has an operation that has a site that corresponds to a field passes. 4. The Customer, Operation and Site are all sent to the FinOpsToAgsync service bus topic. 5. The AxToAgsync integration service receives the message and looks up the UUIDs from CDS then sends the records to Agsync for creation. 6. Agsync creates the records and sends back a GUID. 7. The AxToAgsync integration updates CDS with the GUID values. This communication diagram depicts this interaction: Common Data Service (CDS) The Common Data Service (CDS) is a solution from Microsoft built on top of the Power Platform. CDS tables are used in the integrations between D365 and AgSync to translate values that are different between the 2 systems. For example, in AgSync, customers have unique identifiers called GUIDs. These values are different from the unique identifiers D365 has for the same customer accounts. Levridge has implemented new tables in CDS to translate the D365 Customer account to the AgSync GUID. For the CDS environment there are two purchasing options: 1. If the ag retailer has an existing CE instance, use it. A portion of the CE instance can be firewalled off for security purposes and then it can be used to host the CDS solution. 2. Buy a specific CDS environment for only the CDS solution. This is more expensive. There are 7 types of data stored in CDS: 1. Dispatch Accounts 2. Customers 3. Customer Operations 4. Customer Sites 5. Customer Site Locations 6. Master Items 7. Workers All these types are stored differently in AgSync than they are in D365 so they must be translated using the data in CDS. Products and Employees must be manually entered into the CDS database while Dispatch Accounts, Customers, Customer Operations Customer Sites and Customer Site Locations can be populated using a script that queries the data in AgSync and populates the data in CDS. All this data must exist in CDS prior to using the work order integrations. The Master Item entity translates AgSync operations to D365 Product and Service Items. This states which task you are performing and included in every work order. - Product ID Numbers (this relates to the Master item in D365). AgSync has the ability to generate blends (ex: blending multiple ingredients to produce the fertilizer to be used on a field). There is a Master Item Table in CDS configured specifically for the operation. If there is a blend flag on an order change, it will pull from the Master Item Table. The Worker Table is a translation table required to map the applicator (worker) set up in AgSync to D365. This provides the information of who applied the product to the field and their applicator license number. An applicator license number is required on the sales order. Three entities need to be manually entered in CDS. The CDS setup includes: - CE Levridge - AgSync Solution - Choose appropriate UI Form Once the CDS solution is deployed to an environment, an Application User will need to be created. Work Order Integration Below is the process to generate a work order in AgSync: - Select Customer - State where the application or service location will be completed out of - Include what work is being done - State the work order status (planned or released) - Planned: These are premade orders ready for a certain time. The action is not ready to be performed. Not all users can view planned status due to security measures in place (ex: dispatcher unable to see planned work status). - Released: The work orders are released and assigned to complete field work. - The work order comes to F&O regardless of status. - State the performed operation - State the products being utilized - Indicate the impacted crop(s) - Add in any additional notes - Save Order After a work order is saved, AgSync calls the Levridge AgsyncOrderChanged webhook sending over the work order data. The Levridge AgsyncOrderChanged webhook transforms the work order data and sends it to F&O where it creates or updates a corresponding sales order. If F&O accounts receivables are configured to perform a credit check and the credit check fails, F&O will send back a response indicating that the credit check did not pass. Then a rejection request is sent to Agsync to mark the work order as rejected. When F&O receives an updated work order from Agsync, F&O will remove all existing sales order line(s) and create new sale order line(s) to match the Agsync updated work order. This is done to keep the F&O sale order in sync with the possible changes that came through with Agsync updated work order. If a F&O sale's order has an active dispensing work order associated to it, then F&O will not delete or create new sales order line(s). An important item to note: A business process needs to be in place in the situation if AgSync creates a scheduled work order which gets scheduled, and then a dispensing work order is created and gets sent to Kahler. F&O will not pick up new changes if AgSync tries to go back to a release status and make changes and try to reschedule it. AgSync needs to cancel the Kahler order and F&O dispensing order and reschedule their AgSync work order. Microsoft Azure App Service The Levridge Integration Framework has been written as a web application that hosts HTTP endpoints as REST APIs and background processes that handle integrations. It is most commonly run in the cloud as an Azure App Serivce. It can also run as a windows service or as an IIS application. The Integration Overview provides additional information about the deployment options. Most of the setup will occur in Azure App Service. This is a requirement for AgSync in which AgSync pulls from Levridge's Azure Service Bus and an HTTP-based service for hosting web applications. Azure hosts our web app integration. The steps needed to create an App Service can be found under the Integration Overview . After the App ID has been generated along with the deployed integration code to App Service , there is an application configuration file named [appsettings.json])./appsettings.json.md). This file is located within the generated Azure Service. In the App Services portion of the Azure Portal there is a Kudos button on the left-hand side, with a function to ZipDeploy code . This is where one would deploy the integration code. After deployed, back in Azure Service and right below Kudos button, there is an App Service Editor where one would deploy the App Settings json file to, which will give a list of all the files part of the integration. One of those is called AppSettings.json where the configuration is made for the Azure Service. Configuration Set up the logging level . The next section to be configured would outline what direction information is flowing within the service itself. There is a target configuration and source configuration which is needed to specify: Which system is the source system and which system is the target system. Which configuration section contains data connection information. Which section contains the service bus configuration to use. In the appsettings.json you will need to define the InstanceConfig SourceConfig and TargetConfig nodes as follows: \"InstanceConfig\": { \"AzureTableConfiguration\": \"[section name to Azure Table Configuration\", \"LogRequestsAndResponses\": [true or false] \"EnableAppInsightsAdaptiveSampling\": [true or false] }, \"SourceConfig\": { \"ServiceBusConfigName\": \"[section name with service bus topic and subscription for Agsync Master Data]\", \"ODataConfigName\": \"[section name with F&O data configuration]\", \"SystemName\": \"DynamicsAx\", \"Direction\": \"Source\" }, \"TargetConfig\": { \"ODataConfigName\": \"[section name with Agsync endpoint configuration]\", \"CDSConfigName\": \"[section name with CDS data configuration]\", \"SystemName\": \"AgSync\", \"Direction\": \"Target\", } You must also include the controller entry to have the controller loaded: \"Controllers\": { \"HostController\": \"Levridge.Integration.Host.DefaultController\", \"AgSyncConroller\": \"Levridge.Integration.Host.AgSyncController\" } You must also configure two objects for Agsync integration: AgSyncEndpoint agsync Here is a sample template for the entire appsettings.json file used for the integration from FinOps to Agsync: { \"Controllers\": { \"HostController\": \"Levridge.Integration.Host.DefaultController\", \"AgSyncConroller\": \"Levridge.Integration.Host.AgSyncController\" }, \"Logging\": { \"Debug\": { \"LogLevel\": { \"Default\": \"Information\" } }, \"Console\": { \"IncludeScopes\": true, \"LogLevel\": { \"Default\": \"Information\" } }, \"LogLevel\": { \"Default\": \"Information\" } }, \"AllowedHosts\": \"*\", \"SourceConfig\": { \"ServiceBusConfigName\": \"AgsyncMasterDataServiceBus\", \"ODataConfigName\": \"DynamicsAX\", \"SystemName\": \"DynamicsAx\", \"Direction\": \"Source\" }, \"TargetConfig\": { \"ODataConfigName\": \"AgSyncEndpoint\", \"SystemName\": \"AgSync\", \"Direction\": \"Target\", \"CDSConfigName\": \"CDS\" }, \"DynamicsAX\": { \"UriString\": \"[URL to D365 F&O]\", \"ActiveDirectoryResource\": \"[URL to D365 F&O]\", \"ActiveDirectoryTenant\": \"https://login.microsoftonline.com/[Customer_Tenant_ID]\", \"ActiveDirectoryClientAppId\": \"[Application ID used to register the application in AD]\", \"ActiveDirectoryClientAppSecret\": \"[Client Secret generated for the Application ID in AD]\", \"ODataEntityPath\": \"[URL to D365 F&O]/data/\" }, \"CDS\": { \"UriString\": \"[URL to CDS or Localhost]\", \"ActiveDirectoryResource\": \"[URL to CDS]\", \"ActiveDirectoryTenant\": \"https://login.microsoftonline.com/[Customer_Tenant_ID]\", \"ActiveDirectoryClientAppId\": \"[Application ID used to register the application in AD]\", \"ActiveDirectoryClientAppSecret\": \"[Client Secret generated for the Application ID in AD]\", \"ODataEntityPath\": \"[URL to CDS]/api/data/v9.0/\", \"AssemblyName\": \"Levridge.ODataDataSources.CDS\", \"ClientClassesNameSpace\": \"Levridge.ODataDataSources.CDS\", \"MetadataResource\": \"CDSMetadata.xml\" }, \"AgSyncEndpoint\": { \"MustUseWktProcessor\": true, \"baseUri\": \"https://fields.agsync.com/api/\", \"tokenUrl\": \"https://auth.agsync.com/core/connect/token\", \"ClientId\": \"[client ID assigned by Agsync]\", // customer specific \"ClientPass\": \"[Client Secret assigned by Agsync]\", // customer specific \"ValutURL\": \"[URL to customer Azure Key Vault]\", \"AgSyncTokenKey\": \"AgsyncAccessToken\", \"RedirectUri\": \"[URL to AgsyncAuth controller]\", // customer specific \"IntegrationId\": \"CustomerIntegrationID\" // customer specific }, \"agsync\": { // used by Webhook \"MustUseWktProcessor\": true, \"ConnectionString\": \"[connection string to Agsync master data topic]\", \"TopicName\": \"[Agsync master data topic]\", \"RequiresSession\": true, \"RedirectUri\": \"[URL to AgsyncAuth controller]\", \"TokenUrl\": \"https://auth.agsync.com/core/connect/token\", \"AuthorizeUrl\": \"https://auth.agsync.com/core/connect/authorize\", \"baseUri\": \"https://fields.agsync.com/api/\", \"ClientId\": \"[client ID assigned by Agsync]\", \"ClientPass\": \"[Client Secret assigned by Agsync]\", \"ValutURL\": \"[URL to customer Azure Key Vault]\", \"AgSyncTokenKey\": \"AgsyncAccessToken\" }, \"AgsyncMasterDataServiceBus\": { \"ConnectionString\": \"[connection string to Agsync master data topic]\", \"TopicName\": \"[Agsync master data topic]\", \"SubscriptionName\": \"[Agsync master data subscription name]\", \"RequiresSession\": true } } Microsoft Azure Service Bus Microsoft Azure Service Bus is a message bus for businesses to exchange documents and messages in the Cloud. There are two main tiers: - Standard Service Bus: Supports 250 Kilobytes of data - Premium Service Bus: Supports 1 Megabyte of data Most Levridge clients will be able to utilize the Standard Service Bus tier, however, if a client is integrating well-known-text (WKT) files with their work orders it may push the workorder size over the 250K data limit and they should look at upgrading to Premium Service Bus. Microsoft outlines how to create an Azure Service bus . The Azure Key Vault is a tool for securely storing and accessing secrets. A secret is anything that you want to tightly control access to, such as API keys, passwords, or certificates. The vault enables us to securely store passwords, certificates, etc. with secure access. The key vault stores the authorization credentials for AgSync and by providing the secret to AgSync it lets AgSync know it is Levridge talking to them. This is required for all AgSync integrations. One can read how to setup the Azure Key Vault . When customer entities are sent from D365 to the Azure service bus, the agsync integration queries CDS to find the stored Customer ID. If there is no matching customer record the record is sent as a Create to AgSync. The customer is created in AgSync and the response from Agsync contains the Agsync specific ID for that customer. The newly generated AgSync ID is saved in combination with the D365 customer account in CDS. If CDS did have an existing record for the D365 customer, the message is updated to include the Agsync ID for that customer and the messages is sent to Agsync as an Update. Once a sales order is created in F&O, we will message back to AgSync and update them with the sales order number and the dispensing work order number (if available at the time of creation). This requires an entity setup in the event framework. Integration purchase requirements include: 1. CDS Instance - CE License 2. Azure Subscription - Service Bus - App Service - Key Vault 3. AgSync Subscription Setup Internal setup time for configuration is 24 hours. To integrate from D365 F&O to Agsync you will need to: Create an Azure Service bus topic Create a subscription on the topic above Configure Event Endpoint in F&O Configure Levridge Entity Events Create Filter on Entity Event to only send agronomy customers Get Client ID and CLient password from Agsync Get Customer Specific Integration ID from Agsync Client Redirect URL is [Azure Webapp base URL]/api/AgsyncAuth Setup Azure Keyvault Create an application ID for the integration framework to authenticate to D365 CE Create an application user in D365 CRM and assign the proper role(s) A webhook is an API provided by the Levridge Integration Framework that gives AgSync the ability to send workorders to Levridge. AgSync sends workorders to the Levridge API which places the order on the service bus. The customer's specific URL needs to be whitelisted by AgSync. To whitelist a URL, one would log into AgSync and create a helpdesk ticket requesting AgSync to add the AgsyncOrderChanged URL into their setup file. The expected service time is between 24-48 hours. Escalate after 48 hours. General setup timelines: - Deploying code to App Service takes the longest. 400 table lines for all three. - The worker table is refreshed once a quarter - Service item data is refreshed once a year - Master item data is refreshed once every three year Data Moving Within the Integration: There are 4 types of master records sent from D365 to AgSync: 1. Customers 2. Customer Operations 3. Customer Sites 4. LevAgSyncSalesDetailEntity - The LevAgSyncSalesDetailEntity stores sales, order number, dispensing work order numbers, and a unique work order ID that identifies the AgSync work order that needs to update. A Dispensing Account ID field has been added in F&O on the customer account under the sales order tab. Its purpose is to indicate which dispensing branch the customer receives services from. The dispensing branch within AgSync can be set up as either: - An inventory site indicating where the product is physically located - Commission/Sales groups indicating the agronomist that sold the service to the customer (Confirm and clarify this statement) Integration Framework Setup for Sending Customers \"Out of the Box Customer V3\" is being utilized and the code in the integration is choosing the data fields the entities pass through. The Dispatching Account ID is included on the customer form to only send accounts that have a dispatching account tied to it to AgSync. This is filtered by the Event Framework V3 Entity. An example is a client might have 100K in their database but for work order purposes there are only 30K that will ever work with AgSync. Filtering is again used in the Event Framework V3 Entity in the Customer Operation. This is a more sophisticated filter due to multiple types of operations an individual customer could have. Customer Operation is setup by four different filters: 1. Customer Operation Type 2. A filter is added so customer information which do not have a site is not sent. - Example: Customer operations will not be sent to AgSync if they do not have sites tied to them. 3. Customer Site Types 4. Dispensing Site ID Release Status Process A work order is created at release status. Take release in AgSync and schedule work order. (Scheduling is assigning it out to an applicator to be applied). Dispatcher is going to see all released orders. Dispatcher takes orders, creates task packet, assigns to worker, annotates which piece of application equipment is going to be running, and saves as scheduled packet. The message is once again generated to webhook in F&O. Status is updated to \"scheduled\" if a blend. This creates an automatically explode bomb at scheduled date. (ex: if a client is using automatic fertilizer, dispensing ticket is released at the scheduled state which kicks off another integration to Kahler, so they can blend and send product out to field that is going to be applied.) Controllers Agsync Auth Controller The AgsyncAuth controller is used to generate a token needed to integrate with Agsync. Agsync Auth Test Controller The AgsyncAuthTest controller is used to ... Agsync Order Changed Controller The AgsyncOrderChanged controller is used by Agsync to send work orders as they are created or updated. This controller will bundle the work order into a message and place it in the message topic. Agsync Sync Accounts Controller The AgsyncSyncAccounts controller is used to query Agsync for master data and write the information into CDS. This is done during go-live to populate the lookup data in CDS. Agsync UUID Controller The AgsyncUUID controller provides UUIDs based on Sync Ids passed to the controller. This is used by Field Reveal to obtain the UUID from the Sync ID entered into Field Reveal.","title":"Agsync"},{"location":"Agsync/#agsync","text":"AgSync is a dispatching application owned by Raven Industries. AgSync is used by ag retailers to dispatch applicators and their equipment to different fields. More specifically, within AgSync agronomists can schedule out the days for applicators and their equipment pointing them to field A, then B, then C based on the location of the fields and the machinery and products the applicator takes with them. Levridge has built an integration between D365 finance and AgSync to connect the systems together. Within the integration, there are multiple types of data moving both ways. Master data moves from D365 to AgSync telling AgSync what customers, fields and products the ag retailer has. AgSync uses work orders to schedule the work on specific fields. Once created, work orders are sent from AgSync to D365 as sales orders. It is a bidirectional integration that consists of a Topic for Master Data that goes from D365 F&O to AgSync and Work Orders that go from AgSync to D365 F&O. The Work Order integration utilize a background service running in the same application as the Webhook controller so there is no need for two integration application instances. There are a few unique aspects to the AgSync integration that is different from other integrations: - The Workorder integration service runs in the same app service as the controller and the Master Data integration service. - The Workorder integration service does all the transformation directly rather than using an EntityMapper. - The Workorder integration service makes a direct service call to D365 F&O rather than using a data source. - We are using CDS to provide lookup services for mapping entity identifiers between systems. - We utilize a filter on the event in F&O to send only the customers that need to to be sent from F&O to AgSync. This document will provide an overview of the integration framework used to move the data and then it will describe the data moving within the integration. Let's start with the technology behind the integration.","title":"Agsync"},{"location":"Agsync/#integration-description","text":"Levridge's Integration Framework can be viewed in Integration Overview .","title":"Integration Description"},{"location":"Agsync/#standard-master-data-integration","text":"The standard master data integration configuration is shown below.","title":"Standard Master Data Integration"},{"location":"Agsync/#external-uuid-master-data-integration","text":"There is an alternate configuration that may be required when integrating with Field Reveal when Field Reveal is the master source for customer fields. In this scenario Field Reveal will provide the UUIDs it generates so we can provide those values to Agsync when we create the corresponding records in Agsync. This scenario is complicated and should be avoided if possible. The reason it is necessary is because when Field Reveal creates a work order in Agsync directly it uses an old Agsync API that uses a UUID as the Sync ID. Field Reveal generates the UUID and needs to provide those to Levridge so they can be used when creating the master record in Agsync. In this scenario, it is imperative that the Customer and Customer Operation is not sent from F&O until they have been updated in Field Reveal and a Field created in Field Reveal. Once a record is created in Agsync the UUID for it cannot be modified. The order of creation is as follows: 1. Customer and Customer Operation are created in FinOps. 2. Grower and Farm are created in Field Reveal and SyncIds are added in Field Reveal. 3. Field is created in Field Reveal. 1. This will cause a Field record to be sent from Field Reveal to the Levridge Field controller . 2. The Field data will be placed on a service bus topic that has two subscriptions: 1. One subscription will be serviced by the FieldToCDS integration service. 1. This will use the UUID information to create a lookup record in CDS. 2. The other subscription will be serviced by the FieldToAX integration service 1. This will create the Customer Site in FinOps. 3. When the record is created in FinOps it will trigger the event that sends the data to Agsync. 1. The creation of the Customer Site entity causes the event to be evaluated. 2. The filter that checks to make sure a Customer has an operation that has a site that corresponds to a field passes. 4. The Customer, Operation and Site are all sent to the FinOpsToAgsync service bus topic. 5. The AxToAgsync integration service receives the message and looks up the UUIDs from CDS then sends the records to Agsync for creation. 6. Agsync creates the records and sends back a GUID. 7. The AxToAgsync integration updates CDS with the GUID values. This communication diagram depicts this interaction:","title":"External UUID Master Data Integration"},{"location":"Agsync/#common-data-service-cds","text":"The Common Data Service (CDS) is a solution from Microsoft built on top of the Power Platform. CDS tables are used in the integrations between D365 and AgSync to translate values that are different between the 2 systems. For example, in AgSync, customers have unique identifiers called GUIDs. These values are different from the unique identifiers D365 has for the same customer accounts. Levridge has implemented new tables in CDS to translate the D365 Customer account to the AgSync GUID. For the CDS environment there are two purchasing options: 1. If the ag retailer has an existing CE instance, use it. A portion of the CE instance can be firewalled off for security purposes and then it can be used to host the CDS solution. 2. Buy a specific CDS environment for only the CDS solution. This is more expensive. There are 7 types of data stored in CDS: 1. Dispatch Accounts 2. Customers 3. Customer Operations 4. Customer Sites 5. Customer Site Locations 6. Master Items 7. Workers All these types are stored differently in AgSync than they are in D365 so they must be translated using the data in CDS. Products and Employees must be manually entered into the CDS database while Dispatch Accounts, Customers, Customer Operations Customer Sites and Customer Site Locations can be populated using a script that queries the data in AgSync and populates the data in CDS. All this data must exist in CDS prior to using the work order integrations. The Master Item entity translates AgSync operations to D365 Product and Service Items. This states which task you are performing and included in every work order. - Product ID Numbers (this relates to the Master item in D365). AgSync has the ability to generate blends (ex: blending multiple ingredients to produce the fertilizer to be used on a field). There is a Master Item Table in CDS configured specifically for the operation. If there is a blend flag on an order change, it will pull from the Master Item Table. The Worker Table is a translation table required to map the applicator (worker) set up in AgSync to D365. This provides the information of who applied the product to the field and their applicator license number. An applicator license number is required on the sales order. Three entities need to be manually entered in CDS. The CDS setup includes: - CE Levridge - AgSync Solution - Choose appropriate UI Form Once the CDS solution is deployed to an environment, an Application User will need to be created.","title":"Common Data Service (CDS)"},{"location":"Agsync/#work-order-integration","text":"Below is the process to generate a work order in AgSync: - Select Customer - State where the application or service location will be completed out of - Include what work is being done - State the work order status (planned or released) - Planned: These are premade orders ready for a certain time. The action is not ready to be performed. Not all users can view planned status due to security measures in place (ex: dispatcher unable to see planned work status). - Released: The work orders are released and assigned to complete field work. - The work order comes to F&O regardless of status. - State the performed operation - State the products being utilized - Indicate the impacted crop(s) - Add in any additional notes - Save Order After a work order is saved, AgSync calls the Levridge AgsyncOrderChanged webhook sending over the work order data. The Levridge AgsyncOrderChanged webhook transforms the work order data and sends it to F&O where it creates or updates a corresponding sales order. If F&O accounts receivables are configured to perform a credit check and the credit check fails, F&O will send back a response indicating that the credit check did not pass. Then a rejection request is sent to Agsync to mark the work order as rejected. When F&O receives an updated work order from Agsync, F&O will remove all existing sales order line(s) and create new sale order line(s) to match the Agsync updated work order. This is done to keep the F&O sale order in sync with the possible changes that came through with Agsync updated work order. If a F&O sale's order has an active dispensing work order associated to it, then F&O will not delete or create new sales order line(s). An important item to note: A business process needs to be in place in the situation if AgSync creates a scheduled work order which gets scheduled, and then a dispensing work order is created and gets sent to Kahler. F&O will not pick up new changes if AgSync tries to go back to a release status and make changes and try to reschedule it. AgSync needs to cancel the Kahler order and F&O dispensing order and reschedule their AgSync work order.","title":"Work Order Integration"},{"location":"Agsync/#microsoft-azure-app-service","text":"The Levridge Integration Framework has been written as a web application that hosts HTTP endpoints as REST APIs and background processes that handle integrations. It is most commonly run in the cloud as an Azure App Serivce. It can also run as a windows service or as an IIS application. The Integration Overview provides additional information about the deployment options. Most of the setup will occur in Azure App Service. This is a requirement for AgSync in which AgSync pulls from Levridge's Azure Service Bus and an HTTP-based service for hosting web applications. Azure hosts our web app integration. The steps needed to create an App Service can be found under the Integration Overview . After the App ID has been generated along with the deployed integration code to App Service , there is an application configuration file named [appsettings.json])./appsettings.json.md). This file is located within the generated Azure Service. In the App Services portion of the Azure Portal there is a Kudos button on the left-hand side, with a function to ZipDeploy code . This is where one would deploy the integration code. After deployed, back in Azure Service and right below Kudos button, there is an App Service Editor where one would deploy the App Settings json file to, which will give a list of all the files part of the integration. One of those is called AppSettings.json where the configuration is made for the Azure Service.","title":"Microsoft Azure App Service"},{"location":"Agsync/#configuration","text":"Set up the logging level . The next section to be configured would outline what direction information is flowing within the service itself. There is a target configuration and source configuration which is needed to specify: Which system is the source system and which system is the target system. Which configuration section contains data connection information. Which section contains the service bus configuration to use. In the appsettings.json you will need to define the InstanceConfig SourceConfig and TargetConfig nodes as follows: \"InstanceConfig\": { \"AzureTableConfiguration\": \"[section name to Azure Table Configuration\", \"LogRequestsAndResponses\": [true or false] \"EnableAppInsightsAdaptiveSampling\": [true or false] }, \"SourceConfig\": { \"ServiceBusConfigName\": \"[section name with service bus topic and subscription for Agsync Master Data]\", \"ODataConfigName\": \"[section name with F&O data configuration]\", \"SystemName\": \"DynamicsAx\", \"Direction\": \"Source\" }, \"TargetConfig\": { \"ODataConfigName\": \"[section name with Agsync endpoint configuration]\", \"CDSConfigName\": \"[section name with CDS data configuration]\", \"SystemName\": \"AgSync\", \"Direction\": \"Target\", } You must also include the controller entry to have the controller loaded: \"Controllers\": { \"HostController\": \"Levridge.Integration.Host.DefaultController\", \"AgSyncConroller\": \"Levridge.Integration.Host.AgSyncController\" } You must also configure two objects for Agsync integration: AgSyncEndpoint agsync Here is a sample template for the entire appsettings.json file used for the integration from FinOps to Agsync: { \"Controllers\": { \"HostController\": \"Levridge.Integration.Host.DefaultController\", \"AgSyncConroller\": \"Levridge.Integration.Host.AgSyncController\" }, \"Logging\": { \"Debug\": { \"LogLevel\": { \"Default\": \"Information\" } }, \"Console\": { \"IncludeScopes\": true, \"LogLevel\": { \"Default\": \"Information\" } }, \"LogLevel\": { \"Default\": \"Information\" } }, \"AllowedHosts\": \"*\", \"SourceConfig\": { \"ServiceBusConfigName\": \"AgsyncMasterDataServiceBus\", \"ODataConfigName\": \"DynamicsAX\", \"SystemName\": \"DynamicsAx\", \"Direction\": \"Source\" }, \"TargetConfig\": { \"ODataConfigName\": \"AgSyncEndpoint\", \"SystemName\": \"AgSync\", \"Direction\": \"Target\", \"CDSConfigName\": \"CDS\" }, \"DynamicsAX\": { \"UriString\": \"[URL to D365 F&O]\", \"ActiveDirectoryResource\": \"[URL to D365 F&O]\", \"ActiveDirectoryTenant\": \"https://login.microsoftonline.com/[Customer_Tenant_ID]\", \"ActiveDirectoryClientAppId\": \"[Application ID used to register the application in AD]\", \"ActiveDirectoryClientAppSecret\": \"[Client Secret generated for the Application ID in AD]\", \"ODataEntityPath\": \"[URL to D365 F&O]/data/\" }, \"CDS\": { \"UriString\": \"[URL to CDS or Localhost]\", \"ActiveDirectoryResource\": \"[URL to CDS]\", \"ActiveDirectoryTenant\": \"https://login.microsoftonline.com/[Customer_Tenant_ID]\", \"ActiveDirectoryClientAppId\": \"[Application ID used to register the application in AD]\", \"ActiveDirectoryClientAppSecret\": \"[Client Secret generated for the Application ID in AD]\", \"ODataEntityPath\": \"[URL to CDS]/api/data/v9.0/\", \"AssemblyName\": \"Levridge.ODataDataSources.CDS\", \"ClientClassesNameSpace\": \"Levridge.ODataDataSources.CDS\", \"MetadataResource\": \"CDSMetadata.xml\" }, \"AgSyncEndpoint\": { \"MustUseWktProcessor\": true, \"baseUri\": \"https://fields.agsync.com/api/\", \"tokenUrl\": \"https://auth.agsync.com/core/connect/token\", \"ClientId\": \"[client ID assigned by Agsync]\", // customer specific \"ClientPass\": \"[Client Secret assigned by Agsync]\", // customer specific \"ValutURL\": \"[URL to customer Azure Key Vault]\", \"AgSyncTokenKey\": \"AgsyncAccessToken\", \"RedirectUri\": \"[URL to AgsyncAuth controller]\", // customer specific \"IntegrationId\": \"CustomerIntegrationID\" // customer specific }, \"agsync\": { // used by Webhook \"MustUseWktProcessor\": true, \"ConnectionString\": \"[connection string to Agsync master data topic]\", \"TopicName\": \"[Agsync master data topic]\", \"RequiresSession\": true, \"RedirectUri\": \"[URL to AgsyncAuth controller]\", \"TokenUrl\": \"https://auth.agsync.com/core/connect/token\", \"AuthorizeUrl\": \"https://auth.agsync.com/core/connect/authorize\", \"baseUri\": \"https://fields.agsync.com/api/\", \"ClientId\": \"[client ID assigned by Agsync]\", \"ClientPass\": \"[Client Secret assigned by Agsync]\", \"ValutURL\": \"[URL to customer Azure Key Vault]\", \"AgSyncTokenKey\": \"AgsyncAccessToken\" }, \"AgsyncMasterDataServiceBus\": { \"ConnectionString\": \"[connection string to Agsync master data topic]\", \"TopicName\": \"[Agsync master data topic]\", \"SubscriptionName\": \"[Agsync master data subscription name]\", \"RequiresSession\": true } }","title":"Configuration"},{"location":"Agsync/#microsoft-azure-service-bus","text":"Microsoft Azure Service Bus is a message bus for businesses to exchange documents and messages in the Cloud. There are two main tiers: - Standard Service Bus: Supports 250 Kilobytes of data - Premium Service Bus: Supports 1 Megabyte of data Most Levridge clients will be able to utilize the Standard Service Bus tier, however, if a client is integrating well-known-text (WKT) files with their work orders it may push the workorder size over the 250K data limit and they should look at upgrading to Premium Service Bus. Microsoft outlines how to create an Azure Service bus . The Azure Key Vault is a tool for securely storing and accessing secrets. A secret is anything that you want to tightly control access to, such as API keys, passwords, or certificates. The vault enables us to securely store passwords, certificates, etc. with secure access. The key vault stores the authorization credentials for AgSync and by providing the secret to AgSync it lets AgSync know it is Levridge talking to them. This is required for all AgSync integrations. One can read how to setup the Azure Key Vault . When customer entities are sent from D365 to the Azure service bus, the agsync integration queries CDS to find the stored Customer ID. If there is no matching customer record the record is sent as a Create to AgSync. The customer is created in AgSync and the response from Agsync contains the Agsync specific ID for that customer. The newly generated AgSync ID is saved in combination with the D365 customer account in CDS. If CDS did have an existing record for the D365 customer, the message is updated to include the Agsync ID for that customer and the messages is sent to Agsync as an Update. Once a sales order is created in F&O, we will message back to AgSync and update them with the sales order number and the dispensing work order number (if available at the time of creation). This requires an entity setup in the event framework. Integration purchase requirements include: 1. CDS Instance - CE License 2. Azure Subscription - Service Bus - App Service - Key Vault 3. AgSync Subscription","title":"Microsoft Azure Service Bus"},{"location":"Agsync/#setup","text":"Internal setup time for configuration is 24 hours. To integrate from D365 F&O to Agsync you will need to: Create an Azure Service bus topic Create a subscription on the topic above Configure Event Endpoint in F&O Configure Levridge Entity Events Create Filter on Entity Event to only send agronomy customers Get Client ID and CLient password from Agsync Get Customer Specific Integration ID from Agsync Client Redirect URL is [Azure Webapp base URL]/api/AgsyncAuth Setup Azure Keyvault Create an application ID for the integration framework to authenticate to D365 CE Create an application user in D365 CRM and assign the proper role(s) A webhook is an API provided by the Levridge Integration Framework that gives AgSync the ability to send workorders to Levridge. AgSync sends workorders to the Levridge API which places the order on the service bus. The customer's specific URL needs to be whitelisted by AgSync. To whitelist a URL, one would log into AgSync and create a helpdesk ticket requesting AgSync to add the AgsyncOrderChanged URL into their setup file. The expected service time is between 24-48 hours. Escalate after 48 hours. General setup timelines: - Deploying code to App Service takes the longest. 400 table lines for all three. - The worker table is refreshed once a quarter - Service item data is refreshed once a year - Master item data is refreshed once every three year Data Moving Within the Integration: There are 4 types of master records sent from D365 to AgSync: 1. Customers 2. Customer Operations 3. Customer Sites 4. LevAgSyncSalesDetailEntity - The LevAgSyncSalesDetailEntity stores sales, order number, dispensing work order numbers, and a unique work order ID that identifies the AgSync work order that needs to update. A Dispensing Account ID field has been added in F&O on the customer account under the sales order tab. Its purpose is to indicate which dispensing branch the customer receives services from. The dispensing branch within AgSync can be set up as either: - An inventory site indicating where the product is physically located - Commission/Sales groups indicating the agronomist that sold the service to the customer (Confirm and clarify this statement)","title":"Setup"},{"location":"Agsync/#integration-framework-setup-for-sending-customers","text":"\"Out of the Box Customer V3\" is being utilized and the code in the integration is choosing the data fields the entities pass through. The Dispatching Account ID is included on the customer form to only send accounts that have a dispatching account tied to it to AgSync. This is filtered by the Event Framework V3 Entity. An example is a client might have 100K in their database but for work order purposes there are only 30K that will ever work with AgSync. Filtering is again used in the Event Framework V3 Entity in the Customer Operation. This is a more sophisticated filter due to multiple types of operations an individual customer could have. Customer Operation is setup by four different filters: 1. Customer Operation Type 2. A filter is added so customer information which do not have a site is not sent. - Example: Customer operations will not be sent to AgSync if they do not have sites tied to them. 3. Customer Site Types 4. Dispensing Site ID","title":"Integration Framework Setup for Sending Customers"},{"location":"Agsync/#release-status-process","text":"A work order is created at release status. Take release in AgSync and schedule work order. (Scheduling is assigning it out to an applicator to be applied). Dispatcher is going to see all released orders. Dispatcher takes orders, creates task packet, assigns to worker, annotates which piece of application equipment is going to be running, and saves as scheduled packet. The message is once again generated to webhook in F&O. Status is updated to \"scheduled\" if a blend. This creates an automatically explode bomb at scheduled date. (ex: if a client is using automatic fertilizer, dispensing ticket is released at the scheduled state which kicks off another integration to Kahler, so they can blend and send product out to field that is going to be applied.)","title":"Release Status Process"},{"location":"Agsync/#controllers","text":"","title":"Controllers"},{"location":"Agsync/#agsync-auth-controller","text":"The AgsyncAuth controller is used to generate a token needed to integrate with Agsync.","title":"Agsync Auth Controller"},{"location":"Agsync/#agsync-auth-test-controller","text":"The AgsyncAuthTest controller is used to ...","title":"Agsync Auth Test Controller"},{"location":"Agsync/#agsync-order-changed-controller","text":"The AgsyncOrderChanged controller is used by Agsync to send work orders as they are created or updated. This controller will bundle the work order into a message and place it in the message topic.","title":"Agsync Order Changed Controller"},{"location":"Agsync/#agsync-sync-accounts-controller","text":"The AgsyncSyncAccounts controller is used to query Agsync for master data and write the information into CDS. This is done during go-live to populate the lookup data in CDS.","title":"Agsync Sync Accounts Controller"},{"location":"Agsync/#agsync-uuid-controller","text":"The AgsyncUUID controller provides UUIDs based on Sync Ids passed to the controller. This is used by Field Reveal to obtain the UUID from the Sync ID entered into Field Reveal.","title":"Agsync UUID Controller"},{"location":"AgsyncUUID/","text":"AgsyncUUID Controller The AgsyncUUID controller returns Agsync UUID values based on SyncIds provided to the controller. Overview The AgsyncUUID controller has one GET method and two POST methods. GET Method The GET method in the form of [BaseURL]/api/AgsyncUUID/[recordType]/[id]. The valid record types are: - Account - Grower - Farm - Field POST Methods The two POST methods both take a UuidCompositeRequest and return UuidCompositeResponse as a response. One URL accepts a record type (one of the valid strings above) in the form of [BaseURL]/api/AgsyncUUID/[recordType] with a UuidCompositeRequest in the body. This action will populate the UuidCompositeResponse in a hierarchical manner. For example, if the record type is \"Account\" only the account values will be returned. If the record type is \"Grower\" then the grower and account values will be returned. If the record type is \"Field\" then all record values will be returned. The second POST does not take a record type. It will simply take a UuidCompositeRequest in the body and will populate the values for any non-empty ID. For example, if the UuidCompositeRequest.AccountId and the UuidCompositeRequest.FieldId have values their respective identifying values will be returned in the UuidCompositeResponse . If there are any errors, they will always be reported in the UuidCompositResponse.FieldName value.","title":"AgsyncUUID Controller"},{"location":"AgsyncUUID/#agsyncuuid-controller","text":"The AgsyncUUID controller returns Agsync UUID values based on SyncIds provided to the controller.","title":"AgsyncUUID Controller"},{"location":"AgsyncUUID/#overview","text":"The AgsyncUUID controller has one GET method and two POST methods.","title":"Overview"},{"location":"AgsyncUUID/#get-method","text":"The GET method in the form of [BaseURL]/api/AgsyncUUID/[recordType]/[id]. The valid record types are: - Account - Grower - Farm - Field","title":"GET Method"},{"location":"AgsyncUUID/#post-methods","text":"The two POST methods both take a UuidCompositeRequest and return UuidCompositeResponse as a response. One URL accepts a record type (one of the valid strings above) in the form of [BaseURL]/api/AgsyncUUID/[recordType] with a UuidCompositeRequest in the body. This action will populate the UuidCompositeResponse in a hierarchical manner. For example, if the record type is \"Account\" only the account values will be returned. If the record type is \"Grower\" then the grower and account values will be returned. If the record type is \"Field\" then all record values will be returned. The second POST does not take a record type. It will simply take a UuidCompositeRequest in the body and will populate the values for any non-empty ID. For example, if the UuidCompositeRequest.AccountId and the UuidCompositeRequest.FieldId have values their respective identifying values will be returned in the UuidCompositeResponse . If there are any errors, they will always be reported in the UuidCompositResponse.FieldName value.","title":"POST Methods"},{"location":"ApplicationConfiguration/","text":"Application Configuration The Levridge Integration Framework utilizes standard Microsoft Configuration for ASP.NET Core to mange configuration settings. This document explains what configuration settings are managed and the definition, location and valid options for each setting. Overview There are five sources that are utilized for configuration data. Those sources are: web.config file command-line arguments hostsettings.json file Environment variables appsettings.json file Azure Key Vault","title":"Application Settings Configuration"},{"location":"ApplicationConfiguration/#application-configuration","text":"The Levridge Integration Framework utilizes standard Microsoft Configuration for ASP.NET Core to mange configuration settings. This document explains what configuration settings are managed and the definition, location and valid options for each setting.","title":"Application Configuration"},{"location":"ApplicationConfiguration/#overview","text":"There are five sources that are utilized for configuration data. Those sources are: web.config file command-line arguments hostsettings.json file Environment variables appsettings.json file Azure Key Vault","title":"Overview"},{"location":"AssignItemToProcurementCategory/","text":"Assign an Item to a Procurement Category Brief introduction of the module, component or feature being documented. This document explains ... How to Assign an Item to a Procurement Category Go to Product Information Management > Products > Released Products. In the list, find and select the desired record. Click Product categories. Click Edit. In the Category field, enter or select a value. In the tree, select 'ALL (New Category)\\Farm Supplies (New Category)'. Click OK. Click Save. In the Category field, enter or select a value. In the tree, select 'ALL (New Category)\\Natural Gas (New Category)'. Click OK. Click Save. Close the page. Close the page. Go to Procurement and Sourcing > Procurement Categories. In the tree, expand 'Landus\\Corporate'. In the tree, select 'Landus\\Corporate\\Equipment lease'. Click Add. In the list, find and select the desired record. In the list, find and select the desired record. Click Add. Click OK. Click Save. Close the page. Go to Product Information Management > Products > Released Products. Use the Quick Filter to find records. For example, filter on the Item number field with a value of '110.1' Click Product Categories.","title":"Assign Item to a Procurement Category"},{"location":"AssignItemToProcurementCategory/#assign-an-item-to-a-procurement-category","text":"Brief introduction of the module, component or feature being documented. This document explains ...","title":"Assign an Item to a Procurement Category"},{"location":"AssignItemToProcurementCategory/#how-to-assign-an-item-to-a-procurement-category","text":"Go to Product Information Management > Products > Released Products. In the list, find and select the desired record. Click Product categories. Click Edit. In the Category field, enter or select a value. In the tree, select 'ALL (New Category)\\Farm Supplies (New Category)'. Click OK. Click Save. In the Category field, enter or select a value. In the tree, select 'ALL (New Category)\\Natural Gas (New Category)'. Click OK. Click Save. Close the page. Close the page. Go to Procurement and Sourcing > Procurement Categories. In the tree, expand 'Landus\\Corporate'. In the tree, select 'Landus\\Corporate\\Equipment lease'. Click Add. In the list, find and select the desired record. In the list, find and select the desired record. Click Add. Click OK. Click Save. Close the page. Go to Product Information Management > Products > Released Products. Use the Quick Filter to find records. For example, filter on the Item number field with a value of '110.1' Click Product Categories.","title":"How to Assign an Item to a Procurement Category"},{"location":"AssignPostingProfiletoProcurementCategory/","text":"Assign Posting Profile to Procurement Category Brief introduction of the module, component or feature being documented. This document explains ... How to Assign a Posting Profile to a Procurement Category Go to Inventory Management > Setup > Posting > Posting. Click the Purchase Order tab. In the Select field, select an option. Click New. In the list, mark the selected row. In the Item code field, select an option. In the Category Relation field, enter or select a value. In the tree, expand 'Landus (New category)\\Corporate (Corporate)'. In the tree, select 'Landus (New category)\\Corporate (Corporate)\\Office Supplies (New category)'. Click OK. In the list, find and select the desired record. In the Category relation field, enter or select a value. In the tree, select 'Landus (New Category)\\Animal Nutrition (Animal Nutrition)'. Click OK. In the Main account field, specify the desired values. Click Save. In the list, find and select the desired record. Close the page.","title":"Assign Posting Profile to Procuremnet Category"},{"location":"AssignPostingProfiletoProcurementCategory/#assign-posting-profile-to-procurement-category","text":"Brief introduction of the module, component or feature being documented. This document explains ...","title":"Assign Posting Profile to Procurement Category"},{"location":"AssignPostingProfiletoProcurementCategory/#how-to-assign-a-posting-profile-to-a-procurement-category","text":"Go to Inventory Management > Setup > Posting > Posting. Click the Purchase Order tab. In the Select field, select an option. Click New. In the list, mark the selected row. In the Item code field, select an option. In the Category Relation field, enter or select a value. In the tree, expand 'Landus (New category)\\Corporate (Corporate)'. In the tree, select 'Landus (New category)\\Corporate (Corporate)\\Office Supplies (New category)'. Click OK. In the list, find and select the desired record. In the Category relation field, enter or select a value. In the tree, select 'Landus (New Category)\\Animal Nutrition (Animal Nutrition)'. Click OK. In the Main account field, specify the desired values. Click Save. In the list, find and select the desired record. Close the page.","title":"How to Assign a Posting Profile to a Procurement Category"},{"location":"AzureAd/","text":"AzureAd Settings Optional Example \"AzureAd\": { \"Instance\": \"https://login.microsoftonline.com/\", \"Domain\": \"stoneridgesoftware.com\", \"TenantId\": \"[Tenant ID GUID]\", \"ClientId\": \"[Client ID GUID]\", \"CallbackPath\": \"/signin-oidc\", \"SignedOutCallbackPath \": \"/signout-callback-oidc\" } Definition AzureAd Instance Required No Default value The azure cloud instance to use for authentication. Most of the time this value should be \"https://login.microsoftonline.com/\" for the Azure public cloud. There may be a use for country specific instances such as \"https://login.microsoftonline.de/\" for Azure AD Germany. Domain Required No Default value The domain of the AD tenant used for authentication. This may be your domain (i.e. stoneridgesoftware.com) or it may be sub-domain of onmicrosoft. (i.e. contoso.onmicrosoft.com) TenantId Required No Default value The TenantId (aka audience) that will be used for authentication. The following values are valid: \"TenantId\" as a GUID obtained from the Azure portal to sign in users in your organization \"organizations\" to sign in users in any work or school account \"common\" to sign in users with any work or school account or Microsoft personal account \"consumers\" to sign in users with a Microsoft personal account only ClientId Required No Default value The Client ID (aka application ID) assigned in the Azure Portal. This client ID is obtained by enabling Authentication and Authorization in the Azure Portal. Once Authentication is enabled you can obtain the ClientId from the Authentication / Authorization section of the app service. Select Azure Active Directory and then select the Azure AD App. CallbackPath Optional Default = false SignedOutCallbackPath Optional Default = false","title":"AzureAd Settings"},{"location":"AzureAd/#azuread-settings","text":"Optional","title":"AzureAd Settings"},{"location":"AzureAd/#example","text":"\"AzureAd\": { \"Instance\": \"https://login.microsoftonline.com/\", \"Domain\": \"stoneridgesoftware.com\", \"TenantId\": \"[Tenant ID GUID]\", \"ClientId\": \"[Client ID GUID]\", \"CallbackPath\": \"/signin-oidc\", \"SignedOutCallbackPath \": \"/signout-callback-oidc\" }","title":"Example"},{"location":"AzureAd/#definition","text":"","title":"Definition"},{"location":"AzureAd/#azuread","text":"","title":"AzureAd"},{"location":"AzureAd/#instance","text":"Required No Default value The azure cloud instance to use for authentication. Most of the time this value should be \"https://login.microsoftonline.com/\" for the Azure public cloud. There may be a use for country specific instances such as \"https://login.microsoftonline.de/\" for Azure AD Germany.","title":"Instance"},{"location":"AzureAd/#domain","text":"Required No Default value The domain of the AD tenant used for authentication. This may be your domain (i.e. stoneridgesoftware.com) or it may be sub-domain of onmicrosoft. (i.e. contoso.onmicrosoft.com)","title":"Domain"},{"location":"AzureAd/#tenantid","text":"Required No Default value The TenantId (aka audience) that will be used for authentication. The following values are valid: \"TenantId\" as a GUID obtained from the Azure portal to sign in users in your organization \"organizations\" to sign in users in any work or school account \"common\" to sign in users with any work or school account or Microsoft personal account \"consumers\" to sign in users with a Microsoft personal account only","title":"TenantId"},{"location":"AzureAd/#clientid","text":"Required No Default value The Client ID (aka application ID) assigned in the Azure Portal. This client ID is obtained by enabling Authentication and Authorization in the Azure Portal. Once Authentication is enabled you can obtain the ClientId from the Authentication / Authorization section of the app service. Select Azure Active Directory and then select the Azure AD App.","title":"ClientId"},{"location":"AzureAd/#callbackpath","text":"Optional Default = false","title":"CallbackPath"},{"location":"AzureAd/#signedoutcallbackpath","text":"Optional Default = false","title":"SignedOutCallbackPath"},{"location":"AzureKeyVault/","text":"Introduction Azure Key Vault Overview","title":"Introduction"},{"location":"AzureKeyVault/#introduction","text":"Azure Key Vault Overview","title":"Introduction"},{"location":"AzureTableEntityConfiguration/","text":"AzureTableEntityConfiguration Brief introduction of the module, component or feature being documented. This document explains ... Overview Main Point 1 Sub Point 1.1","title":"AzureTableEntityConfiguration"},{"location":"AzureTableEntityConfiguration/#azuretableentityconfiguration","text":"Brief introduction of the module, component or feature being documented. This document explains ...","title":"AzureTableEntityConfiguration"},{"location":"AzureTableEntityConfiguration/#overview","text":"","title":"Overview"},{"location":"AzureTableEntityConfiguration/#main-point-1","text":"","title":"Main Point 1"},{"location":"AzureTableEntityConfiguration/#sub-point-11","text":"","title":"Sub Point 1.1"},{"location":"BaseFinanceProgram/","text":"Create a Base Finance Program This document explains how to create a base Finance Program. Overview Go to Accounts receivable > Setup > Agriculture > Finance programs. Click New. In the Name field, type a value. In the Description field, type a value. Click to follow the link in the Method of payment field. (If establishing a NEW method, otherwise select the appropriate one) a. Depending on how payments are received/processed, as well as bank configuration, there will need to be some thought given to how these should be established. Period, Payment status, Bank transaction type, file formats, payment control and payment attributes should all be considered. Click New. (Minimum setup required, but should be established in more detail & consideration) In the Method of payment field, type a value. In the Description field, type a value. In the Account type field, select an option. Click Save. Close the page. In the Method of payment field, enter or select a value. If desired, in the address section: Click Add. In the Name or description field, type a value. In the City field, type a value. In the State field, type a value. In the City field, enter or select a value. In the ZIP/postal code field, enter or select a value. In the Street field, enter a value. Click OK. In the Contact section, if desired : Click Add. In the Description field, type a relevant value such as the finance prog mgr name In the Contact number/address field, type a relevant value like '641-555-1999'. Click Add. In the Description field, type a relevant value such as the Finance prog mgr email In the Type field, select 'Email address'. In the Contact number/address field, type a relevant value like 'Test@124.com'. Click Save.","title":"Create a Base Finance Program"},{"location":"BaseFinanceProgram/#create-a-base-finance-program","text":"This document explains how to create a base Finance Program.","title":"Create a Base Finance Program"},{"location":"BaseFinanceProgram/#overview","text":"Go to Accounts receivable > Setup > Agriculture > Finance programs. Click New. In the Name field, type a value. In the Description field, type a value. Click to follow the link in the Method of payment field. (If establishing a NEW method, otherwise select the appropriate one) a. Depending on how payments are received/processed, as well as bank configuration, there will need to be some thought given to how these should be established. Period, Payment status, Bank transaction type, file formats, payment control and payment attributes should all be considered. Click New. (Minimum setup required, but should be established in more detail & consideration) In the Method of payment field, type a value. In the Description field, type a value. In the Account type field, select an option. Click Save. Close the page. In the Method of payment field, enter or select a value. If desired, in the address section: Click Add. In the Name or description field, type a value. In the City field, type a value. In the State field, type a value. In the City field, enter or select a value. In the ZIP/postal code field, enter or select a value. In the Street field, enter a value. Click OK. In the Contact section, if desired : Click Add. In the Description field, type a relevant value such as the finance prog mgr name In the Contact number/address field, type a relevant value like '641-555-1999'. Click Add. In the Description field, type a relevant value such as the Finance prog mgr email In the Type field, select 'Email address'. In the Contact number/address field, type a relevant value like 'Test@124.com'. Click Save.","title":"Overview"},{"location":"BasicCustomerPriceSetup/","text":"Basic Customer Price Setup Brief introduction of the module, component or feature being documented. This document explains ... Basic Customer Price Setup Go to Product information management > Products > Released products. In the list, click the link in the selected row. Click Edit. In the Price field, enter a number. Click Save. Close the page. Go to Accounts receivable > Orders > All sales orders. In the list, find and select the desired record. In the list, click the link in the selected row. Click Remove. Click Yes. Click Add line. In the Item number field, type a value. In the Site field, type a value. In the Warehouse field, type a value.","title":"Basic Customer Price Setup"},{"location":"BasicCustomerPriceSetup/#basic-customer-price-setup","text":"Brief introduction of the module, component or feature being documented. This document explains ...","title":"Basic Customer Price Setup"},{"location":"BasicCustomerPriceSetup/#basic-customer-price-setup_1","text":"Go to Product information management > Products > Released products. In the list, click the link in the selected row. Click Edit. In the Price field, enter a number. Click Save. Close the page. Go to Accounts receivable > Orders > All sales orders. In the list, find and select the desired record. In the list, click the link in the selected row. Click Remove. Click Yes. Click Add line. In the Item number field, type a value. In the Site field, type a value. In the Warehouse field, type a value.","title":"Basic Customer Price Setup"},{"location":"BasicVendorPriceWithPriceUpdate/","text":"Basic Vendor Price with Price Update Brief introduction of the module, component or feature being documented. This document explains ... Basic Vendor Price with Price Update Go to Product Information Management > Products > Released Products. In the list, click the link in the selected row. Click Edit. Select Yes in the Latest pruchase price field. In the Price field, enter a number. Click Save. Close the page. Go to Procurement and Sourcing > Puchase Orders > All purchase orders. In the list, find and select the desired record. In the list, click the link in the selected row. Click Remove. Click Yes. Click Add line. In the Item number field, type a value. In the list, mark the selected row. In the unit price field, enter a number. Click Save. On the Action Pane, click Purchase. Click Confirm. On the Action Pane, click Receive. Click Product Receipt. In the list, mark the selected row. Open Product receipt column filter. Sort A to Z In the Product receipt field, type a value. Click OK. On the Action Pane, click Invoice. Click Invoice. In the Number field, type a value. Click Post. Click Update match status. Click Post. Close the page. Close the page. Go to Product Information Management > Products > Released Products. In the list, click the link in the selected row.","title":"Basic Vendor Price with Price Update"},{"location":"BasicVendorPriceWithPriceUpdate/#basic-vendor-price-with-price-update","text":"Brief introduction of the module, component or feature being documented. This document explains ...","title":"Basic Vendor Price with Price Update"},{"location":"BasicVendorPriceWithPriceUpdate/#basic-vendor-price-with-price-update_1","text":"Go to Product Information Management > Products > Released Products. In the list, click the link in the selected row. Click Edit. Select Yes in the Latest pruchase price field. In the Price field, enter a number. Click Save. Close the page. Go to Procurement and Sourcing > Puchase Orders > All purchase orders. In the list, find and select the desired record. In the list, click the link in the selected row. Click Remove. Click Yes. Click Add line. In the Item number field, type a value. In the list, mark the selected row. In the unit price field, enter a number. Click Save. On the Action Pane, click Purchase. Click Confirm. On the Action Pane, click Receive. Click Product Receipt. In the list, mark the selected row. Open Product receipt column filter. Sort A to Z In the Product receipt field, type a value. Click OK. On the Action Pane, click Invoice. Click Invoice. In the Number field, type a value. Click Post. Click Update match status. Click Post. Close the page. Close the page. Go to Product Information Management > Products > Released Products. In the list, click the link in the selected row.","title":"Basic Vendor Price with Price Update"},{"location":"BasicVendorPurchasePrice/","text":"Basic Vendor Purchase Price Brief introduction of the module, component or feature being documented. This document explains ... Basic Vendor Purchase Price Go to Product information management > Products > Released products. In the list, click the link in the selected row. Click Edit. In the Price field, enter a number. Click Save. Close the page. Go to Procurement and sourcing > Purchase orders > All purchase orders. Click New. In the Vendor account field, type a value. In the Warehouse field, type a value. Click OK. In the Item number field, type a value.","title":"Basic Vendor Purchase Price"},{"location":"BasicVendorPurchasePrice/#basic-vendor-purchase-price","text":"Brief introduction of the module, component or feature being documented. This document explains ...","title":"Basic Vendor Purchase Price"},{"location":"BasicVendorPurchasePrice/#basic-vendor-purchase-price_1","text":"Go to Product information management > Products > Released products. In the list, click the link in the selected row. Click Edit. In the Price field, enter a number. Click Save. Close the page. Go to Procurement and sourcing > Purchase orders > All purchase orders. Click New. In the Vendor account field, type a value. In the Warehouse field, type a value. Click OK. In the Item number field, type a value.","title":"Basic Vendor Purchase Price"},{"location":"CDSConfig/","text":"","title":"CDSConfig"},{"location":"CE-configuration/","text":"Configuration Setup CE To begin configuration setup in CE, first import LevCore Solution followed by importing Agronomy Solution. Migrate the ESG Configuration data using the Configuration Migration Tool. This tool can be downloaded at https://docs.microsoft.com/en-us/dynamics365/customer-engagement/developer/download-tools-nuget Ensure an Application User has been created with admin security roles assigned. If integrating records from CE to AX, you will need to create steps on the service endpoint using the Plugin Registration tool. Steps will also need to be created on plugins which require the pricing service. The configuration will look like this: { \"clientappid\": \"[Client AppID from AD]\", \"clientappsecret\": \"[secret from AD]\", \"tenant\": \"https://login.microsoftonline.com/5555a5b1-fbt8-465b-ad9d-21e21129e610/oauth2/token\", \"uristring\": \"https://[environment subdomain].cloudax.dynamics.com/api/services/LevPricingServices/PricingService/getPricing\", \"resource\": \"https://[environment subdomain].cloudax.dynamics.com\", \"username\": \"John.smith@email.com\", \"password\": \"plaintextpassword\" } Setup the data through integrations, create it in CE, or import data packets. Entities in pink must be set up in CE. Entities in yellow will integrate over from FinOps. Entities in green will integrate from Agsync. Once item categories have been either created or imported in, the filtered xmls on the Plans and Batch Plans will need to be updated to reflect the item category GUIDs within your environment. The Proposal OOB Proposal line Subgrids will also require filter updates to reflect the Item Category's in your environment.","title":"CE Platform Configuration"},{"location":"CE-configuration/#configuration-setup-ce","text":"To begin configuration setup in CE, first import LevCore Solution followed by importing Agronomy Solution. Migrate the ESG Configuration data using the Configuration Migration Tool. This tool can be downloaded at https://docs.microsoft.com/en-us/dynamics365/customer-engagement/developer/download-tools-nuget Ensure an Application User has been created with admin security roles assigned. If integrating records from CE to AX, you will need to create steps on the service endpoint using the Plugin Registration tool. Steps will also need to be created on plugins which require the pricing service. The configuration will look like this: { \"clientappid\": \"[Client AppID from AD]\", \"clientappsecret\": \"[secret from AD]\", \"tenant\": \"https://login.microsoftonline.com/5555a5b1-fbt8-465b-ad9d-21e21129e610/oauth2/token\", \"uristring\": \"https://[environment subdomain].cloudax.dynamics.com/api/services/LevPricingServices/PricingService/getPricing\", \"resource\": \"https://[environment subdomain].cloudax.dynamics.com\", \"username\": \"John.smith@email.com\", \"password\": \"plaintextpassword\" } Setup the data through integrations, create it in CE, or import data packets. Entities in pink must be set up in CE. Entities in yellow will integrate over from FinOps. Entities in green will integrate from Agsync. Once item categories have been either created or imported in, the filtered xmls on the Plans and Batch Plans will need to be updated to reflect the item category GUIDs within your environment. The Proposal OOB Proposal line Subgrids will also require filter updates to reflect the Item Category's in your environment.","title":"Configuration Setup CE"},{"location":"Class.Method.Template/","text":"ClassName.MethodName Method Namespace: [Enter Namespace Name] Assemblies: [Enter Assembly FileName] Describe the method here Overloads Overload Description MethodName (ParameterType1 Parameter1) description of overload MethodName (ParameterType1 Parameter1, ParameterType2 Parameter2) description of overload MethodName (ParameterType1 Parameter1) description of overload public Return MethodName<TA, TB>(ParameterType1 Parameter1); Parameters Returns Exceptions Examples MethodName (ParameterType1 Parameter1, ParameterType2 Parameter2) description of overload public Return MethodName<TA, TB>(ParameterType1 Parameter1, ParameterType2 Parameter2); Parameters Returns Exceptions Examples","title":"ClassName.MethodName Method"},{"location":"Class.Method.Template/#classnamemethodname-method","text":"Namespace: [Enter Namespace Name] Assemblies: [Enter Assembly FileName] Describe the method here","title":"ClassName.MethodName Method"},{"location":"Class.Method.Template/#overloads","text":"Overload Description MethodName (ParameterType1 Parameter1) description of overload MethodName (ParameterType1 Parameter1, ParameterType2 Parameter2) description of overload","title":"Overloads"},{"location":"Class.Method.Template/#methodnameparametertype1-parameter1","text":"description of overload public Return MethodName<TA, TB>(ParameterType1 Parameter1);","title":"MethodName(ParameterType1 Parameter1)"},{"location":"Class.Method.Template/#parameters","text":"","title":"Parameters"},{"location":"Class.Method.Template/#returns","text":"","title":"Returns"},{"location":"Class.Method.Template/#exceptions","text":"","title":"Exceptions"},{"location":"Class.Method.Template/#examples","text":"","title":"Examples"},{"location":"Class.Method.Template/#methodnameparametertype1-parameter1-parametertype2-parameter2","text":"description of overload public Return MethodName<TA, TB>(ParameterType1 Parameter1, ParameterType2 Parameter2);","title":"MethodName(ParameterType1 Parameter1, ParameterType2 Parameter2)"},{"location":"Class.Method.Template/#parameters_1","text":"","title":"Parameters"},{"location":"Class.Method.Template/#returns_1","text":"","title":"Returns"},{"location":"Class.Method.Template/#exceptions_1","text":"","title":"Exceptions"},{"location":"Class.Method.Template/#examples_1","text":"","title":"Examples"},{"location":"CloseInventory/","text":"Close Inventory Brief introduction of the module, component or feature being documented. This document explains ... How to Close Inventory Go to Inventory Management > Periodic Tasks > Closing and Adjustment. Click Close Procedure. Click Close Inventory. Expand the Run in the background section. In the Close Inventory up to field, enter a date. Click OK. Click Yes. Click Cancel.","title":"Close Inventory"},{"location":"CloseInventory/#close-inventory","text":"Brief introduction of the module, component or feature being documented. This document explains ...","title":"Close Inventory"},{"location":"CloseInventory/#how-to-close-inventory","text":"Go to Inventory Management > Periodic Tasks > Closing and Adjustment. Click Close Procedure. Click Close Inventory. Expand the Run in the background section. In the Close Inventory up to field, enter a date. Click OK. Click Yes. Click Cancel.","title":"How to Close Inventory"},{"location":"CommandLineParameters/","text":"Introduction Brief introduction of the module, component or feature being documented. This document explains ... Overview Main Point 1 Sub Point 1.1","title":"Introduction"},{"location":"CommandLineParameters/#introduction","text":"Brief introduction of the module, component or feature being documented. This document explains ...","title":"Introduction"},{"location":"CommandLineParameters/#overview","text":"","title":"Overview"},{"location":"CommandLineParameters/#main-point-1","text":"","title":"Main Point 1"},{"location":"CommandLineParameters/#sub-point-11","text":"","title":"Sub Point 1.1"},{"location":"Commissions/","text":"Commissions Brief introduction of the module, component or feature being documented. This document explains ... Commissions Go to Sales and marketing > Commissions > Commission Calculation. Click New. In the item relation field, enter or select a value. In the customer relation field, enter or select a value. In the sales rep. relation field, enter or select a value. In the Discount field, select an option. In the commission percentage field, enter a number. In the From field, enter a date. In the To field, enter a date. Click Save","title":"Commissions"},{"location":"Commissions/#commissions","text":"Brief introduction of the module, component or feature being documented. This document explains ...","title":"Commissions"},{"location":"Commissions/#commissions_1","text":"Go to Sales and marketing > Commissions > Commission Calculation. Click New. In the item relation field, enter or select a value. In the customer relation field, enter or select a value. In the sales rep. relation field, enter or select a value. In the Discount field, select an option. In the commission percentage field, enter a number. In the From field, enter a date. In the To field, enter a date. Click Save","title":"Commissions"},{"location":"Commodities/","text":"Introduction Brief introduction of the module, component or feature being documented. This document explains ... Overview Main Point 1 Sub Point 1.1","title":"Commodity Accounting"},{"location":"Commodities/#introduction","text":"Brief introduction of the module, component or feature being documented. This document explains ...","title":"Introduction"},{"location":"Commodities/#overview","text":"","title":"Overview"},{"location":"Commodities/#main-point-1","text":"","title":"Main Point 1"},{"location":"Commodities/#sub-point-11","text":"","title":"Sub Point 1.1"},{"location":"ConfigurationTemplate/","text":"Custom Mapping Assemblies Config Settings Example { ConfigEntry } Definition ConfigEntry Not Required Default:","title":"Custom Mapping Assemblies Config Settings"},{"location":"ConfigurationTemplate/#custom-mapping-assemblies-config-settings","text":"","title":"Custom Mapping Assemblies Config Settings"},{"location":"ConfigurationTemplate/#example","text":"{ ConfigEntry }","title":"Example"},{"location":"ConfigurationTemplate/#definition","text":"","title":"Definition"},{"location":"ConfigurationTemplate/#configentry","text":"Not Required Default:","title":"ConfigEntry"},{"location":"ConfigureAuthentication/","text":"Configure Authentication Register your application To register your application and manually add the app's registration information to your solution, follow these steps: Sign in to the Azure portal using either a work or school account, or a personal Microsoft account. If your account gives you access to more than one tenant, select your account in the top right corner, and set your portal session to the desired Azure AD tenant. Navigate to the Microsoft identity platform for developers App registrations page. Select New registration. When the Register an application page appears, enter your application's registration information: In the Name section, enter a meaningful application name that will be displayed to users of the app, for example AspNetCore-Quickstart. In Redirect URI, add https://localhost:44321/, and select Register. Select the Authentication menu, and then add the following information: In Redirect URIs, add https://localhost:44321/signin-oidc, and select Save. In the Advanced settings section, set Logout URL to https://localhost:44321/signout-oidc. Under Implicit grant, check ID tokens. Select Save. Create a section in Appsettings.json named AzureAd . \"AzureAd\": { \"Instance\": \"https://login.microsoftonline.com/\", \"Domain\": \"yourdomain.com\", \"TenantId\": \"00000000-0000-0000-0000-000000000000\", \"ClientId\": \"00000000-0000-0000-0000-000000000000\", \"CallbackPath\": \"/signin-oidc\" } Taken from Quickstart: Add sign-in with Microsoft to an ASP.NET Core web app .","title":"Configure Authentication"},{"location":"ConfigureAuthentication/#configure-authentication","text":"","title":"Configure Authentication"},{"location":"ConfigureAuthentication/#register-your-application","text":"To register your application and manually add the app's registration information to your solution, follow these steps: Sign in to the Azure portal using either a work or school account, or a personal Microsoft account. If your account gives you access to more than one tenant, select your account in the top right corner, and set your portal session to the desired Azure AD tenant. Navigate to the Microsoft identity platform for developers App registrations page. Select New registration. When the Register an application page appears, enter your application's registration information: In the Name section, enter a meaningful application name that will be displayed to users of the app, for example AspNetCore-Quickstart. In Redirect URI, add https://localhost:44321/, and select Register. Select the Authentication menu, and then add the following information: In Redirect URIs, add https://localhost:44321/signin-oidc, and select Save. In the Advanced settings section, set Logout URL to https://localhost:44321/signout-oidc. Under Implicit grant, check ID tokens. Select Save. Create a section in Appsettings.json named AzureAd . \"AzureAd\": { \"Instance\": \"https://login.microsoftonline.com/\", \"Domain\": \"yourdomain.com\", \"TenantId\": \"00000000-0000-0000-0000-000000000000\", \"ClientId\": \"00000000-0000-0000-0000-000000000000\", \"CallbackPath\": \"/signin-oidc\" } Taken from Quickstart: Add sign-in with Microsoft to an ASP.NET Core web app .","title":"Register your application"},{"location":"Configuring-Levridge-Entity-Event-Endpoint/","text":"Introduction Brief introduction of the module, component or feature being documented. This document explains ... Overview Main Point 1 Sub Point 1.1","title":"Introduction"},{"location":"Configuring-Levridge-Entity-Event-Endpoint/#introduction","text":"Brief introduction of the module, component or feature being documented. This document explains ...","title":"Introduction"},{"location":"Configuring-Levridge-Entity-Event-Endpoint/#overview","text":"","title":"Overview"},{"location":"Configuring-Levridge-Entity-Event-Endpoint/#main-point-1","text":"","title":"Main Point 1"},{"location":"Configuring-Levridge-Entity-Event-Endpoint/#sub-point-11","text":"","title":"Sub Point 1.1"},{"location":"Configuring-Levridge-Entity-Events/","text":"Introduction Brief introduction of the module, component or feature being documented. This document explains ... Overview Main Point 1 Sub Point 1.1","title":"Introduction"},{"location":"Configuring-Levridge-Entity-Events/#introduction","text":"Brief introduction of the module, component or feature being documented. This document explains ...","title":"Introduction"},{"location":"Configuring-Levridge-Entity-Events/#overview","text":"","title":"Overview"},{"location":"Configuring-Levridge-Entity-Events/#main-point-1","text":"","title":"Main Point 1"},{"location":"Configuring-Levridge-Entity-Events/#sub-point-11","text":"","title":"Sub Point 1.1"},{"location":"ControllerSecurity/","text":"Configuring Controller Security Overview Levridge has provided configurable security for our controllers. Once a controller is secured each instance of the Integration Framework in which it is used must be configured in the Active Directory to expose the API. Here is a link to the Microsoft documentation that explains the process of registering an application in Azure Active Directory. Application Managed Identity If the Levridge Integration Framework is deployed in an Azure tenant Microsoft recommends using a managed identity. This makes is much simpler to manage access to Azure resources such as the Azure Service Bus, Azure Key Vault and Azure Configuration Serivice. To setup a system managed identity, use the instructions provided here . You will still need to provide a ClientId . To obtain a ClientId you will need to turn on Security in the Azure Portal. Follow the instructions here to enable security. In one of the steps it has you select the Express option for Azure Active Directory. This creates an Application Registration for you. You can also select an existing Application Registration if you have already created one for the app service. It is the ClientId of the Application Registration that will be used in the AzureAd node of the appsettings.json file. Application Permissions (Roles) Since controllers are generally called by applications that are not using the Azure Active Directory to authenticate users we have configured our controllers to utilize Application Permissions rather than Delegated permissions . Exposing Application Permissions To expose application permissions you must add one or more roles to the registered application manifest. Use this link for specific instructions. Don't forget to provide admin consent. Assigning Application Permissions Once you have exposed application permissions you will need to assign those permissions to the application that will be calling the API. You will assign Application Permissions as opposed to Delegated Permisions. Use this link for specific instructions. Configuring Authorized Roles Once the role(s) are defined as application permissions and the calling application is assigned the necessary permissions you will need to add the role(s) to the InstanceConfig appsettings.json . Configure AzureAd Settings You will need to provide the information necessary to utilize Azure AD to authenticate calls to the controller. See AzureAd Settings for more information. Enabling Security on a Controller At the time of this writing (5/15/2020) we only have security enabled on the default controller. Be careful about enabling security on controllers. Many of the controllers are called by applcations that expect anonymous access. To enable security you will need to add the following attribute to the controller class: [Authorize(AuthenticationSchemes = OpenIdConnectDefaults.AuthenticationScheme + \",\" + JwtBearerDefaults.AuthenticationScheme)] You can simply add the JwtBearer authentication scheme but then you will not be able to access the API from a browser. Allowing Anonymous Access If you have enabled security on a controller but you want certain actions to allow anonymous access you can add the [AllowAnonymous] attribute to the method. If you want all but a few actions to allow anonymous access you can simply add the [Authorize] attribute only to the actions that you want to secure and don't add it to the class. Be sure to specify the Authentication Schemes you want to use or it will simply use the default which is OpenIdConnect.","title":"Configuring Controller Security"},{"location":"ControllerSecurity/#configuring-controller-security","text":"","title":"Configuring Controller Security"},{"location":"ControllerSecurity/#overview","text":"Levridge has provided configurable security for our controllers. Once a controller is secured each instance of the Integration Framework in which it is used must be configured in the Active Directory to expose the API. Here is a link to the Microsoft documentation that explains the process of registering an application in Azure Active Directory.","title":"Overview"},{"location":"ControllerSecurity/#application-managed-identity","text":"If the Levridge Integration Framework is deployed in an Azure tenant Microsoft recommends using a managed identity. This makes is much simpler to manage access to Azure resources such as the Azure Service Bus, Azure Key Vault and Azure Configuration Serivice. To setup a system managed identity, use the instructions provided here . You will still need to provide a ClientId . To obtain a ClientId you will need to turn on Security in the Azure Portal. Follow the instructions here to enable security. In one of the steps it has you select the Express option for Azure Active Directory. This creates an Application Registration for you. You can also select an existing Application Registration if you have already created one for the app service. It is the ClientId of the Application Registration that will be used in the AzureAd node of the appsettings.json file.","title":"Application Managed Identity"},{"location":"ControllerSecurity/#application-permissions-roles","text":"Since controllers are generally called by applications that are not using the Azure Active Directory to authenticate users we have configured our controllers to utilize Application Permissions rather than Delegated permissions .","title":"Application Permissions (Roles)"},{"location":"ControllerSecurity/#exposing-application-permissions","text":"To expose application permissions you must add one or more roles to the registered application manifest. Use this link for specific instructions. Don't forget to provide admin consent.","title":"Exposing Application Permissions"},{"location":"ControllerSecurity/#assigning-application-permissions","text":"Once you have exposed application permissions you will need to assign those permissions to the application that will be calling the API. You will assign Application Permissions as opposed to Delegated Permisions. Use this link for specific instructions.","title":"Assigning Application Permissions"},{"location":"ControllerSecurity/#configuring-authorized-roles","text":"Once the role(s) are defined as application permissions and the calling application is assigned the necessary permissions you will need to add the role(s) to the InstanceConfig appsettings.json .","title":"Configuring Authorized Roles"},{"location":"ControllerSecurity/#configure-azuread-settings","text":"You will need to provide the information necessary to utilize Azure AD to authenticate calls to the controller. See AzureAd Settings for more information.","title":"Configure AzureAd Settings"},{"location":"ControllerSecurity/#enabling-security-on-a-controller","text":"At the time of this writing (5/15/2020) we only have security enabled on the default controller. Be careful about enabling security on controllers. Many of the controllers are called by applcations that expect anonymous access. To enable security you will need to add the following attribute to the controller class: [Authorize(AuthenticationSchemes = OpenIdConnectDefaults.AuthenticationScheme + \",\" + JwtBearerDefaults.AuthenticationScheme)] You can simply add the JwtBearer authentication scheme but then you will not be able to access the API from a browser.","title":"Enabling Security on a Controller"},{"location":"ControllerSecurity/#allowing-anonymous-access","text":"If you have enabled security on a controller but you want certain actions to allow anonymous access you can add the [AllowAnonymous] attribute to the method. If you want all but a few actions to allow anonymous access you can simply add the [Authorize] attribute only to the actions that you want to secure and don't add it to the class. Be sure to specify the Authentication Schemes you want to use or it will simply use the default which is OpenIdConnect.","title":"Allowing Anonymous Access"},{"location":"Controllers/","text":"Controllers Settings Controllers section is a json object in the appsettings.json file used by the Levridge Integration Framework to define which controllers to load for the current Levridge Integration Framework instance. Example \"Controllers\": { \"HostController\": \"Levridge.Integration.Host.DefaultController\", \"AgSyncConroller\": \"Levridge.Integration.Host.AgSyncController\" }, Definition Controllers The controllers json object is comma delimited list of json objects that define which controllers to load for the current Levridge Integration Framework instance. Each json object contains a name and an assembly name. The name is not used by the system, but is used to provide a human friendly name for the assembly referenced with it. In the example above, the Controllers object informs the Levridge Integration Framework to load two assemblies: - \"Levridge.Integration.Host.DefaultController\" - \"Levridge.Integration.Host.AgSyncController\" See Also Controller Security","title":"Controllers Settings"},{"location":"Controllers/#controllers-settings","text":"Controllers section is a json object in the appsettings.json file used by the Levridge Integration Framework to define which controllers to load for the current Levridge Integration Framework instance.","title":"Controllers Settings"},{"location":"Controllers/#example","text":"\"Controllers\": { \"HostController\": \"Levridge.Integration.Host.DefaultController\", \"AgSyncConroller\": \"Levridge.Integration.Host.AgSyncController\" },","title":"Example"},{"location":"Controllers/#definition","text":"","title":"Definition"},{"location":"Controllers/#controllers","text":"The controllers json object is comma delimited list of json objects that define which controllers to load for the current Levridge Integration Framework instance. Each json object contains a name and an assembly name. The name is not used by the system, but is used to provide a human friendly name for the assembly referenced with it. In the example above, the Controllers object informs the Levridge Integration Framework to load two assemblies: - \"Levridge.Integration.Host.DefaultController\" - \"Levridge.Integration.Host.AgSyncController\"","title":"Controllers"},{"location":"Controllers/#see-also","text":"Controller Security","title":"See Also"},{"location":"Create-Split-Group/","text":"Create a New Split Group Steps Split Group Setup: - In Accounts Receivable > Customers > All customers > Select a customer and under the \u201cAgriculture\u201d tab - Click on \"Split groups\" under \"Operation Management\" - The Split Group screen will show the specific split groups associated to the customer. - There is no limit to the number of split groups that can be created per customer. - A specific split group can be set to active or inactive when it is no longer needed. - After a split group is created and made active, it can be selected for use on sales agreements and sales orders. Prepayments are applied against a sales invoice according to the split selected on the sales order. Steps Expected Result 1. Navigate to Accounts Receivable > Setup >Agriculture > Split Groups The split groups grid will display. 2. From the top menu bar click the +New button. A fly-out menu on the right sidebar will appear. The split group code field will automatically be assigned a sequential number sequence. 3. Select/change the Start Date to reflect the split group start date by clicking on the calendar icon and choosing a date or enter a date manually. The start date will default to today's date but can be changed to the actual start date. Once a date is selected from the calendar or manually a date should appear in the Start Date field. 4. Select/change the End Date to reflect the split group end date by clicking on the calendar icon and choosing a date or enter a date manually. The end date will default to the system's infinity date but can be changed to the actual end date. Once a date is selected from the calendar or manually, a date should appear in the End Date field. This field should always contain a date later than the start date.","title":"Create a New Split Group"},{"location":"Create-Split-Group/#create-a-new-split-group","text":"","title":"Create a New Split Group"},{"location":"Create-Split-Group/#steps","text":"Split Group Setup: - In Accounts Receivable > Customers > All customers > Select a customer and under the \u201cAgriculture\u201d tab - Click on \"Split groups\" under \"Operation Management\" - The Split Group screen will show the specific split groups associated to the customer. - There is no limit to the number of split groups that can be created per customer. - A specific split group can be set to active or inactive when it is no longer needed. - After a split group is created and made active, it can be selected for use on sales agreements and sales orders. Prepayments are applied against a sales invoice according to the split selected on the sales order. Steps Expected Result 1. Navigate to Accounts Receivable > Setup >Agriculture > Split Groups The split groups grid will display. 2. From the top menu bar click the +New button. A fly-out menu on the right sidebar will appear. The split group code field will automatically be assigned a sequential number sequence. 3. Select/change the Start Date to reflect the split group start date by clicking on the calendar icon and choosing a date or enter a date manually. The start date will default to today's date but can be changed to the actual start date. Once a date is selected from the calendar or manually a date should appear in the Start Date field. 4. Select/change the End Date to reflect the split group end date by clicking on the calendar icon and choosing a date or enter a date manually. The end date will default to the system's infinity date but can be changed to the actual end date. Once a date is selected from the calendar or manually, a date should appear in the End Date field. This field should always contain a date later than the start date.","title":"Steps"},{"location":"CreateMaintainMonitorCustomerPrograms/","text":"Create, Maintain, Monitor Customer Programs This document explains how to create, maintain, and monitor customer programs. Create a New Customer Program Go to Accounts receivable > Customers > All customers. In the list, find and select the desired record. On the Action Pane, click Agriculture. Click Customer finance programs. Click New. In the Finance program field, enter or select a value. In the Program account number field, type a value. a. Required field. Enter N/A if an account number is not relevant. In the Maximum amount field, enter a number. In the Renewable field, select an option (Yes/No). In the From date field, enter a start date. In the To date field, enter a date. In the Notes field, type a value, if applicable. In the Address field, select a value. \u2013 Would like this to default if there is only one value In the Contact information field, select a value. \u2013 Would like this to default if there is only one value Click Save. Close the page. View or Edit a Customer Program Go to Accounts receivable > Customers > All customers. In the list, find and select the desired record. On the Action Pane, click Agriculture. Click Customer finance programs. View program information or edit as appropriate. If applicable, Click Recalculate balances. \u2013 Would like to add a batch process Status of \"Active\" is defaulted from the base program value. Date columns, as well as most other data is available to sort/filter to limit this view over time. To view all customer transactions or the transactions that account for the \"Amount used\" and/or \"Uncollected amount\" follow these steps: a. Go to Accounts receivable > Customers > All customers. b. In the list, find and select the desired record. c. On the Action Pane, click Customer. d. Click Transactions. e. Default view displays \"All\" transactions. i. Hint: the user may personalize the transaction list to include and display the Method of payment. ii. Corrections: when the Method of payment is visible in the list, it is available for edit with appropriate security access. This allows for transactions not assigned correctly to be adjusted to the intended Method of payment. If adjustment is made, the \"Recalculate balances\" process must be executed in order to update the displayed values for Amount used and Uncollected amount. f. Select show \"Open\", if applicable to display unapplied payments and unpaid/unsettled invoices For the date range applicable to the Finance program (& the Method of payment), the Amount used is the sum of all sales order, free text, or credit memo transactions. For the date range applicable to the Finance program (& the Method of payment), the Uncollected amount is the sum of all sales order, free text, or credit memo transactions where the balance of the transaction is not = zero. PLUS any payments with the same criteria. View All Customer Finance Programs Go to Accounts receivable > Financing > Customer finance programs. Click Recalculate balances. Would like to add a batch process This is available for export to excel. Status of \"Active\" is defaulted from the base program value. Date columns, as well as most other data is available to sort/filter to limit this view over time. Process Finance Program Payments Go to Accounts receivable > Payments > Customer payment journal. Click New. In the list, select the applicable journal. Click Enter customer payments. In the Customer field, specify the value or search for a value. In the Payment reference (check # or ACH trans code) field, enter a value. In the Amount (of payment) field, enter a number. In the Description field, type a value such a JDF payment. In the Method of payment field, enter or select the value which represents the Finance program. If applicable, Mark any transactions for settlement. a. Hint: the user may personalize the transaction list to include and display the Method of payment. If not settled, the payment credit will be available for manually matching of current or future transactions. Click Save in journal. Close the page. When the journal has been completed. Click Post.","title":"Create, Maintain, Monitor Customer Programs"},{"location":"CreateMaintainMonitorCustomerPrograms/#create-maintain-monitor-customer-programs","text":"This document explains how to create, maintain, and monitor customer programs.","title":"Create, Maintain, Monitor Customer Programs"},{"location":"CreateMaintainMonitorCustomerPrograms/#create-a-new-customer-program","text":"Go to Accounts receivable > Customers > All customers. In the list, find and select the desired record. On the Action Pane, click Agriculture. Click Customer finance programs. Click New. In the Finance program field, enter or select a value. In the Program account number field, type a value. a. Required field. Enter N/A if an account number is not relevant. In the Maximum amount field, enter a number. In the Renewable field, select an option (Yes/No). In the From date field, enter a start date. In the To date field, enter a date. In the Notes field, type a value, if applicable. In the Address field, select a value. \u2013 Would like this to default if there is only one value In the Contact information field, select a value. \u2013 Would like this to default if there is only one value Click Save. Close the page.","title":"Create a New Customer Program"},{"location":"CreateMaintainMonitorCustomerPrograms/#view-or-edit-a-customer-program","text":"Go to Accounts receivable > Customers > All customers. In the list, find and select the desired record. On the Action Pane, click Agriculture. Click Customer finance programs. View program information or edit as appropriate. If applicable, Click Recalculate balances. \u2013 Would like to add a batch process Status of \"Active\" is defaulted from the base program value. Date columns, as well as most other data is available to sort/filter to limit this view over time. To view all customer transactions or the transactions that account for the \"Amount used\" and/or \"Uncollected amount\" follow these steps: a. Go to Accounts receivable > Customers > All customers. b. In the list, find and select the desired record. c. On the Action Pane, click Customer. d. Click Transactions. e. Default view displays \"All\" transactions. i. Hint: the user may personalize the transaction list to include and display the Method of payment. ii. Corrections: when the Method of payment is visible in the list, it is available for edit with appropriate security access. This allows for transactions not assigned correctly to be adjusted to the intended Method of payment. If adjustment is made, the \"Recalculate balances\" process must be executed in order to update the displayed values for Amount used and Uncollected amount. f. Select show \"Open\", if applicable to display unapplied payments and unpaid/unsettled invoices For the date range applicable to the Finance program (& the Method of payment), the Amount used is the sum of all sales order, free text, or credit memo transactions. For the date range applicable to the Finance program (& the Method of payment), the Uncollected amount is the sum of all sales order, free text, or credit memo transactions where the balance of the transaction is not = zero. PLUS any payments with the same criteria.","title":"View or Edit a Customer Program"},{"location":"CreateMaintainMonitorCustomerPrograms/#view-all-customer-finance-programs","text":"Go to Accounts receivable > Financing > Customer finance programs. Click Recalculate balances. Would like to add a batch process This is available for export to excel. Status of \"Active\" is defaulted from the base program value. Date columns, as well as most other data is available to sort/filter to limit this view over time.","title":"View All Customer Finance Programs"},{"location":"CreateMaintainMonitorCustomerPrograms/#process-finance-program-payments","text":"Go to Accounts receivable > Payments > Customer payment journal. Click New. In the list, select the applicable journal. Click Enter customer payments. In the Customer field, specify the value or search for a value. In the Payment reference (check # or ACH trans code) field, enter a value. In the Amount (of payment) field, enter a number. In the Description field, type a value such a JDF payment. In the Method of payment field, enter or select the value which represents the Finance program. If applicable, Mark any transactions for settlement. a. Hint: the user may personalize the transaction list to include and display the Method of payment. If not settled, the payment credit will be available for manually matching of current or future transactions. Click Save in journal. Close the page. When the journal has been completed. Click Post.","title":"Process Finance Program Payments"},{"location":"CreateProcurementCategory/","text":"Creating a Procurement Category Brief introduction of the module, component or feature being documented. This document explains ... How to Create a Procurement Category, Assign an Item, and Assign a GL Go to Product Information Management > Setup > Categories and Attributes > Category Hierarchies. In the list, find and select the desired record. In the list, click the link in the selected row. Click New category node. In the Name field, type a value. Click Save. Click Save. Click Add. Use the Quick Filter to find records. For example, filter on the Product number field with a value of '1987'. In the list, mark the selected row. Click Add. Click OK. Close the page. Close the page. Go to Product Information Management > Products > Released Products. Use the Quick Filter to find records. For example, filter on the Item number field with a value of '1987'. Click Product categories. Click New. In the Category hierarchy field, enter or select a value. In the list, select row 2. In the list, click the link in the selected row. In the Category field, enter or select a value. In the tree, select 'Landus (New Category)\\Monsanto (New Category)'. Click OK. Click Save. Click Delete. Click Save. Close the page. Close the page. Go to Inventory Management > Setup > Posting > Posting. Click the Purchase order tab. In the Select field, select an option. Click New. In the list, mark the selected row. In the Item code field, select an option. In the Category relation field, enter or select a value. In the tree, select 'Lanuds (New Category)\\Monsanto (New Category)'. Click OK. In the Main account field, specify the desired values. Click Save. Close the page.","title":"Create a Procurement Category"},{"location":"CreateProcurementCategory/#creating-a-procurement-category","text":"Brief introduction of the module, component or feature being documented. This document explains ...","title":"Creating a Procurement Category"},{"location":"CreateProcurementCategory/#how-to-create-a-procurement-category-assign-an-item-and-assign-a-gl","text":"Go to Product Information Management > Setup > Categories and Attributes > Category Hierarchies. In the list, find and select the desired record. In the list, click the link in the selected row. Click New category node. In the Name field, type a value. Click Save. Click Save. Click Add. Use the Quick Filter to find records. For example, filter on the Product number field with a value of '1987'. In the list, mark the selected row. Click Add. Click OK. Close the page. Close the page. Go to Product Information Management > Products > Released Products. Use the Quick Filter to find records. For example, filter on the Item number field with a value of '1987'. Click Product categories. Click New. In the Category hierarchy field, enter or select a value. In the list, select row 2. In the list, click the link in the selected row. In the Category field, enter or select a value. In the tree, select 'Landus (New Category)\\Monsanto (New Category)'. Click OK. Click Save. Click Delete. Click Save. Close the page. Close the page. Go to Inventory Management > Setup > Posting > Posting. Click the Purchase order tab. In the Select field, select an option. Click New. In the list, mark the selected row. In the Item code field, select an option. In the Category relation field, enter or select a value. In the tree, select 'Lanuds (New Category)\\Monsanto (New Category)'. Click OK. In the Main account field, specify the desired values. Click Save. Close the page.","title":"How to Create a Procurement Category, Assign an Item, and Assign a GL"},{"location":"CreateProcurementHierarchy/","text":"Create Procurement Hierarchy Brief introduction of the module, component or feature being documented. This document explains ... How to Create Procurement Hierarchy Go to Product Information Management > Setup > Categories and Attributes > Category Hierarchies. Click New. In the Name field, type a value. In the Description field, type a value. Click Create. Click New Category node. In the Name field, type a value. In the Friendly name field, type a value. Click Save. Click New category node. In the Name field, type a value. In the Friendly name field, type a value. In the tree, select 'Procurement'. Click New category node. In the Name field, type a value. In the Friendly name field, type a value. Click Save. In the tree, select 'Procurement'. Click New category node. In the Name field, type a value. In the Friendly name field, type a value. Click Save. In the tree, select 'Procurement\\Agronomy'. Click New category node. In the Name field, type a value. In the Friendly name field, type a value. Click Save. In the tree, select 'Procurement\\Feed'. Click New category node. In the Name field, type a value. In the Friendly name field, type a value. Click Save. Expand the Products section. In the tree, select 'Procurement\\Agronomy\\Fert'. Click Add. In the list, find and select the desired record. In the list, find and select the desired record. Click Add. In the list, find and select the desired record. Click Add. Click OK. Click Add. In the list, find and select the desired record. In the list, find and select the desired record. In the list, find and select the desired record. In the list, find and select the desired record. Click Add. Click OK. Click Save.","title":"Create a Procurement Hierarchy"},{"location":"CreateProcurementHierarchy/#create-procurement-hierarchy","text":"Brief introduction of the module, component or feature being documented. This document explains ...","title":"Create Procurement Hierarchy"},{"location":"CreateProcurementHierarchy/#how-to-create-procurement-hierarchy","text":"Go to Product Information Management > Setup > Categories and Attributes > Category Hierarchies. Click New. In the Name field, type a value. In the Description field, type a value. Click Create. Click New Category node. In the Name field, type a value. In the Friendly name field, type a value. Click Save. Click New category node. In the Name field, type a value. In the Friendly name field, type a value. In the tree, select 'Procurement'. Click New category node. In the Name field, type a value. In the Friendly name field, type a value. Click Save. In the tree, select 'Procurement'. Click New category node. In the Name field, type a value. In the Friendly name field, type a value. Click Save. In the tree, select 'Procurement\\Agronomy'. Click New category node. In the Name field, type a value. In the Friendly name field, type a value. Click Save. In the tree, select 'Procurement\\Feed'. Click New category node. In the Name field, type a value. In the Friendly name field, type a value. Click Save. Expand the Products section. In the tree, select 'Procurement\\Agronomy\\Fert'. Click Add. In the list, find and select the desired record. In the list, find and select the desired record. Click Add. In the list, find and select the desired record. Click Add. Click OK. Click Add. In the list, find and select the desired record. In the list, find and select the desired record. In the list, find and select the desired record. In the list, find and select the desired record. Click Add. Click OK. Click Save.","title":"How to Create Procurement Hierarchy"},{"location":"CreatePurchaseRequistionAndConsolidation/","text":"Create a Purchase Requisition and Consolidation Brief introduction of the module, component or feature being documented. This document explains ... How to Create a Purchase Requisition and Consolidation Go to Procurement and sourcing > Purchase Requisitions > All purchase requisitions. Click New. In the Name field, type a value. Click OK. In the Reason field, enter or select a value. In the list, click the link in the selected row. Click Add line. In the list, mark the selected row. In the Item number field, type a value. In the Quantity field, enter a number. Click Save. Click the Inventory Dimensions tab. In the Site field, type a value. In the Warehouse field, enter or select a value. In the list, click the link in the selected row. Click Save. Click Workflow to open the drop dialog. Click Submit. Click Workflow to open the drop dialog. Close the page. Click Workflow to open the drog dialog. Click Approve. Close the page. How to Create a Purchase Consolidation Go to Procurement and Sourcing > Purchase Requisitions > Approved Purchase requisition processing > Consolidation Opportunities. Click New to open the drop dialog. In the Name field, type a value. Click Ok. Click Add to opportunity. In the list, find and select the desired record. In the list, find and select the desired record. Click OK. Click Close Opportunity. In the list, mark the selected row. In the Consolidation quotation number field, type a value. In the list, find and select the desired record. In the list, find and select the desired record. In the Consolidation quotation number field, type a value. In the list, find and select the desired record. In the list, unmark the selected row. In the list, mark the selected row. In the Vendor account field, enter or select a value. In the list, click the link in the selected row. In the list, find and select the desired record. In the Vendor account field, type a value. In the list, find and select the desired record. In the list, unmark the selected row. In the list, mark the selected row. In the Consolidation quotation number field, type a value. In the list, mark or unmark all rows. Click Close opportunity. In the list, mark or unmark all rows. Click Create Purchase Order.","title":"Create a Purchase Requisition and Consolidation"},{"location":"CreatePurchaseRequistionAndConsolidation/#create-a-purchase-requisition-and-consolidation","text":"Brief introduction of the module, component or feature being documented. This document explains ...","title":"Create a Purchase Requisition and Consolidation"},{"location":"CreatePurchaseRequistionAndConsolidation/#how-to-create-a-purchase-requisition-and-consolidation","text":"Go to Procurement and sourcing > Purchase Requisitions > All purchase requisitions. Click New. In the Name field, type a value. Click OK. In the Reason field, enter or select a value. In the list, click the link in the selected row. Click Add line. In the list, mark the selected row. In the Item number field, type a value. In the Quantity field, enter a number. Click Save. Click the Inventory Dimensions tab. In the Site field, type a value. In the Warehouse field, enter or select a value. In the list, click the link in the selected row. Click Save. Click Workflow to open the drop dialog. Click Submit. Click Workflow to open the drop dialog. Close the page. Click Workflow to open the drog dialog. Click Approve. Close the page.","title":"How to Create a Purchase Requisition and Consolidation"},{"location":"CreatePurchaseRequistionAndConsolidation/#how-to-create-a-purchase-consolidation","text":"Go to Procurement and Sourcing > Purchase Requisitions > Approved Purchase requisition processing > Consolidation Opportunities. Click New to open the drop dialog. In the Name field, type a value. Click Ok. Click Add to opportunity. In the list, find and select the desired record. In the list, find and select the desired record. Click OK. Click Close Opportunity. In the list, mark the selected row. In the Consolidation quotation number field, type a value. In the list, find and select the desired record. In the list, find and select the desired record. In the Consolidation quotation number field, type a value. In the list, find and select the desired record. In the list, unmark the selected row. In the list, mark the selected row. In the Vendor account field, enter or select a value. In the list, click the link in the selected row. In the list, find and select the desired record. In the Vendor account field, type a value. In the list, find and select the desired record. In the list, unmark the selected row. In the list, mark the selected row. In the Consolidation quotation number field, type a value. In the list, mark or unmark all rows. Click Close opportunity. In the list, mark or unmark all rows. Click Create Purchase Order.","title":"How to Create a Purchase Consolidation"},{"location":"CustomPluginAssemblyConfig/","text":"Custom Plugin Assemblies Config Settings The CustomPluginAssemblies configuration node is used to define any custom plugin assemblies to be loaded by the integration framework and the class method to call to register the custom plugin from the assembly. This node is a list (not an array) of named CustomPluginAssemblyConfig json objects. Example \"CustomPluginAssemblies\": { \"CustomFieldsAssembly\": { \"Path\": \".\\\\plugins\", \"AssemblyName\": \"Levridge.Integration.IntegrationService.Mapping.CustomFields\", \"Namespace\": \"Levridge.Integration.IntegrationService.Mapping.CustomFields\", \"ClassName\": \"CustomFieldsEntityMapBuilderExtensions\", \"MethodName\": \"AddCustomFields\" } }, Definition The CustomMappingAssemblies node above has a single CustomPluginAssemblyConfig object named \"CustomeFieldAssembly\". Name Required Default: No default value. This is the name of the custom mapping assembly configuration. Each configuration in the list must have a unique name. Because you can have multiple classes and or multiple methods we do not use the AssemblyName as the configuration name. Path Not Required Default: The path of Levridge.Integration.Host This is the path for the custom plugin assembly. It can be specified as an absolute path or a relative path. The relative path is relative to the current execution directory. (Directory.GetCurrentDirectory()) AssemblyName Required Default: No default value. This is the actual name of the assembly, not the file name. In the current version the file name is assumed to be the \"[AssemblyName].dll\". Do not add the file extension or a file path to this value. NameSpace Required Default: No default value. This property must contain the namespace that contains the class. This value will be added to the class name to obtain the class type. ClassName Required Default: No default value. This property is the name of the class that contains the method used to register the custom mapping. Do not include the namespace. The namespace will be added to the class name to obtain the type. MethodName Required Default: No default value. This property is the name of the method that will be called on the class that is used to register the custom mapping. Do not include the class name here.","title":"Custom Plugin Assemblies Config Settings"},{"location":"CustomPluginAssemblyConfig/#custom-plugin-assemblies-config-settings","text":"The CustomPluginAssemblies configuration node is used to define any custom plugin assemblies to be loaded by the integration framework and the class method to call to register the custom plugin from the assembly. This node is a list (not an array) of named CustomPluginAssemblyConfig json objects.","title":"Custom Plugin Assemblies Config Settings"},{"location":"CustomPluginAssemblyConfig/#example","text":"\"CustomPluginAssemblies\": { \"CustomFieldsAssembly\": { \"Path\": \".\\\\plugins\", \"AssemblyName\": \"Levridge.Integration.IntegrationService.Mapping.CustomFields\", \"Namespace\": \"Levridge.Integration.IntegrationService.Mapping.CustomFields\", \"ClassName\": \"CustomFieldsEntityMapBuilderExtensions\", \"MethodName\": \"AddCustomFields\" } },","title":"Example"},{"location":"CustomPluginAssemblyConfig/#definition","text":"The CustomMappingAssemblies node above has a single CustomPluginAssemblyConfig object named \"CustomeFieldAssembly\".","title":"Definition"},{"location":"CustomPluginAssemblyConfig/#name","text":"Required Default: No default value. This is the name of the custom mapping assembly configuration. Each configuration in the list must have a unique name. Because you can have multiple classes and or multiple methods we do not use the AssemblyName as the configuration name.","title":"Name"},{"location":"CustomPluginAssemblyConfig/#path","text":"Not Required Default: The path of Levridge.Integration.Host This is the path for the custom plugin assembly. It can be specified as an absolute path or a relative path. The relative path is relative to the current execution directory. (Directory.GetCurrentDirectory())","title":"Path"},{"location":"CustomPluginAssemblyConfig/#assemblyname","text":"Required Default: No default value. This is the actual name of the assembly, not the file name. In the current version the file name is assumed to be the \"[AssemblyName].dll\". Do not add the file extension or a file path to this value.","title":"AssemblyName"},{"location":"CustomPluginAssemblyConfig/#namespace","text":"Required Default: No default value. This property must contain the namespace that contains the class. This value will be added to the class name to obtain the class type.","title":"NameSpace"},{"location":"CustomPluginAssemblyConfig/#classname","text":"Required Default: No default value. This property is the name of the class that contains the method used to register the custom mapping. Do not include the namespace. The namespace will be added to the class name to obtain the type.","title":"ClassName"},{"location":"CustomPluginAssemblyConfig/#methodname","text":"Required Default: No default value. This property is the name of the method that will be called on the class that is used to register the custom mapping. Do not include the class name here.","title":"MethodName"},{"location":"CustomerCreation/","text":"Customer Creation Brief introduction of the module, component or feature being documented. This document explains ... Customer Creation Go to Accounts receivable > Customers > All Customers. Click New. Click OK. In the First name field, type a value. In the Last name field, type a value. In the Customer group field, enter or select a value. In the list, click the link in the selected row. In the Terms of payment field, enter or select a value. In the list, click the link in the selected row. In the ZIP/postal code field, type a value. In the Street field, type a value. In the Street field, type a value. Click Save. Expand the Addresses section. Click Add. In the Description field, type a value. In the Contact number/address field, type a value. Select the Primary check box. Click Add. In the Type field, select an option. In the Contact number/address field, type a value. Expand the Credit and collections section. In the Credit limit field, enter a number. In the Credit rating field, type a value. In the Method of payment field, enter or select a value. In the list, select row 3. In the list, click the link in the selected row. Expand the Financial dimensions section. In the LocationSite value field, enter or select a value. In the list, select row 5. In the list, click the link in the selected row. Close the page.","title":"Customer Creation"},{"location":"CustomerCreation/#customer-creation","text":"Brief introduction of the module, component or feature being documented. This document explains ...","title":"Customer Creation"},{"location":"CustomerCreation/#customer-creation_1","text":"Go to Accounts receivable > Customers > All Customers. Click New. Click OK. In the First name field, type a value. In the Last name field, type a value. In the Customer group field, enter or select a value. In the list, click the link in the selected row. In the Terms of payment field, enter or select a value. In the list, click the link in the selected row. In the ZIP/postal code field, type a value. In the Street field, type a value. In the Street field, type a value. Click Save. Expand the Addresses section. Click Add. In the Description field, type a value. In the Contact number/address field, type a value. Select the Primary check box. Click Add. In the Type field, select an option. In the Contact number/address field, type a value. Expand the Credit and collections section. In the Credit limit field, enter a number. In the Credit rating field, type a value. In the Method of payment field, enter or select a value. In the list, select row 3. In the list, click the link in the selected row. Expand the Financial dimensions section. In the LocationSite value field, enter or select a value. In the list, select row 5. In the list, click the link in the selected row. Close the page.","title":"Customer Creation"},{"location":"CustomerCreationOrganization/","text":"Customer Creation Organzation Brief introduction of the module, component or feature being documented. This document explains ... Customer Creation Organization Go to Accounts Receivable > Customers > All Customers. Click New. Click OK. In the Type field, select an option. Click OK. In the Name field, type a value. In the Customer group field, enter or select a value. In the list, select row 2. In the list, click the link in the select row. In the Terms of Payment field, enter or select a value. In the list, click the link in the select row. In the ZIP/postal code field, type a value. In the Street field, type a value. Click Save. Click Show more fields. Click Add. In the Description field, type a value. In the Contact number/address field, type a value. Select the Primary check box. Click Add. In the Type field, select an option. In the Contact number/address field, type a value. In the Sales group field, enter or select a value. In the list, select row 2. In the list, click the link in the selected row. In the Method of payment field, enter or select a value. In the list, select row 3. In the list, click the link in the selected row. In the LocationSite value field, enter or select a value. In the list, click the link in the selected row. Click Save.","title":"Customer Creation Organization"},{"location":"CustomerCreationOrganization/#customer-creation-organzation","text":"Brief introduction of the module, component or feature being documented. This document explains ...","title":"Customer Creation Organzation"},{"location":"CustomerCreationOrganization/#customer-creation-organization","text":"Go to Accounts Receivable > Customers > All Customers. Click New. Click OK. In the Type field, select an option. Click OK. In the Name field, type a value. In the Customer group field, enter or select a value. In the list, select row 2. In the list, click the link in the select row. In the Terms of Payment field, enter or select a value. In the list, click the link in the select row. In the ZIP/postal code field, type a value. In the Street field, type a value. Click Save. Click Show more fields. Click Add. In the Description field, type a value. In the Contact number/address field, type a value. Select the Primary check box. Click Add. In the Type field, select an option. In the Contact number/address field, type a value. In the Sales group field, enter or select a value. In the list, select row 2. In the list, click the link in the selected row. In the Method of payment field, enter or select a value. In the list, select row 3. In the list, click the link in the selected row. In the LocationSite value field, enter or select a value. In the list, click the link in the selected row. Click Save.","title":"Customer Creation Organization"},{"location":"CustomerFinancePrograms/","text":"Customer Finance Programs Overview Create a Finance Program Create, Maintain, Monitor Customer Programs Finance Program Processes","title":"Customer Finance Programs"},{"location":"CustomerFinancePrograms/#customer-finance-programs","text":"","title":"Customer Finance Programs"},{"location":"CustomerFinancePrograms/#overview","text":"Create a Finance Program Create, Maintain, Monitor Customer Programs Finance Program Processes","title":"Overview"},{"location":"CustomerOperations_CustomerSites_SplitGroups/","text":"Customer Operations, Customer Sites and Split Groups Overview The following provides an overview of customer operations, customer sites and split groups in Levridge. Customer operations and sites are master data concepts in Levridge. They are defined as: Account/Customer: A farm or production organization Customer operation: A farm or livestock operation Customer site: A field or barn Customer site location: A zone in a field or a pen in a barn Contact: Members of a farm or livestock organization. These contacts can be involved in the organization in different ways. For example, a veterinarian or farm hand might be contacts for a farm. An account or customer is who the ag retailer does business with. This party is often referred to as the grower. This party has the financial responsibilities with the Ag retailer. The customer operation is the business entity the account or customer participates in. A customer might be part of multiple customer operations. Products and services are exchanged between the Ag retailer and the customer and the operation the customer is participating in. In Levridge, an Ag retailer could provide feedback in the form of a return on investment (ROI) or balance sheet to the grower based on information delivered to a specific customer operation. Historical information can also be tracked to individuals assigned to or working on the specific operation (ex: agronomist, feed salesperson, tractor driver). This is helpful for corporate farms to be able to break down who is handling what operation. A customer site is a subset of a customer operation. A customer operation might have many customer sites/fields. Data is stored on customer sites and tasks are performed against the sites. For example, the following information can be tracked on a customer site: - Farmable Acres \u2013 used in calculations of seed requirements and fertilizer amounts - County, Township, Plat, etc. - Field Photo - Crop History - Commodity varieties and yields over time - Pest pressures The following example tasks that can be performed against customer sites: - Applications \u2013 track all applications per growing season - Soil Samples \u2013 track all sampling activity As a customer operation can have several customer sites, within the animal feed space, customer sites can have several customer site locations. For example, a barn can have several bins. Split Groups Split groups (commonly referred to as splits) are the link between customers and customer operations. Split groups define how to allocate costs or revenue across the one or more customers involved in the split. For example, if there are 3 different participants involved in a single customer operation, the split group might look like this: Customer A \u2013 40% Customer B \u2013 30% Customer C \u2013 30% Default split groups can be defined for ordering purposes and for commodity interactions with the Ag retailer. These default splits can be set on customer operations or on each customer site which means, if desired, each field can have a different default split group. Default split groups can also be defined at the item category or product level as well. In Levridge, there are two different types of default split groups: 1. Ordering split 2. Commodity split The Commodity default split defines ownership of products when the grower is selling to the Ag retailer. The Ordering default split is used when the grower is buying products from the Ag retailer (that will be utilized in customer sites and operations). Here is How to Create a Split Group in F&O and information on how to Manage Splits, Sales Contracts and Prepayments .","title":"Customer Operations, Customer Sites and Split Groups"},{"location":"CustomerOperations_CustomerSites_SplitGroups/#customer-operations-customer-sites-and-split-groups","text":"","title":"Customer Operations, Customer Sites and Split Groups"},{"location":"CustomerOperations_CustomerSites_SplitGroups/#overview","text":"The following provides an overview of customer operations, customer sites and split groups in Levridge. Customer operations and sites are master data concepts in Levridge. They are defined as: Account/Customer: A farm or production organization Customer operation: A farm or livestock operation Customer site: A field or barn Customer site location: A zone in a field or a pen in a barn Contact: Members of a farm or livestock organization. These contacts can be involved in the organization in different ways. For example, a veterinarian or farm hand might be contacts for a farm. An account or customer is who the ag retailer does business with. This party is often referred to as the grower. This party has the financial responsibilities with the Ag retailer. The customer operation is the business entity the account or customer participates in. A customer might be part of multiple customer operations. Products and services are exchanged between the Ag retailer and the customer and the operation the customer is participating in. In Levridge, an Ag retailer could provide feedback in the form of a return on investment (ROI) or balance sheet to the grower based on information delivered to a specific customer operation. Historical information can also be tracked to individuals assigned to or working on the specific operation (ex: agronomist, feed salesperson, tractor driver). This is helpful for corporate farms to be able to break down who is handling what operation. A customer site is a subset of a customer operation. A customer operation might have many customer sites/fields. Data is stored on customer sites and tasks are performed against the sites. For example, the following information can be tracked on a customer site: - Farmable Acres \u2013 used in calculations of seed requirements and fertilizer amounts - County, Township, Plat, etc. - Field Photo - Crop History - Commodity varieties and yields over time - Pest pressures The following example tasks that can be performed against customer sites: - Applications \u2013 track all applications per growing season - Soil Samples \u2013 track all sampling activity As a customer operation can have several customer sites, within the animal feed space, customer sites can have several customer site locations. For example, a barn can have several bins.","title":"Overview"},{"location":"CustomerOperations_CustomerSites_SplitGroups/#split-groups","text":"Split groups (commonly referred to as splits) are the link between customers and customer operations. Split groups define how to allocate costs or revenue across the one or more customers involved in the split. For example, if there are 3 different participants involved in a single customer operation, the split group might look like this: Customer A \u2013 40% Customer B \u2013 30% Customer C \u2013 30% Default split groups can be defined for ordering purposes and for commodity interactions with the Ag retailer. These default splits can be set on customer operations or on each customer site which means, if desired, each field can have a different default split group. Default split groups can also be defined at the item category or product level as well. In Levridge, there are two different types of default split groups: 1. Ordering split 2. Commodity split The Commodity default split defines ownership of products when the grower is selling to the Ag retailer. The Ordering default split is used when the grower is buying products from the Ag retailer (that will be utilized in customer sites and operations). Here is How to Create a Split Group in F&O and information on how to Manage Splits, Sales Contracts and Prepayments .","title":"Split Groups"},{"location":"CustomerTemplate/","text":"Customer Template Brief introduction of the module, component or feature being documented. This document explains ... Customer Template Close the page. Go to Accounts Receivable > Customers > All Customers. In the list, find and select the desired record. Click Record Info. Click New. In the list, find and select the desired record. Select the Is Default check box. Click OK. Click Cancel.","title":"Customer Template"},{"location":"CustomerTemplate/#customer-template","text":"Brief introduction of the module, component or feature being documented. This document explains ...","title":"Customer Template"},{"location":"CustomerTemplate/#customer-template_1","text":"Close the page. Go to Accounts Receivable > Customers > All Customers. In the list, find and select the desired record. Click Record Info. Click New. In the list, find and select the desired record. Select the Is Default check box. Click OK. Click Cancel.","title":"Customer Template"},{"location":"D365-CE-to-D365-F%26O/","text":"D365 CE to D365 F&O Setup To integrate from D365 CE to D365 F&O you will need to: - Configure Azure Service Bus Endpoint in CE - Configure Azure Service Bus plug-in on the appropriate entities - Create an application ID for the integration framework to authenticate to D365 F&O - Create an Azure Active Directory Application in D365 F&O - Create an Azure Service bus topic - Create a subscription on the topic above Note: Because CE does not support sending messages to topics with subscriptions that require sessions, it is important to make sure that the subscription is created without enabling sessions. In order to support message ordering without the use of sessions the TopicDescription.SupportOrdering property must be set to true on the topic. You will need to use the service bus explorer to set this. Configuration In the appsettings.json you will need to define the SourceConfig and TargetConfig nodes as follows: \"SourceConfig\": { \"ServiceBusConfigName\": [section name with service bus topic], \"ODataConfigName\": [section name with CRM data configuration], \"SystemName\": \"DynamicsCRM\", \"Direction\": \"Source\" }, \"TargetConfig\": { \"ODataConfigName\": [section name with F&O data configuration], \"CDSConfigName\": [section name with CDS data configuration], \"SystemName\": \"DynamicsAX\", \"Direction\": \"Target\" }","title":"D365 CE to D365 F&O"},{"location":"D365-CE-to-D365-F%26O/#d365-ce-to-d365-fo","text":"","title":"D365 CE to D365 F&amp;O"},{"location":"D365-CE-to-D365-F%26O/#setup","text":"To integrate from D365 CE to D365 F&O you will need to: - Configure Azure Service Bus Endpoint in CE - Configure Azure Service Bus plug-in on the appropriate entities - Create an application ID for the integration framework to authenticate to D365 F&O - Create an Azure Active Directory Application in D365 F&O - Create an Azure Service bus topic - Create a subscription on the topic above Note: Because CE does not support sending messages to topics with subscriptions that require sessions, it is important to make sure that the subscription is created without enabling sessions. In order to support message ordering without the use of sessions the TopicDescription.SupportOrdering property must be set to true on the topic. You will need to use the service bus explorer to set this.","title":"Setup"},{"location":"D365-CE-to-D365-F%26O/#configuration","text":"In the appsettings.json you will need to define the SourceConfig and TargetConfig nodes as follows: \"SourceConfig\": { \"ServiceBusConfigName\": [section name with service bus topic], \"ODataConfigName\": [section name with CRM data configuration], \"SystemName\": \"DynamicsCRM\", \"Direction\": \"Source\" }, \"TargetConfig\": { \"ODataConfigName\": [section name with F&O data configuration], \"CDSConfigName\": [section name with CDS data configuration], \"SystemName\": \"DynamicsAX\", \"Direction\": \"Target\" }","title":"Configuration"},{"location":"D365-F%26O-to-D365-CE/","text":"D365 F&O to D365 CE Setup To integrate from D365 F&O to D365 CE you will need to: - Configure Event Endpoint in F&O - configure Levridge Entity Events - Create an application ID for the integration framework to authenticate to D365 CE - Create an application user in D365 CE and assign the proper role(s) - Create an Azure Service bus topic - Create a subscription on the topic above Configuration In the appsettings.json you will need to define the SourceConfig and TargetConfig nodes as follows: \"SourceConfig\": { \"ServiceBusConfigName\": [section name with service bus topic], \"ODataConfigName\": [section name with F&O data configuration], \"SystemName\": \"DynamicsAX\", \"Direction\": \"Source\" }, \"TargetConfig\": { \"ODataConfigName\": [section name with CRM data configuration], \"CDSConfigName\": [section name with CDS data configuration], \"SystemName\": \"DynamicsCRM\", \"Direction\": \"Target\" }","title":"D365 F&O to D365 CE"},{"location":"D365-F%26O-to-D365-CE/#d365-fo-to-d365-ce","text":"","title":"D365 F&amp;O to D365 CE"},{"location":"D365-F%26O-to-D365-CE/#setup","text":"To integrate from D365 F&O to D365 CE you will need to: - Configure Event Endpoint in F&O - configure Levridge Entity Events - Create an application ID for the integration framework to authenticate to D365 CE - Create an application user in D365 CE and assign the proper role(s) - Create an Azure Service bus topic - Create a subscription on the topic above","title":"Setup"},{"location":"D365-F%26O-to-D365-CE/#configuration","text":"In the appsettings.json you will need to define the SourceConfig and TargetConfig nodes as follows: \"SourceConfig\": { \"ServiceBusConfigName\": [section name with service bus topic], \"ODataConfigName\": [section name with F&O data configuration], \"SystemName\": \"DynamicsAX\", \"Direction\": \"Source\" }, \"TargetConfig\": { \"ODataConfigName\": [section name with CRM data configuration], \"CDSConfigName\": [section name with CDS data configuration], \"SystemName\": \"DynamicsCRM\", \"Direction\": \"Target\" }","title":"Configuration"},{"location":"Deploy-Integration-As-A-Service/","text":"Introduction This webhost application can be run in three modes: As a web application (in Azure or IIS) As a console application As a service Run as a service To run as a service it must be installed. To install as a service you must get the code and place it in the folder from which you want the service to run. The simplest way to get the code is to get the zip file from the latest build. Here are some instructsion for getting the latest code. Get the zip file and unzip it into the folder from which you want the service to run. Deployed with the code should bethree powershell scripts: InstallService.ps1 - this script will install the service InstallEventSource.ps1 - this script will install an event source on the Application EventLog on the local machine RemoveService.ps1 - this script will uninstall / remove the service InstallService.ps1 This script installs a service and sets it to automatically run on startup. It accepts the following command line parameters: PublishPath - the path to the folder from which you want the service to run ServiceUser - user account under which the service will run (default = $env:computername+\"\\IntegrationHost\") ServiceName - the name of the service (default = \"Levridge.Integration.Host\") ServiceDescription - the description for the service (default = \"Levridge Integration Host Service\") ServiceDisplayName - the display name for the service (default = \"Levridge Integration Host\") SourceName - the source name under which EventLog entries should be logged (default = $ServiceName) This script will also execute the InstallEventSource.ps1 InstallEventSource.ps1 This script will install the specified EventLog source. It accepts the following command line parameter: SourceName - the source name under which EventLog entries should be logged (default = \"Levridge.Integration.Host\") RemoveService.ps1 This script will remove the specified service. It accepts the following command line parameter: ServiceName Command line parameters This application can be run with the following command line parameters: debug (-d) - This will cause the application to wait for a debugger to be attached before it continues. This can be helpful to debug startup issues for services or web applications. service (-s) - This will cause the application to run as a service (if the debugger is not attached). SourceName (sn) - This will cause the application to use the specified source name for the application EventLog Source Name. If no source name is specified, the default application source name is use.","title":"Introduction"},{"location":"Deploy-Integration-As-A-Service/#introduction","text":"This webhost application can be run in three modes: As a web application (in Azure or IIS) As a console application As a service","title":"Introduction"},{"location":"Deploy-Integration-As-A-Service/#run-as-a-service","text":"To run as a service it must be installed. To install as a service you must get the code and place it in the folder from which you want the service to run. The simplest way to get the code is to get the zip file from the latest build. Here are some instructsion for getting the latest code. Get the zip file and unzip it into the folder from which you want the service to run. Deployed with the code should bethree powershell scripts: InstallService.ps1 - this script will install the service InstallEventSource.ps1 - this script will install an event source on the Application EventLog on the local machine RemoveService.ps1 - this script will uninstall / remove the service","title":"Run as a service"},{"location":"Deploy-Integration-As-A-Service/#installserviceps1","text":"This script installs a service and sets it to automatically run on startup. It accepts the following command line parameters: PublishPath - the path to the folder from which you want the service to run ServiceUser - user account under which the service will run (default = $env:computername+\"\\IntegrationHost\") ServiceName - the name of the service (default = \"Levridge.Integration.Host\") ServiceDescription - the description for the service (default = \"Levridge Integration Host Service\") ServiceDisplayName - the display name for the service (default = \"Levridge Integration Host\") SourceName - the source name under which EventLog entries should be logged (default = $ServiceName) This script will also execute the InstallEventSource.ps1","title":"InstallService.ps1"},{"location":"Deploy-Integration-As-A-Service/#installeventsourceps1","text":"This script will install the specified EventLog source. It accepts the following command line parameter: SourceName - the source name under which EventLog entries should be logged (default = \"Levridge.Integration.Host\")","title":"InstallEventSource.ps1"},{"location":"Deploy-Integration-As-A-Service/#removeserviceps1","text":"This script will remove the specified service. It accepts the following command line parameter: ServiceName","title":"RemoveService.ps1"},{"location":"Deploy-Integration-As-A-Service/#command-line-parameters","text":"This application can be run with the following command line parameters: debug (-d) - This will cause the application to wait for a debugger to be attached before it continues. This can be helpful to debug startup issues for services or web applications. service (-s) - This will cause the application to run as a service (if the debugger is not attached). SourceName (sn) - This will cause the application to use the specified source name for the application EventLog Source Name. If no source name is specified, the default application source name is use.","title":"Command line parameters"},{"location":"Deploy-Integration-Framework-as-Zip-File/","text":"Deploy Integration Framework as a Zip File At Levridge we build the Integration Framework every night. The output of the build is stored at \\\\devvmhost\\releases . Each build has two folders, \"Integration Framework Main\" and \"Levridge Main\". The integration framework is in the \"drop\" sub-folder of \"Integration Framework Main\". The entire integration framework is contained in the Levridge.Integration.Host.zip file. To deploy the build: Open a browser and navigate to https://<App Service Name>.scm.azurewebsites.net/ZipDeployUI example: https://levdevag.scm.azurewebsites.net/ZipDeployUI From the \"Integration Main\\drop\" folder drag the Levridge.Integration.Host.zip and drop it on the file explorer area on the web page. When deployment is in progress, an icon in the top right corner shows you the progress in percentage. The page also shows verbose messages for the operation below the explorer area. When it is finished, the last deployment message should say \"Deployment successful\". Because this will overwrite the appsettings.json you will need to updated the settings to your desired configuration. Resources Microsoft documentation","title":"Deploy Integration Framework as Zip File"},{"location":"Deploy-Integration-Framework-as-Zip-File/#deploy-integration-framework-as-a-zip-file","text":"At Levridge we build the Integration Framework every night. The output of the build is stored at \\\\devvmhost\\releases . Each build has two folders, \"Integration Framework Main\" and \"Levridge Main\". The integration framework is in the \"drop\" sub-folder of \"Integration Framework Main\". The entire integration framework is contained in the Levridge.Integration.Host.zip file. To deploy the build: Open a browser and navigate to https://<App Service Name>.scm.azurewebsites.net/ZipDeployUI example: https://levdevag.scm.azurewebsites.net/ZipDeployUI From the \"Integration Main\\drop\" folder drag the Levridge.Integration.Host.zip and drop it on the file explorer area on the web page. When deployment is in progress, an icon in the top right corner shows you the progress in percentage. The page also shows verbose messages for the operation below the explorer area. When it is finished, the last deployment message should say \"Deployment successful\". Because this will overwrite the appsettings.json you will need to updated the settings to your desired configuration.","title":"Deploy Integration Framework as a Zip File"},{"location":"Deploy-Integration-Framework-as-Zip-File/#resources","text":"Microsoft documentation","title":"Resources"},{"location":"Deploying-Integration-Framework/","text":"Deploying the Integration Framework There are several ways to deploy the integration framework. Since the framework is a standard Azure App Service we can use any of the deployment options available. The simplest deployment is to deploy a zip file . Select one of the following options for more information on Deploying to Azure: Deploy as Zip file Deploy via FTP Deploy with Script The Integration Framework can also be deployed as a Windows Service . In order to integrate with D365 you must register the integration framework in Active Directory. Here are some articles that explain how to do that: - https://docs.microsoft.com/en-us/azure/active-directory/develop/howto-create-service-principal-portal - https://docs.microsoft.com/en-us/powerapps/developer/common-data-service/use-single-tenant-server-server-authentication#azure-application-registration - https://docs.microsoft.com/en-us/azure/active-directory/azuread-dev/v1-protocols-oauth-code#register-your-application-with-your-ad-tenant In order to access certain Azure resources like Azure Key Vault you will need to make sure your App Service has a system-assigned identity . Next Steps Create Service Bus Configure the application","title":"Deploying the Integration Framework"},{"location":"Deploying-Integration-Framework/#deploying-the-integration-framework","text":"There are several ways to deploy the integration framework. Since the framework is a standard Azure App Service we can use any of the deployment options available. The simplest deployment is to deploy a zip file . Select one of the following options for more information on Deploying to Azure: Deploy as Zip file Deploy via FTP Deploy with Script The Integration Framework can also be deployed as a Windows Service . In order to integrate with D365 you must register the integration framework in Active Directory. Here are some articles that explain how to do that: - https://docs.microsoft.com/en-us/azure/active-directory/develop/howto-create-service-principal-portal - https://docs.microsoft.com/en-us/powerapps/developer/common-data-service/use-single-tenant-server-server-authentication#azure-application-registration - https://docs.microsoft.com/en-us/azure/active-directory/azuread-dev/v1-protocols-oauth-code#register-your-application-with-your-ad-tenant In order to access certain Azure resources like Azure Key Vault you will need to make sure your App Service has a system-assigned identity .","title":"Deploying the Integration Framework"},{"location":"Deploying-Integration-Framework/#next-steps","text":"Create Service Bus Configure the application","title":"Next Steps"},{"location":"DeployingCDS/","text":"Introduction the Levridge Integration Framework uses the CDS repository to store reference data. Because CDS licenses are included with every Dynamics 365 user, this should not result in an additional licensing fees to our customers. This document explains the steps necessary to deploy CDS to a customer environment. Deployment Overview The following steps are necessary to deploy the CDS solutions and data to customer. Create the CDS Environment Create the CDS Database Set Data Loss Prevention policies Configure database security Deploy Solution Deploy Applications Setup Users Import Data","title":"Deploying CDS"},{"location":"DeployingCDS/#introduction","text":"the Levridge Integration Framework uses the CDS repository to store reference data. Because CDS licenses are included with every Dynamics 365 user, this should not result in an additional licensing fees to our customers. This document explains the steps necessary to deploy CDS to a customer environment.","title":"Introduction"},{"location":"DeployingCDS/#deployment","text":"","title":"Deployment"},{"location":"DeployingCDS/#overview","text":"The following steps are necessary to deploy the CDS solutions and data to customer. Create the CDS Environment Create the CDS Database Set Data Loss Prevention policies Configure database security Deploy Solution Deploy Applications Setup Users Import Data","title":"Overview"},{"location":"DeployingCustomFieldMappingAssemblies/","text":"Deploying Custom Plugin Assemblies This document explains how to deploy a custom plugin assembly. Custom plugins can be used for numerous purposes, but the most common is for field mapping assemblies. See Integrating Custom Fields for instructions on how to create a custom field mapping assembly. Overview If you have developed custom plugin assembly you will have to deploy it to a Levridge Integration Framework instance. You will need to deploy the assembly library and any custom dependencies you may have built. Deployment It is important to deploy only the custom assemblies and not any referenced assemblies that are shared with the Levridge Integration Framework. The referenced assemblies will already be deployed with the Levridge Integration Framework. To avoid deploying the shared referenced assemblies, be sure to follow the instructions in the section titled Update the references to be excluded from deployment . You can use visual studio to deploy to a local folder or use Azure DevOps to obtain a zipped folder or your custom assemblies. You will need to copy the custom assemblies into the same folder as the Levridge Integration Framework. Configuration You will need to add a CustomMappingAssemblies node to the appsettings.json file.","title":"Deploying Custom Field Mapping Assemblies"},{"location":"DeployingCustomFieldMappingAssemblies/#deploying-custom-plugin-assemblies","text":"This document explains how to deploy a custom plugin assembly. Custom plugins can be used for numerous purposes, but the most common is for field mapping assemblies. See Integrating Custom Fields for instructions on how to create a custom field mapping assembly.","title":"Deploying Custom Plugin Assemblies"},{"location":"DeployingCustomFieldMappingAssemblies/#overview","text":"If you have developed custom plugin assembly you will have to deploy it to a Levridge Integration Framework instance. You will need to deploy the assembly library and any custom dependencies you may have built.","title":"Overview"},{"location":"DeployingCustomFieldMappingAssemblies/#deployment","text":"It is important to deploy only the custom assemblies and not any referenced assemblies that are shared with the Levridge Integration Framework. The referenced assemblies will already be deployed with the Levridge Integration Framework. To avoid deploying the shared referenced assemblies, be sure to follow the instructions in the section titled Update the references to be excluded from deployment . You can use visual studio to deploy to a local folder or use Azure DevOps to obtain a zipped folder or your custom assemblies. You will need to copy the custom assemblies into the same folder as the Levridge Integration Framework.","title":"Deployment"},{"location":"DeployingCustomFieldMappingAssemblies/#configuration","text":"You will need to add a CustomMappingAssemblies node to the appsettings.json file.","title":"Configuration"},{"location":"EntityMap.AddFieldMap/","text":"EntityMap.AddFieldMap Method Namespace: Levridge.Integration.IntegrationService.Mapping Assemblies: Levridge.Integration.IntegrationService.Mapping.dll Adds a field map to an EntityMap. A field map contains the defintion of the source and target fields to be mapped and any transformation code necessary to perform the map. Overloads Overload Description AddFieldMap<TA, TB>(IEntityField sourceFieldA, IEntityField sourceFieldB) Maps sourceFieldA to sourceFieldB using the default assignment sourceFieldB = sourceFieldA. AddFieldMap<TA, TB>(IEntityField sourceFieldA, IEntityField sourceFieldB, Func<IEntityField, IEntityField, TB> transformA2B, Func<IEntityField, IEntityField, TA> transformB2A) Maps sourceFieldA to sourceFieldB using the transformA2B and transformB2A parameters to provide the transformations. AddFieldMap<TA, TB>(IEntityField sourceFieldA, IEntityField sourceFieldB, Func<IEntityField, IEntityField, ISourceConfiguration, ISourceConfiguration, TB> transformA2B, Func<IEntityField, IEntityField, ISourceConfiguration, ISourceConfiguration, TA> transformB2A) Maps sourceFieldA to sourceFieldB using the transformA2B and transformB2A parameters to provide the transformations. transformA2B and transformB2A take ISourceConfiguration parameters to access the source and target data sources. AddFieldMap (IEntityField sourceFieldA, IEntityField sourceFieldB) Maps sourceFieldA to sourceFieldB using the default assignment sourceFieldB = sourceFieldA. AddFieldMap (IEntityField sourceFieldA, IEntityField sourceFieldB, Func transformA2B, Func transformB2A) Maps sourceFieldA to sourceFieldB using the transformA2B and transformB2A parameters to provide the transformations. AddFieldMap (IEntityField sourceFieldA, IEntityField sourceFieldB, Func transformA2B, Func transformB2A) Maps sourceFieldA to sourceFieldB using the transformA2B and transformB2A parameters to provide the transformations. transformA2B and transformB2A take ISourceConfiguration parameters to access the source and target data sources. AddFieldMap (IEntityField sourceFieldA, IEntityField sourceFieldB) Maps sourceFieldA to sourceFieldB using the default assignment sourceFieldB = sourceFieldA. public IFieldMap AddFieldMap<TA, TB>(IEntityField sourceFieldA, IEntityField sourceFieldB); Parameters Returns Exceptions Examples AddFieldMap (IEntityField sourceFieldA, IEntityField sourceFieldB, Func transformA2B, Func transformB2A) Maps sourceFieldA to sourceFieldB using the transformA2B and transformB2A parameters to provide the transformations. public IFieldMap AddFieldMap<TA, TB>(IEntityField sourceFieldA, IEntityField sourceFieldB, Func<IEntityField, IEntityField, TB> transformA2B, Func<IEntityField, IEntityField, TA> transformB2A); Parameters Returns Exceptions Examples AddFieldMap (IEntityField sourceFieldA, IEntityField sourceFieldB, Func transformA2B, Func transformB2A) Maps sourceFieldA to sourceFieldB using the transformA2B and transformB2A parameters to provide the transformations. transformA2B and transformB2A take ISourceConfiguration parameters to access the source and target data sources. public IFieldMap AddFieldMap<TA, TB>(IEntityField sourceFieldA, IEntityField sourceFieldB, Func<IEntityField, IEntityField, ISourceConfiguration, ISourceConfiguration, TB> transformA2B, Func<IEntityField, IEntityField, ISourceConfiguration, ISourceConfiguration, TA> transformB2A); Parameters Returns Exceptions Examples","title":"EntityMap.AddFieldMap Method"},{"location":"EntityMap.AddFieldMap/#entitymapaddfieldmap-method","text":"Namespace: Levridge.Integration.IntegrationService.Mapping Assemblies: Levridge.Integration.IntegrationService.Mapping.dll Adds a field map to an EntityMap. A field map contains the defintion of the source and target fields to be mapped and any transformation code necessary to perform the map.","title":"EntityMap.AddFieldMap Method"},{"location":"EntityMap.AddFieldMap/#overloads","text":"Overload Description AddFieldMap<TA, TB>(IEntityField sourceFieldA, IEntityField sourceFieldB) Maps sourceFieldA to sourceFieldB using the default assignment sourceFieldB = sourceFieldA. AddFieldMap<TA, TB>(IEntityField sourceFieldA, IEntityField sourceFieldB, Func<IEntityField, IEntityField, TB> transformA2B, Func<IEntityField, IEntityField, TA> transformB2A) Maps sourceFieldA to sourceFieldB using the transformA2B and transformB2A parameters to provide the transformations. AddFieldMap<TA, TB>(IEntityField sourceFieldA, IEntityField sourceFieldB, Func<IEntityField, IEntityField, ISourceConfiguration, ISourceConfiguration, TB> transformA2B, Func<IEntityField, IEntityField, ISourceConfiguration, ISourceConfiguration, TA> transformB2A) Maps sourceFieldA to sourceFieldB using the transformA2B and transformB2A parameters to provide the transformations. transformA2B and transformB2A take ISourceConfiguration parameters to access the source and target data sources. AddFieldMap (IEntityField sourceFieldA, IEntityField sourceFieldB) Maps sourceFieldA to sourceFieldB using the default assignment sourceFieldB = sourceFieldA. AddFieldMap (IEntityField sourceFieldA, IEntityField sourceFieldB, Func transformA2B, Func transformB2A) Maps sourceFieldA to sourceFieldB using the transformA2B and transformB2A parameters to provide the transformations. AddFieldMap (IEntityField sourceFieldA, IEntityField sourceFieldB, Func transformA2B, Func transformB2A) Maps sourceFieldA to sourceFieldB using the transformA2B and transformB2A parameters to provide the transformations. transformA2B and transformB2A take ISourceConfiguration parameters to access the source and target data sources.","title":"Overloads"},{"location":"EntityMap.AddFieldMap/#addfieldmapientityfield-sourcefielda-ientityfield-sourcefieldb","text":"Maps sourceFieldA to sourceFieldB using the default assignment sourceFieldB = sourceFieldA. public IFieldMap AddFieldMap<TA, TB>(IEntityField sourceFieldA, IEntityField sourceFieldB);","title":"AddFieldMap(IEntityField sourceFieldA, IEntityField sourceFieldB)"},{"location":"EntityMap.AddFieldMap/#parameters","text":"","title":"Parameters"},{"location":"EntityMap.AddFieldMap/#returns","text":"","title":"Returns"},{"location":"EntityMap.AddFieldMap/#exceptions","text":"","title":"Exceptions"},{"location":"EntityMap.AddFieldMap/#examples","text":"","title":"Examples"},{"location":"EntityMap.AddFieldMap/#addfieldmapientityfield-sourcefielda-ientityfield-sourcefieldb-func-transforma2b-func-transformb2a","text":"Maps sourceFieldA to sourceFieldB using the transformA2B and transformB2A parameters to provide the transformations. public IFieldMap AddFieldMap<TA, TB>(IEntityField sourceFieldA, IEntityField sourceFieldB, Func<IEntityField, IEntityField, TB> transformA2B, Func<IEntityField, IEntityField, TA> transformB2A);","title":"AddFieldMap(IEntityField sourceFieldA, IEntityField sourceFieldB, Func transformA2B, Func transformB2A)"},{"location":"EntityMap.AddFieldMap/#parameters_1","text":"","title":"Parameters"},{"location":"EntityMap.AddFieldMap/#returns_1","text":"","title":"Returns"},{"location":"EntityMap.AddFieldMap/#exceptions_1","text":"","title":"Exceptions"},{"location":"EntityMap.AddFieldMap/#examples_1","text":"","title":"Examples"},{"location":"EntityMap.AddFieldMap/#addfieldmapientityfield-sourcefielda-ientityfield-sourcefieldb-func-transforma2b-func-transformb2a_1","text":"Maps sourceFieldA to sourceFieldB using the transformA2B and transformB2A parameters to provide the transformations. transformA2B and transformB2A take ISourceConfiguration parameters to access the source and target data sources. public IFieldMap AddFieldMap<TA, TB>(IEntityField sourceFieldA, IEntityField sourceFieldB, Func<IEntityField, IEntityField, ISourceConfiguration, ISourceConfiguration, TB> transformA2B, Func<IEntityField, IEntityField, ISourceConfiguration, ISourceConfiguration, TA> transformB2A);","title":"AddFieldMap(IEntityField sourceFieldA, IEntityField sourceFieldB, Func transformA2B, Func transformB2A)"},{"location":"EntityMap.AddFieldMap/#parameters_2","text":"","title":"Parameters"},{"location":"EntityMap.AddFieldMap/#returns_2","text":"","title":"Returns"},{"location":"EntityMap.AddFieldMap/#exceptions_2","text":"","title":"Exceptions"},{"location":"EntityMap.AddFieldMap/#examples_2","text":"","title":"Examples"},{"location":"EntityMapBuilderExtensions.AddEntityMap/","text":"EntityMapBuilderExtensions.AddEntityMap Method Namespace: Levridge.Integration.IntegrationService.Mapping Assemblies: Levridge.Integration.IntegrationService.Mapping.dll Adds an entity to the EntityMap. An entity map contains the defintion of the source and target entities to be mapped and the necessary code to configure the field maps for the entity. Overloads Overload Description AddEntityMap (this IEntityMapBuilder builder, Action > configure) Maps sourceFieldA to sourceFieldB using the default assignment sourceFieldB = sourceFieldA. AddEntityMap(this IEntityMapBuilder builder, string entityTypeA, string entityTypeB, Action configure) Maps sourceFieldA to sourceFieldB using the transformA2B and transformB2A parameters to provide the transformations. AddEntityMap (this IEntityMapBuilder builder, Action > configure) Adds an Entity Map to the IEntityMapBuilder and provides a configuration action to configure the Entity Mapping. public static IEntityMapBuilder AddEntityMap<TLeft, TRight>(this IEntityMapBuilder builder, Action<EntityMap<TLeft, TRight>> configure); Parameters Returns Exceptions Examples AddEntityMap(this IEntityMapBuilder builder, string entityTypeA, string entityTypeB, Action configure) Adds an Entity Map to the IEntityMapBuilder and provides a configuration action to configure the Entity Mapping. public static IEntityMapBuilder AddEntityMap(this IEntityMapBuilder builder, string entityTypeA, string entityTypeB, Action<EntityMap> configure); Parameters Returns Exceptions Examples","title":"EntityMapBuilderExtensions.AddEntityMap Method"},{"location":"EntityMapBuilderExtensions.AddEntityMap/#entitymapbuilderextensionsaddentitymap-method","text":"Namespace: Levridge.Integration.IntegrationService.Mapping Assemblies: Levridge.Integration.IntegrationService.Mapping.dll Adds an entity to the EntityMap. An entity map contains the defintion of the source and target entities to be mapped and the necessary code to configure the field maps for the entity.","title":"EntityMapBuilderExtensions.AddEntityMap Method"},{"location":"EntityMapBuilderExtensions.AddEntityMap/#overloads","text":"Overload Description AddEntityMap (this IEntityMapBuilder builder, Action > configure) Maps sourceFieldA to sourceFieldB using the default assignment sourceFieldB = sourceFieldA. AddEntityMap(this IEntityMapBuilder builder, string entityTypeA, string entityTypeB, Action configure) Maps sourceFieldA to sourceFieldB using the transformA2B and transformB2A parameters to provide the transformations.","title":"Overloads"},{"location":"EntityMapBuilderExtensions.AddEntityMap/#addentitymapthis-ientitymapbuilder-builder-action-configure","text":"Adds an Entity Map to the IEntityMapBuilder and provides a configuration action to configure the Entity Mapping. public static IEntityMapBuilder AddEntityMap<TLeft, TRight>(this IEntityMapBuilder builder, Action<EntityMap<TLeft, TRight>> configure);","title":"AddEntityMap(this IEntityMapBuilder builder, Action configure)"},{"location":"EntityMapBuilderExtensions.AddEntityMap/#parameters","text":"","title":"Parameters"},{"location":"EntityMapBuilderExtensions.AddEntityMap/#returns","text":"","title":"Returns"},{"location":"EntityMapBuilderExtensions.AddEntityMap/#exceptions","text":"","title":"Exceptions"},{"location":"EntityMapBuilderExtensions.AddEntityMap/#examples","text":"","title":"Examples"},{"location":"EntityMapBuilderExtensions.AddEntityMap/#addentitymapthis-ientitymapbuilder-builder-string-entitytypea-string-entitytypeb-action-configure","text":"Adds an Entity Map to the IEntityMapBuilder and provides a configuration action to configure the Entity Mapping. public static IEntityMapBuilder AddEntityMap(this IEntityMapBuilder builder, string entityTypeA, string entityTypeB, Action<EntityMap> configure);","title":"AddEntityMap(this IEntityMapBuilder builder, string entityTypeA, string entityTypeB, Action configure)"},{"location":"EntityMapBuilderExtensions.AddEntityMap/#parameters_1","text":"","title":"Parameters"},{"location":"EntityMapBuilderExtensions.AddEntityMap/#returns_1","text":"","title":"Returns"},{"location":"EntityMapBuilderExtensions.AddEntityMap/#exceptions_1","text":"","title":"Exceptions"},{"location":"EntityMapBuilderExtensions.AddEntityMap/#examples_1","text":"","title":"Examples"},{"location":"Environment-variables/","text":"Introduction Brief introduction of the module, component or feature being documented. This document explains ... Overview Main Point 1 Sub Point 1.1","title":"Introduction"},{"location":"Environment-variables/#introduction","text":"Brief introduction of the module, component or feature being documented. This document explains ...","title":"Introduction"},{"location":"Environment-variables/#overview","text":"","title":"Overview"},{"location":"Environment-variables/#main-point-1","text":"","title":"Main Point 1"},{"location":"Environment-variables/#sub-point-11","text":"","title":"Sub Point 1.1"},{"location":"EnvironmentPlanning/","text":"Environment Planning A standard D365 implementation is used when launching a Levridge environment plan. The standard D365 implementation guidelines are captured in the Stoneridge Software Asset Library . Overview D365 is set up with Levridge sitting on top of the functionality. With the start of a new project, the team performs a technical review of what is required and outlines a plan that fits the client\u2019s needs. The team utilizes the latest Microsoft Environment planning when implementing a new project due to information continually being revised and updated. The Microsoft Environment planning provides an overview of various aspects that you must consider while you plan for your project's environment. To help guarantee a successful cloud implementation, it is important that you discuss and plan your environment early in the project. Getting Started Configure your Azure Subscription Create a D365 Subscription Create Azure Active Directory accounts Request vCPU quota increase for planned environments Azure VM Quota Increase Deploy your Azure DevOps project Microsoft Azure Dev Ops Create a security token using your D365Admin account (Used for integration with Lifecycle Services) Deploy your Lifecycle Services project (D365 licensing required) Microsoft Dynamics Lifecycle Services Complete the Lifecycle Services Onboarding workflow Includes DevOps integration and Azure integration for environment deployments Project Onboarding Deploy environments Deployed via Lifecycle Services Lifecycle Services (LCS) user guide Configure Levridge Integrations Levridge Integration Deployment Procedures D365 F&O System Requirements Azure Tenant Azure Active Directory Accounts D365Admin D365Test1 D365Test2 LevridgeIntegrations Azure Execution Account (Manage Azure VM schedules) Lifecycle Services Project (included with D365 licensing) Azure DevOps (Cloud based) D365 F&O Environments (Deploy from Lifecycle Services) D365 CE Environments (Deploy from 365 Admin Portal) Azure Integration Components Document Routing Agent (Installed on local infrastructure to support server-based network printing) Power BI Subscription","title":"Environment Planning"},{"location":"EnvironmentPlanning/#environment-planning","text":"A standard D365 implementation is used when launching a Levridge environment plan. The standard D365 implementation guidelines are captured in the Stoneridge Software Asset Library .","title":"Environment Planning"},{"location":"EnvironmentPlanning/#overview","text":"D365 is set up with Levridge sitting on top of the functionality. With the start of a new project, the team performs a technical review of what is required and outlines a plan that fits the client\u2019s needs. The team utilizes the latest Microsoft Environment planning when implementing a new project due to information continually being revised and updated. The Microsoft Environment planning provides an overview of various aspects that you must consider while you plan for your project's environment. To help guarantee a successful cloud implementation, it is important that you discuss and plan your environment early in the project.","title":"Overview"},{"location":"EnvironmentPlanning/#getting-started","text":"Configure your Azure Subscription Create a D365 Subscription Create Azure Active Directory accounts Request vCPU quota increase for planned environments Azure VM Quota Increase Deploy your Azure DevOps project Microsoft Azure Dev Ops Create a security token using your D365Admin account (Used for integration with Lifecycle Services) Deploy your Lifecycle Services project (D365 licensing required) Microsoft Dynamics Lifecycle Services Complete the Lifecycle Services Onboarding workflow Includes DevOps integration and Azure integration for environment deployments Project Onboarding Deploy environments Deployed via Lifecycle Services Lifecycle Services (LCS) user guide Configure Levridge Integrations Levridge Integration Deployment Procedures","title":"Getting Started"},{"location":"EnvironmentPlanning/#d365-fo-system-requirements","text":"Azure Tenant Azure Active Directory Accounts D365Admin D365Test1 D365Test2 LevridgeIntegrations Azure Execution Account (Manage Azure VM schedules) Lifecycle Services Project (included with D365 licensing) Azure DevOps (Cloud based) D365 F&O Environments (Deploy from Lifecycle Services) D365 CE Environments (Deploy from 365 Admin Portal) Azure Integration Components Document Routing Agent (Installed on local infrastructure to support server-based network printing) Power BI Subscription","title":"D365 F&amp;O System Requirements"},{"location":"Equity/","text":"Introduction Brief introduction of the module, component or feature being documented. This document explains ... Overview Main Point 1 Sub Point 1.1","title":"Introduction"},{"location":"Equity/#introduction","text":"Brief introduction of the module, component or feature being documented. This document explains ...","title":"Introduction"},{"location":"Equity/#overview","text":"","title":"Overview"},{"location":"Equity/#main-point-1","text":"","title":"Main Point 1"},{"location":"Equity/#sub-point-11","text":"","title":"Sub Point 1.1"},{"location":"Feed/","text":"Feed Overview Main Point 1 Sub Point 1.1","title":"Feed"},{"location":"Feed/#feed","text":"","title":"Feed"},{"location":"Feed/#overview","text":"","title":"Overview"},{"location":"Feed/#main-point-1","text":"","title":"Main Point 1"},{"location":"Feed/#sub-point-11","text":"","title":"Sub Point 1.1"},{"location":"Field-Integration/","text":"Field Integration The Field integration is a unidirectional integration that receives messages to create a Levridge Customer Site using a configured Customer Site Type that represents a customer field. Overview The Field Integration API is a Webhook that receives a posted message that is used to create a new Customer Site in Levridge. API POST /api/field HTTP/1.1 Host: [Customer Integration Framework Host] Content-Type: application/json { \"growerID\": \"CUS-000071\", \"growerUuid\": \"5ddf9e57-b1ba-4af5-90bf-e9454e87f170\", \"farmID\": \"COP000010\", \"farmUuid\": \"5ddf9e57-b1ba-4af5-90bf-e9454e87f171\", \"fieldID\": \"CST-000150\", \"fieldUuid\":\"5ddf9e57-b1ba-4af5-90bf-e9454e87f172\", \"fieldName\": \"Test Field Type\", \"fieldWkt\" : \"MULTIPOLYGON(((-97.027808062981 46.999026392986,-97.017851703117 46.998792242651,-97.018023364494 46.991884346149,-97.027636401605 46.991767255461,-97.027808062981 46.999026392986)))\", \"farmableAcres\" : 19.5 } Authentication Error Codes Rate limit Message Example { \"growerID\": \"CUS-000071\", \"growerUuid\": \"5ddf9e57-b1ba-4af5-90bf-e9454e87f170\", \"farmID\": \"COP000010\", \"farmUuid\": \"5ddf9e57-b1ba-4af5-90bf-e9454e87f171\", \"fieldID\": \"CST-000150\", \"fieldUuid\":\"5ddf9e57-b1ba-4af5-90bf-e9454e87f172\", \"fieldName\": \"Test Field Type\", \"fieldWkt\" : \"MULTIPOLYGON(((-97.027808062981 46.999026392986,-97.017851703117 46.998792242651,-97.018023364494 46.991884346149,-97.027636401605 46.991767255461,-97.027808062981 46.999026392986)))\", \"farmableAcres\" : 19.5 } Message Schema 1 { 2 \"definitions\": {}, 3 \"$schema\": \"http://json-schema.org/draft-07/schema#\", 4 \"$id\": \"http://example.com/root.json\", 5 \"type\": \"object\", 6 \"title\": \"The Root Schema\", 7 \"required\": [ 8 \"growerID\", 9 \"farmID\", 10 \"fieldID\", 11 \"fieldName\" 12 ], 13 \"properties\": { 14 \"growerID\": { 15 \"$id\": \"#/properties/growerID\", 16 \"type\": \"string\", 17 \"title\": \"The Growerid Schema\", 18 \"default\": \"\", 19 \"examples\": [ 20 \"CUS-000071\" 21 ], 22 \"pattern\": \"^(.*)$\" 23 }, 24 \"growerUuid\": { 25 \"$id\": \"#/properties/growerUuid\", 26 \"type\": \"string\", 27 \"title\": \"The Groweruuid Schema\", 28 \"default\": \"\", 29 \"examples\": [ 30 \"\" 31 ], 32 \"pattern\": \"^(.*)$\" 33 }, 34 \"farmID\": { 35 \"$id\": \"#/properties/farmID\", 36 \"type\": \"string\", 37 \"title\": \"The Farmid Schema\", 38 \"default\": \"\", 39 \"examples\": [ 40 \"COP000010\" 41 ], 42 \"pattern\": \"^(.*)$\" 43 }, 44 \"farmUuid\": { 45 \"$id\": \"#/properties/farmUuid\", 46 \"type\": \"string\", 47 \"title\": \"The Farmuuid Schema\", 48 \"default\": \"\", 49 \"examples\": [ 50 \"\" 51 ], 52 \"pattern\": \"^(.*)$\" 53 }, 54 \"fieldID\": { 55 \"$id\": \"#/properties/fieldID\", 56 \"type\": \"string\", 57 \"title\": \"The Fieldid Schema\", 58 \"default\": \"\", 59 \"examples\": [ 60 \"CST-000150\" 61 ], 62 \"pattern\": \"^(.*)$\" 63 }, 64 \"fieldUuid\": { 65 \"$id\": \"#/properties/fieldUuid\", 66 \"type\": \"string\", 67 \"title\": \"The Fielduuid Schema\", 68 \"default\": \"\", 69 \"examples\": [ 70 \"5ddf9e57-b1ba-4af5-90bf-e9454e87f172\" 71 ], 72 \"pattern\": \"^(.*)$\" 73 }, 74 \"fieldName\": { 75 \"$id\": \"#/properties/fieldName\", 76 \"type\": \"string\", 77 \"title\": \"The Fieldname Schema\", 78 \"default\": \"\", 79 \"examples\": [ 80 \"Test Field Type\" 81 ], 82 \"pattern\": \"^(.*)$\" 83 }, 84 \"fieldWkt\": { 85 \"$id\": \"#/properties/fieldWkt\", 86 \"type\": \"string\", 87 \"title\": \"The Fieldwkt Schema\", 88 \"default\": \"\", 89 \"examples\": [ 90 \"MULTIPOLYGON(((-97.027808062981 46.999026392986,-97.017851703117 46.998792242651,-97.018023364494 46.991884346149,-97.027636401605 46.991767255461,-97.027808062981 46.999026392986)))\" 91 ], 92 \"pattern\": \"^(.*)$\" 93 }, 94 \"farmableAcres\": { 95 \"$id\": \"#/properties/farmableAcres\", 96 \"type\": \"number\", 97 \"title\": \"The Farmableacres Schema\", 98 \"default\": 0.0, 99 \"examples\": [ 100 19.5 101 ] 102 } 103 } 104 } Configuration In the appsettings.json you will need to define a section named \"Levridge.Integration.Host.FieldController\". The controller looks for this section to get the Service Bus information needed to place messages into a Topic. You can run the Field to CRM integration in the same instance simply by pointing the SourceConfig.ServiceBusConfigName to \"Levridge.Integration.Host.FieldController\" also, or a section that defines a connection to the same topic the controller is sending the message to. For the instance that handles the integration mapping the SourceConfig and TargetConfig nodes should be setup as follows: \"SourceConfig\": { \"ServiceBusConfigName\": [section name with service bus topic], \"ODataConfigName\": \"\", \"SystemName\": \"Field\", \"Direction\": \"Source\" }, \"TargetConfig\": { \"ODataConfigName\": [section name with CRM data configuration], \"CDSConfigName\": [section name with CDS data configuration], \"SystemName\": \"DynamicsCRM\", \"Direction\": \"Target\" } In the appsettings.json you will need to add the assembly into the list of controllers. It doesn't matter what name you assign to the assembly entry but the assembly name must be \"Levridge.Integration.Host.FieldController\". In the example below the assembly is named \"FieldController\". (i.e. \"FieldController\": \"Levridge.Integration.Host.FieldController\") \"Controllers\": { \"HostController\": \"Levridge.Integration.Host.DefaultController\", \"FieldController\": \"Levridge.Integration.Host.FieldController\" } The \"Controllers\" section above will load the DefaultController and the FieldController. { \"Controllers\": { \"HostController\": \"Levridge.Integration.Host.DefaultController\", \"FieldController\": \"Levridge.Integration.Host.FieldController\" }, \"Logging\": { \"Debug\": { \"LogLevel\": { \"Default\": \"Information\" } }, \"Console\": { \"IncludeScopes\": true, \"LogLevel\": { \"Default\": \"Information\" } }, \"LogLevel\": { \"Default\": \"Information\" } }, \"AllowedHosts\": \"*\", \"InstanceConfig\": { \"AzureTableConfiguration\": \"AzureTableConfiguration\" }, \"SourceConfig\": { \"ServiceBusConfigName\": \"Levridge.Integration.Host.FieldController\", \"ODataConfigName\": \"CDS\", \"SystemName\": \"Field\", \"Direction\": \"Source\" }, \"TargetConfig\": { \"ODataConfigName\": \"DynamicsCRM\", \"SystemName\": \"DynamicsCRM\", \"Direction\": \"Target\", \"CDSConfigName\": \"CDS\" }, \"DynamicsCRM\": { \"UriString\": \"https://localhost\", \"ActiveDirectoryResource\": \"https://[Customer CRM Instance].crm.dynamics.com\", \"ActiveDirectoryTenant\": \"https://login.microsoftonline.com/2e14a5b1-fbf8-415b-bc7d-93e20829e510\", \"ActiveDirectoryClientAppId\": \"[Application ID]\", \"ActiveDirectoryClientAppSecret\": \"[Client Secret]\", \"ODataEntityPath\": \"https://[Customer CRM Instance].crm.dynamics.com/api/data/v9.0/\", \"AssemblyName\": \"Levridge.ODataDataSources.DynamicsCRM\", \"AssemblyFile\": \"Levridge.ODataDataSources.DynamicsCRM.dll\", \"ClientClassesNameSpace\": \"Levridge.ODataDataSources.DynamicsCRM\", \"MetadataResource\": \"CRMMetadata.xml\" }, \"CDS\": { \"UriString\": \"https://localhost\", \"ActiveDirectoryResource\": \"https://[Customer CDS Instance].api.crm.dynamics.com\", \"ActiveDirectoryTenant\": \"https://login.microsoftonline.com/2e14a5b1-fbf8-415b-bc7d-93e20829e510\", \"ActiveDirectoryClientAppId\": \"[Application ID]\", \"ActiveDirectoryClientAppSecret\": \"[Client Secret]\", \"ODataEntityPath\": \"https://[Customer CDS Instance].api.crm.dynamics.com/api/data/v9.0/\", \"AssemblyName\": \"Levridge.ODataDataSources.CDS\", \"ClientClassesNameSpace\": \"Levridge.ODataDataSources.CDS\", \"MetadataResource\": \"CDSMetadata.xml\" }, \"AzureTableConfiguration\": { \"ConnectionString\": \"DefaultEndpointsProtocol=https;AccountName=levridgegeneralstorage;AccountKey=MFkzIfLUU1KyCMxfZVPk7HelhWlC0TZyBnFDMEty4y3D4YnqgA4RHSHu8es+R91C/MmDNjtuKJ1x8yDMJ4vLGA==;EndpointSuffix=core.windows.net\", \"Table\": \"FinOpsAndCRM\" }, \"Levridge.Integration.Host.FieldController\": { // used by Webhook \"ConnectionString\": \"Endpoint=sb://[Customer Service Bus Instance].servicebus.windows.net/;SharedAccessKeyName=RootManageSharedAccessKey;SharedAccessKey=[Customer Access Key]\", \"TopicName\": \"[Customer Topic Name]\", \"SubscriptionName\": \"[Customer Subscription Name]\", \"RequiresSession\": true, \"CustomerSiteTypeCode\": \"[CustomerSite TypeCode from FinOps]\", \"IdOption\": \"FieldId\" } }","title":"Field Integration"},{"location":"Field-Integration/#field-integration","text":"The Field integration is a unidirectional integration that receives messages to create a Levridge Customer Site using a configured Customer Site Type that represents a customer field.","title":"Field Integration"},{"location":"Field-Integration/#overview","text":"The Field Integration API is a Webhook that receives a posted message that is used to create a new Customer Site in Levridge.","title":"Overview"},{"location":"Field-Integration/#api","text":"POST /api/field HTTP/1.1 Host: [Customer Integration Framework Host] Content-Type: application/json { \"growerID\": \"CUS-000071\", \"growerUuid\": \"5ddf9e57-b1ba-4af5-90bf-e9454e87f170\", \"farmID\": \"COP000010\", \"farmUuid\": \"5ddf9e57-b1ba-4af5-90bf-e9454e87f171\", \"fieldID\": \"CST-000150\", \"fieldUuid\":\"5ddf9e57-b1ba-4af5-90bf-e9454e87f172\", \"fieldName\": \"Test Field Type\", \"fieldWkt\" : \"MULTIPOLYGON(((-97.027808062981 46.999026392986,-97.017851703117 46.998792242651,-97.018023364494 46.991884346149,-97.027636401605 46.991767255461,-97.027808062981 46.999026392986)))\", \"farmableAcres\" : 19.5 }","title":"API"},{"location":"Field-Integration/#authentication","text":"","title":"Authentication"},{"location":"Field-Integration/#error-codes","text":"","title":"Error Codes"},{"location":"Field-Integration/#rate-limit","text":"","title":"Rate limit"},{"location":"Field-Integration/#message-example","text":"{ \"growerID\": \"CUS-000071\", \"growerUuid\": \"5ddf9e57-b1ba-4af5-90bf-e9454e87f170\", \"farmID\": \"COP000010\", \"farmUuid\": \"5ddf9e57-b1ba-4af5-90bf-e9454e87f171\", \"fieldID\": \"CST-000150\", \"fieldUuid\":\"5ddf9e57-b1ba-4af5-90bf-e9454e87f172\", \"fieldName\": \"Test Field Type\", \"fieldWkt\" : \"MULTIPOLYGON(((-97.027808062981 46.999026392986,-97.017851703117 46.998792242651,-97.018023364494 46.991884346149,-97.027636401605 46.991767255461,-97.027808062981 46.999026392986)))\", \"farmableAcres\" : 19.5 }","title":"Message Example"},{"location":"Field-Integration/#message-schema","text":"1 { 2 \"definitions\": {}, 3 \"$schema\": \"http://json-schema.org/draft-07/schema#\", 4 \"$id\": \"http://example.com/root.json\", 5 \"type\": \"object\", 6 \"title\": \"The Root Schema\", 7 \"required\": [ 8 \"growerID\", 9 \"farmID\", 10 \"fieldID\", 11 \"fieldName\" 12 ], 13 \"properties\": { 14 \"growerID\": { 15 \"$id\": \"#/properties/growerID\", 16 \"type\": \"string\", 17 \"title\": \"The Growerid Schema\", 18 \"default\": \"\", 19 \"examples\": [ 20 \"CUS-000071\" 21 ], 22 \"pattern\": \"^(.*)$\" 23 }, 24 \"growerUuid\": { 25 \"$id\": \"#/properties/growerUuid\", 26 \"type\": \"string\", 27 \"title\": \"The Groweruuid Schema\", 28 \"default\": \"\", 29 \"examples\": [ 30 \"\" 31 ], 32 \"pattern\": \"^(.*)$\" 33 }, 34 \"farmID\": { 35 \"$id\": \"#/properties/farmID\", 36 \"type\": \"string\", 37 \"title\": \"The Farmid Schema\", 38 \"default\": \"\", 39 \"examples\": [ 40 \"COP000010\" 41 ], 42 \"pattern\": \"^(.*)$\" 43 }, 44 \"farmUuid\": { 45 \"$id\": \"#/properties/farmUuid\", 46 \"type\": \"string\", 47 \"title\": \"The Farmuuid Schema\", 48 \"default\": \"\", 49 \"examples\": [ 50 \"\" 51 ], 52 \"pattern\": \"^(.*)$\" 53 }, 54 \"fieldID\": { 55 \"$id\": \"#/properties/fieldID\", 56 \"type\": \"string\", 57 \"title\": \"The Fieldid Schema\", 58 \"default\": \"\", 59 \"examples\": [ 60 \"CST-000150\" 61 ], 62 \"pattern\": \"^(.*)$\" 63 }, 64 \"fieldUuid\": { 65 \"$id\": \"#/properties/fieldUuid\", 66 \"type\": \"string\", 67 \"title\": \"The Fielduuid Schema\", 68 \"default\": \"\", 69 \"examples\": [ 70 \"5ddf9e57-b1ba-4af5-90bf-e9454e87f172\" 71 ], 72 \"pattern\": \"^(.*)$\" 73 }, 74 \"fieldName\": { 75 \"$id\": \"#/properties/fieldName\", 76 \"type\": \"string\", 77 \"title\": \"The Fieldname Schema\", 78 \"default\": \"\", 79 \"examples\": [ 80 \"Test Field Type\" 81 ], 82 \"pattern\": \"^(.*)$\" 83 }, 84 \"fieldWkt\": { 85 \"$id\": \"#/properties/fieldWkt\", 86 \"type\": \"string\", 87 \"title\": \"The Fieldwkt Schema\", 88 \"default\": \"\", 89 \"examples\": [ 90 \"MULTIPOLYGON(((-97.027808062981 46.999026392986,-97.017851703117 46.998792242651,-97.018023364494 46.991884346149,-97.027636401605 46.991767255461,-97.027808062981 46.999026392986)))\" 91 ], 92 \"pattern\": \"^(.*)$\" 93 }, 94 \"farmableAcres\": { 95 \"$id\": \"#/properties/farmableAcres\", 96 \"type\": \"number\", 97 \"title\": \"The Farmableacres Schema\", 98 \"default\": 0.0, 99 \"examples\": [ 100 19.5 101 ] 102 } 103 } 104 }","title":"Message Schema"},{"location":"Field-Integration/#configuration","text":"In the appsettings.json you will need to define a section named \"Levridge.Integration.Host.FieldController\". The controller looks for this section to get the Service Bus information needed to place messages into a Topic. You can run the Field to CRM integration in the same instance simply by pointing the SourceConfig.ServiceBusConfigName to \"Levridge.Integration.Host.FieldController\" also, or a section that defines a connection to the same topic the controller is sending the message to. For the instance that handles the integration mapping the SourceConfig and TargetConfig nodes should be setup as follows: \"SourceConfig\": { \"ServiceBusConfigName\": [section name with service bus topic], \"ODataConfigName\": \"\", \"SystemName\": \"Field\", \"Direction\": \"Source\" }, \"TargetConfig\": { \"ODataConfigName\": [section name with CRM data configuration], \"CDSConfigName\": [section name with CDS data configuration], \"SystemName\": \"DynamicsCRM\", \"Direction\": \"Target\" } In the appsettings.json you will need to add the assembly into the list of controllers. It doesn't matter what name you assign to the assembly entry but the assembly name must be \"Levridge.Integration.Host.FieldController\". In the example below the assembly is named \"FieldController\". (i.e. \"FieldController\": \"Levridge.Integration.Host.FieldController\") \"Controllers\": { \"HostController\": \"Levridge.Integration.Host.DefaultController\", \"FieldController\": \"Levridge.Integration.Host.FieldController\" } The \"Controllers\" section above will load the DefaultController and the FieldController. { \"Controllers\": { \"HostController\": \"Levridge.Integration.Host.DefaultController\", \"FieldController\": \"Levridge.Integration.Host.FieldController\" }, \"Logging\": { \"Debug\": { \"LogLevel\": { \"Default\": \"Information\" } }, \"Console\": { \"IncludeScopes\": true, \"LogLevel\": { \"Default\": \"Information\" } }, \"LogLevel\": { \"Default\": \"Information\" } }, \"AllowedHosts\": \"*\", \"InstanceConfig\": { \"AzureTableConfiguration\": \"AzureTableConfiguration\" }, \"SourceConfig\": { \"ServiceBusConfigName\": \"Levridge.Integration.Host.FieldController\", \"ODataConfigName\": \"CDS\", \"SystemName\": \"Field\", \"Direction\": \"Source\" }, \"TargetConfig\": { \"ODataConfigName\": \"DynamicsCRM\", \"SystemName\": \"DynamicsCRM\", \"Direction\": \"Target\", \"CDSConfigName\": \"CDS\" }, \"DynamicsCRM\": { \"UriString\": \"https://localhost\", \"ActiveDirectoryResource\": \"https://[Customer CRM Instance].crm.dynamics.com\", \"ActiveDirectoryTenant\": \"https://login.microsoftonline.com/2e14a5b1-fbf8-415b-bc7d-93e20829e510\", \"ActiveDirectoryClientAppId\": \"[Application ID]\", \"ActiveDirectoryClientAppSecret\": \"[Client Secret]\", \"ODataEntityPath\": \"https://[Customer CRM Instance].crm.dynamics.com/api/data/v9.0/\", \"AssemblyName\": \"Levridge.ODataDataSources.DynamicsCRM\", \"AssemblyFile\": \"Levridge.ODataDataSources.DynamicsCRM.dll\", \"ClientClassesNameSpace\": \"Levridge.ODataDataSources.DynamicsCRM\", \"MetadataResource\": \"CRMMetadata.xml\" }, \"CDS\": { \"UriString\": \"https://localhost\", \"ActiveDirectoryResource\": \"https://[Customer CDS Instance].api.crm.dynamics.com\", \"ActiveDirectoryTenant\": \"https://login.microsoftonline.com/2e14a5b1-fbf8-415b-bc7d-93e20829e510\", \"ActiveDirectoryClientAppId\": \"[Application ID]\", \"ActiveDirectoryClientAppSecret\": \"[Client Secret]\", \"ODataEntityPath\": \"https://[Customer CDS Instance].api.crm.dynamics.com/api/data/v9.0/\", \"AssemblyName\": \"Levridge.ODataDataSources.CDS\", \"ClientClassesNameSpace\": \"Levridge.ODataDataSources.CDS\", \"MetadataResource\": \"CDSMetadata.xml\" }, \"AzureTableConfiguration\": { \"ConnectionString\": \"DefaultEndpointsProtocol=https;AccountName=levridgegeneralstorage;AccountKey=MFkzIfLUU1KyCMxfZVPk7HelhWlC0TZyBnFDMEty4y3D4YnqgA4RHSHu8es+R91C/MmDNjtuKJ1x8yDMJ4vLGA==;EndpointSuffix=core.windows.net\", \"Table\": \"FinOpsAndCRM\" }, \"Levridge.Integration.Host.FieldController\": { // used by Webhook \"ConnectionString\": \"Endpoint=sb://[Customer Service Bus Instance].servicebus.windows.net/;SharedAccessKeyName=RootManageSharedAccessKey;SharedAccessKey=[Customer Access Key]\", \"TopicName\": \"[Customer Topic Name]\", \"SubscriptionName\": \"[Customer Subscription Name]\", \"RequiresSession\": true, \"CustomerSiteTypeCode\": \"[CustomerSite TypeCode from FinOps]\", \"IdOption\": \"FieldId\" } }","title":"Configuration"},{"location":"Field-Reveal/","text":"Field Reveal Integration There are two aspects of integration with Field Reveal . Recommendations Fields Recommendation Integrations The Recommendation Integration is a general integration provided by the Levridge API. Field Reveal has built the capabillites to send a perscription for a specific field to Levridge through our Recommendation Integration API . Field Integrations The Field Integration is a general integration provided by the Levridge API. Field Reveal has built the capabillites to send field information to Levridge through our Field Integration API .","title":"Field Reveal"},{"location":"Field-Reveal/#field-reveal-integration","text":"There are two aspects of integration with Field Reveal . Recommendations Fields","title":"Field Reveal Integration"},{"location":"Field-Reveal/#recommendation-integrations","text":"The Recommendation Integration is a general integration provided by the Levridge API. Field Reveal has built the capabillites to send a perscription for a specific field to Levridge through our Recommendation Integration API .","title":"Recommendation Integrations"},{"location":"Field-Reveal/#field-integrations","text":"The Field Integration is a general integration provided by the Levridge API. Field Reveal has built the capabillites to send field information to Levridge through our Field Integration API .","title":"Field Integrations"},{"location":"FinanceProgramProcesses/","text":"Finance Program Processes This document outlines other finance program related processes. Process Finance Program Payments via Transfer Scenario: Payment is accepted to a central customer account for the Finance program. Funds are then transferred from this account to the intended Customer(s) using a specially configured Customer payment journal. 1. Go to Accounts receivable > Payments > Customer payment journal. 2. Click New. 3. In the list, select the applicable journal that supports a transfer between customer accounts. 4. Click Lines. 5. In the Account field, specify the customer to transfer payment FROM. 6. Tab off the account field. The customer name will populate to the field. 7. Click Settle transactions. 8. In the list, locate the payment/credit transaction to be transferred. a. Hint: May need to add personalization to settlement screen to include and display the Method of payment. 9. Select the Mark check box. 10. Click OK. 11. The debit value will default to the journal line. 12. In the Description field, type a value such a JDF payment transfer. 13. Offset account type is already set to Customer. In the Offset account field, specify the customer to transfer payment TO. a. Alternate: Click the arrow down key to enter multiple lines for multiple customers to transfer TO. Be sure to enter the appropriate Credit value for each added line before using arrow down to ensure the voucher number is retained. 14. In the Method of payment field, enter or select the value which represents the Finance program. 15. Click Validate. Address any errors. 16. To verify the accounting entry is the expected outcome: a. Click Print. b. Click Journal. c. Set Totals = Yes d. Click OK. 17. Close the page when review is complete. 18. When the journal has been completed. Click Post. Process Finance Program Refund via Transfer Scenario: Payment is applied to a customer account for the Finance program from the finance company. Funds are not used and must be refunded back to the Finance company. The Finance company has a customer account. Funds are transferred from the Customer(s) to the Finance account using a specially configured Customer payment journal. Once this journal is posted, the Reimbursement process may be executed. 1. Go to Accounts receivable > Payments > Customer payment journal. 2. Click New. 3. In the list, select the applicable journal that supports a transfer between customer accounts. 4. Click Lines. 5. In the Account field, specify the customer to transfer payment FROM. 6. Tab off the account field. The customer name will populate to the field. 7. Click Settle transactions. 8. In the list, locate the payment/credit transaction to be transferred. a. Hint: May need to add personalization to settlement screen to include and display the Method of payment. 9. Select the Mark check box. 10. Click OK. 11. The debit value will default to the journal line. 12. In the Description field, type a value such a JDF payment refund. 13. Offset account type is already set to Customer. In the Offset account field, specify the customer to transfer payment TO. 14. In the Method of payment field, enter or select the value which represents the Finance program. 15. Click Validate. Address any errors. 16. To verify the accounting entry is the expected outcome: a. Click Print. b. Click Journal. c. Set Totals = Yes d. Click OK. 17. Close the page when review is complete. 18. When the journal has been completed. Click Post. Reimbursement/Issue Refund Process Go to Accounts receivable > Customers > All customers. Use the Quick Filter to find records. For example, filter on the Name field or Account number. Select the record. Click the Collect tab in the ribbon. Click Reimburse customer. Click OK. On the ribbon, click Customer. Click Balance. Verify the change in Customer and Vendor balances as expected. Close the page. Accounts Payable Processing Steps Go to Accounts payable > Payments > Vendor payment journal. In the list, find and select the desired record. OR create a new payment journal. Click Lines. In the Account field, specify the Customer vendor value. (Located on the Misc fast tab of the customer account) Click Settle transactions. Select the Mark check box. Click OK. In the Description field, type a value. Process for review and payment per usual processes. Process Finance Program Refund via Journal Scenario: Payment is applied to a customer account for the Finance program from the finance company. Funds are not used and must be refunded back to the Finance company. The Finance company has a vendor account. Funds are transferred from the Customer(s) to the Finance account using a specially configured Customer payment journal. Once this journal is posted, the AP process may be executed. 1. Go to Accounts receivable > Payments > Customer payment journal. 2. Click New. 3. In the list, select the applicable journal that supports a refund to a vendor account. 4. Click Lines. 5. In the Account field, specify the customer to transfer payment FROM. 6. Tab off the account field. The customer name will populate to the field. 7. Click Settle transactions. 8. In the list, locate the payment/credit transaction to be transferred. a. Hint: May need to add personalization to settlement screen to include and display the Method of payment. 9. Select the Mark check box. 10. Click OK. 11. The debit value will default to the journal line. 12. In the Description field, type a value such a JDF payment refund. 13. Offset account type is already set to Vendor. In the Offset account field, the vendor is already set. Verify this value. 14. In the Method of payment field, enter or select the value which represents the Finance program. 15. Click Validate. Address any errors. 16. To verify the accounting entry is the expected outcome: a. Click Print. b. Click Journal. c. Set Totals = Yes d. Click OK. 17. Close the page when review is complete. 18. When the journal has been completed. Click Post. Accounts payable Processing Steps Go to Accounts payable > Payments > Vendor payment journal. In the list, find and select the desired record. OR create a new payment journal. Click Lines. In the Account field, specify the Customer vendor value. (Located on the Misc fast tab of the customer account) Click Settle transactions. Select the Mark check box. Click OK. In the Description field, type a value. Process for review and payment per usual processes.","title":"Finance Program Processes"},{"location":"FinanceProgramProcesses/#finance-program-processes","text":"This document outlines other finance program related processes.","title":"Finance Program Processes"},{"location":"FinanceProgramProcesses/#process-finance-program-payments-via-transfer","text":"Scenario: Payment is accepted to a central customer account for the Finance program. Funds are then transferred from this account to the intended Customer(s) using a specially configured Customer payment journal. 1. Go to Accounts receivable > Payments > Customer payment journal. 2. Click New. 3. In the list, select the applicable journal that supports a transfer between customer accounts. 4. Click Lines. 5. In the Account field, specify the customer to transfer payment FROM. 6. Tab off the account field. The customer name will populate to the field. 7. Click Settle transactions. 8. In the list, locate the payment/credit transaction to be transferred. a. Hint: May need to add personalization to settlement screen to include and display the Method of payment. 9. Select the Mark check box. 10. Click OK. 11. The debit value will default to the journal line. 12. In the Description field, type a value such a JDF payment transfer. 13. Offset account type is already set to Customer. In the Offset account field, specify the customer to transfer payment TO. a. Alternate: Click the arrow down key to enter multiple lines for multiple customers to transfer TO. Be sure to enter the appropriate Credit value for each added line before using arrow down to ensure the voucher number is retained. 14. In the Method of payment field, enter or select the value which represents the Finance program. 15. Click Validate. Address any errors. 16. To verify the accounting entry is the expected outcome: a. Click Print. b. Click Journal. c. Set Totals = Yes d. Click OK. 17. Close the page when review is complete. 18. When the journal has been completed. Click Post.","title":"Process Finance Program Payments via Transfer"},{"location":"FinanceProgramProcesses/#process-finance-program-refund-via-transfer","text":"Scenario: Payment is applied to a customer account for the Finance program from the finance company. Funds are not used and must be refunded back to the Finance company. The Finance company has a customer account. Funds are transferred from the Customer(s) to the Finance account using a specially configured Customer payment journal. Once this journal is posted, the Reimbursement process may be executed. 1. Go to Accounts receivable > Payments > Customer payment journal. 2. Click New. 3. In the list, select the applicable journal that supports a transfer between customer accounts. 4. Click Lines. 5. In the Account field, specify the customer to transfer payment FROM. 6. Tab off the account field. The customer name will populate to the field. 7. Click Settle transactions. 8. In the list, locate the payment/credit transaction to be transferred. a. Hint: May need to add personalization to settlement screen to include and display the Method of payment. 9. Select the Mark check box. 10. Click OK. 11. The debit value will default to the journal line. 12. In the Description field, type a value such a JDF payment refund. 13. Offset account type is already set to Customer. In the Offset account field, specify the customer to transfer payment TO. 14. In the Method of payment field, enter or select the value which represents the Finance program. 15. Click Validate. Address any errors. 16. To verify the accounting entry is the expected outcome: a. Click Print. b. Click Journal. c. Set Totals = Yes d. Click OK. 17. Close the page when review is complete. 18. When the journal has been completed. Click Post.","title":"Process Finance Program Refund via Transfer"},{"location":"FinanceProgramProcesses/#reimbursementissue-refund-process","text":"Go to Accounts receivable > Customers > All customers. Use the Quick Filter to find records. For example, filter on the Name field or Account number. Select the record. Click the Collect tab in the ribbon. Click Reimburse customer. Click OK. On the ribbon, click Customer. Click Balance. Verify the change in Customer and Vendor balances as expected. Close the page.","title":"Reimbursement/Issue Refund Process"},{"location":"FinanceProgramProcesses/#accounts-payable-processing-steps","text":"Go to Accounts payable > Payments > Vendor payment journal. In the list, find and select the desired record. OR create a new payment journal. Click Lines. In the Account field, specify the Customer vendor value. (Located on the Misc fast tab of the customer account) Click Settle transactions. Select the Mark check box. Click OK. In the Description field, type a value. Process for review and payment per usual processes.","title":"Accounts Payable Processing Steps"},{"location":"FinanceProgramProcesses/#process-finance-program-refund-via-journal","text":"Scenario: Payment is applied to a customer account for the Finance program from the finance company. Funds are not used and must be refunded back to the Finance company. The Finance company has a vendor account. Funds are transferred from the Customer(s) to the Finance account using a specially configured Customer payment journal. Once this journal is posted, the AP process may be executed. 1. Go to Accounts receivable > Payments > Customer payment journal. 2. Click New. 3. In the list, select the applicable journal that supports a refund to a vendor account. 4. Click Lines. 5. In the Account field, specify the customer to transfer payment FROM. 6. Tab off the account field. The customer name will populate to the field. 7. Click Settle transactions. 8. In the list, locate the payment/credit transaction to be transferred. a. Hint: May need to add personalization to settlement screen to include and display the Method of payment. 9. Select the Mark check box. 10. Click OK. 11. The debit value will default to the journal line. 12. In the Description field, type a value such a JDF payment refund. 13. Offset account type is already set to Vendor. In the Offset account field, the vendor is already set. Verify this value. 14. In the Method of payment field, enter or select the value which represents the Finance program. 15. Click Validate. Address any errors. 16. To verify the accounting entry is the expected outcome: a. Click Print. b. Click Journal. c. Set Totals = Yes d. Click OK. 17. Close the page when review is complete. 18. When the journal has been completed. Click Post.","title":"Process Finance Program Refund via Journal"},{"location":"FinanceProgramProcesses/#accounts-payable-processing-steps_1","text":"Go to Accounts payable > Payments > Vendor payment journal. In the list, find and select the desired record. OR create a new payment journal. Click Lines. In the Account field, specify the Customer vendor value. (Located on the Misc fast tab of the customer account) Click Settle transactions. Select the Mark check box. Click OK. In the Description field, type a value. Process for review and payment per usual processes.","title":"Accounts payable Processing Steps"},{"location":"HowtoupdateLevridgeScale/","text":"How to Update Levridge Scale Prerequisites An older Version of Levridge Scale must be installed and configured Setup Download the new version installer exe file Right click and Run as Administrator Agree to terms and click Install When the install is complete, click Finish Open IIS On the LevScaleAPI and LevPrint Application Pools, right click > Advanced settings In the Identity field select Custom account > Set and enter username and password Open Services On the LevHardwareInterfaceService and LevScaleClient services , right click > Properties Click the Log On tab Enter account and password and click OK. If the account is part of a directory, click Browse. Then enter the username and click Check Names. Check the configuration of appsettings Open file explorer and go to C:\\Program Files (x86)\\Levridge\\LevridgeScaleHouse\\Servers In the LevScaleAPI and LevScaleClient folders, configure appsettings.json Open file explorer and go to C:\\Program Files (x86)\\Levridge\\LevridgeScaleHouse\\Services In the AxToScaleIntegration and ScaleToAxIntegration folders configure appsettings.json Restart the PC","title":"How to Update Levridge Scale"},{"location":"HowtoupdateLevridgeScale/#how-to-update-levridge-scale","text":"","title":"How to Update Levridge Scale"},{"location":"HowtoupdateLevridgeScale/#prerequisites","text":"An older Version of Levridge Scale must be installed and configured","title":"Prerequisites"},{"location":"HowtoupdateLevridgeScale/#setup","text":"Download the new version installer exe file Right click and Run as Administrator Agree to terms and click Install When the install is complete, click Finish Open IIS On the LevScaleAPI and LevPrint Application Pools, right click > Advanced settings In the Identity field select Custom account > Set and enter username and password Open Services On the LevHardwareInterfaceService and LevScaleClient services , right click > Properties Click the Log On tab Enter account and password and click OK. If the account is part of a directory, click Browse. Then enter the username and click Check Names. Check the configuration of appsettings Open file explorer and go to C:\\Program Files (x86)\\Levridge\\LevridgeScaleHouse\\Servers In the LevScaleAPI and LevScaleClient folders, configure appsettings.json Open file explorer and go to C:\\Program Files (x86)\\Levridge\\LevridgeScaleHouse\\Services In the AxToScaleIntegration and ScaleToAxIntegration folders configure appsettings.json Restart the PC","title":"Setup"},{"location":"InstanceConfig/","text":"InstanceConfig Settings InstanceConfig is an object in the appsettings.json file used by the Levridge Integration Framework to define the configuration for the Current Instance of the integration framework. Example \"InstanceConfig\": { \"AzureTableConfiguration\": \"AzureTableConfiguration\", \"LogRequestsAndResponses\": true, \"EnableAppInsightsAdaptiveSampling\": true, \"HttpClientTimeout\": 100, \"AcceptedApiRoles\":[ \"access_as_application\" ] } Definition InstanceConfig The node in the appsettings.json file does not actually need to be named \"InstanceConfig\". You can use a command line parameter to specify the node name (section name) that contains the InstanceConfig data. No matter the name, the instance config section must contain the following attributes: AzureTableConfiguration LogRequestsAndResponses EnableAppInsightsAdaptiveSampling HttpClientTimeout AcceptedApiRoles AzureAdSection AzureTableConfiguration Required The AzureTableConfiguration attribute contains a string that specifies the configuration node (section) that holds the Azure Table configuration used by the current instance of the Integration Framework. This must point to a node that is a AzureTableEntityConfiguration json object LogRequestsAndResponses Optional Default = false The LogRequestsAndResponses attribute contains a boolean that specifies whether or not to log the requests and responses of the controllers in the current instance of the Integration Framework. EnableAppInsightsAdaptiveSampling Optional Default = true The EnableAppInsightsAdaptiveSampling attribute contains a boolean that specifies whether or not to enable application insights adaptive sampling. If this is disabled all the messages logged to Application Insights. This should only be used during troubleshooting and testing because the cost for Application Insights is billed based on volume. The default state is \"true\" which means that adaptive sampling is enabled by default. See Sampling in Application Insights for more information. HttpClientTimeout Optional Default = 100 seconds The HttpClientTimeout attribute specifies the number of seconds to use for the timeout on all HTTP Client calls in the integration framework. The default is 100 seconds. In some cases this is not enough time under heavy load, particularly the Agsync to FinOps work order integration. To change this set the value to an integer value that represents the number of seconds you want for the timeout. AcceptedApiRoles Optional Default = Empty Array The AcceptedApiRoles attribute specifies what roles in the Active Directory Application Registration are accepted as adequate permissions to authorize an application to interact with secured APIs on this integration instance. AzureAdSection Optional Default = \"AzureAD\" The name of the section that contains the AzureAD configuration options used for AzureAD authentication.","title":"InstanceConfig Settings"},{"location":"InstanceConfig/#instanceconfig-settings","text":"InstanceConfig is an object in the appsettings.json file used by the Levridge Integration Framework to define the configuration for the Current Instance of the integration framework.","title":"InstanceConfig Settings"},{"location":"InstanceConfig/#example","text":"\"InstanceConfig\": { \"AzureTableConfiguration\": \"AzureTableConfiguration\", \"LogRequestsAndResponses\": true, \"EnableAppInsightsAdaptiveSampling\": true, \"HttpClientTimeout\": 100, \"AcceptedApiRoles\":[ \"access_as_application\" ] }","title":"Example"},{"location":"InstanceConfig/#definition","text":"","title":"Definition"},{"location":"InstanceConfig/#instanceconfig","text":"The node in the appsettings.json file does not actually need to be named \"InstanceConfig\". You can use a command line parameter to specify the node name (section name) that contains the InstanceConfig data. No matter the name, the instance config section must contain the following attributes: AzureTableConfiguration LogRequestsAndResponses EnableAppInsightsAdaptiveSampling HttpClientTimeout AcceptedApiRoles AzureAdSection","title":"InstanceConfig"},{"location":"InstanceConfig/#azuretableconfiguration","text":"Required The AzureTableConfiguration attribute contains a string that specifies the configuration node (section) that holds the Azure Table configuration used by the current instance of the Integration Framework. This must point to a node that is a AzureTableEntityConfiguration json object","title":"AzureTableConfiguration"},{"location":"InstanceConfig/#logrequestsandresponses","text":"Optional Default = false The LogRequestsAndResponses attribute contains a boolean that specifies whether or not to log the requests and responses of the controllers in the current instance of the Integration Framework.","title":"LogRequestsAndResponses"},{"location":"InstanceConfig/#enableappinsightsadaptivesampling","text":"Optional Default = true The EnableAppInsightsAdaptiveSampling attribute contains a boolean that specifies whether or not to enable application insights adaptive sampling. If this is disabled all the messages logged to Application Insights. This should only be used during troubleshooting and testing because the cost for Application Insights is billed based on volume. The default state is \"true\" which means that adaptive sampling is enabled by default. See Sampling in Application Insights for more information.","title":"EnableAppInsightsAdaptiveSampling"},{"location":"InstanceConfig/#httpclienttimeout","text":"Optional Default = 100 seconds The HttpClientTimeout attribute specifies the number of seconds to use for the timeout on all HTTP Client calls in the integration framework. The default is 100 seconds. In some cases this is not enough time under heavy load, particularly the Agsync to FinOps work order integration. To change this set the value to an integer value that represents the number of seconds you want for the timeout.","title":"HttpClientTimeout"},{"location":"InstanceConfig/#acceptedapiroles","text":"Optional Default = Empty Array The AcceptedApiRoles attribute specifies what roles in the Active Directory Application Registration are accepted as adequate permissions to authorize an application to interact with secured APIs on this integration instance.","title":"AcceptedApiRoles"},{"location":"InstanceConfig/#azureadsection","text":"Optional Default = \"AzureAD\" The name of the section that contains the AzureAD configuration options used for AzureAD authentication.","title":"AzureAdSection"},{"location":"IntegratingCustomFields/","text":"Introduction In many implementations a client will have specific needs that require custom fields to be added to existing entities or custom behavior for an existing field in an integration. This document shows how to create a custom field extension to have those custom fields integrated when the entity is being integrated. Overview In order to integrate custom fields you will need to create a Visual Studio solution with upto three projects: Custom Source Proxy Project Contains the proxy for the source datasource with the custom fields in the entities This project is only necessary if there are custom fields in the source. If your customizations only include custom behavior for existing fields then a custom source proxy will not be needed. Custom Destination Proxy Project Contains the proxy for the desitination datasource with the custom fields in the entities This project is only necessary if there are custom fields in the destination. If your customizations only include custom behavior for existing fields then a custom destination proxy will not be needed. Custom Mapping Project Contains the code need to map the integration for the custom fields between the source and destination entities Note: If there are no custom fields involved and you are only defining custom behavior for existing fields you will not need new proxies. You only need a proxy to define the metadata for custom fields. Please see Deploying Custom Field Mapping Assemblies for information on deploying the custom mapping assembly. Tutorial Create the Solution The first step is to create a Visual Studio solution for the custom mapping. In Visual Studio, create a new C# Class Library (.NET Core). You may want to give the solution a different name than the project. In our example, the customer is Comstock so we will name the solution \"ComstockCustomMapping\" and the project Levridge.Integration.IntegrationService.Mapping.Comstock. We recommend you use this naming convention for your custom mapping project: Levridge.Integration.IntegrationService.Mapping.[clientname]. Create the Source Proxy Project Create the Destination Proxy Project Add References to the Custom Mapping Project The Levridge packages are published to our DevOps Artifacts so you will need to add a Nuget source for those packages. This is done in Visual Studio from Tools/Options/NuGet Package Manager/Package Sources. The source is https://pkgs.dev.azure.com/stoneridgesoftware/Levridge/_packaging/LevridgePackages-beta/nuget/v3/index.json If you are working the LevDevelopment branch. If you are working in the customer's repository you should use 'https://pkgs.dev.azure.com/stoneridgesoftware/Levridge/_packaging/LevridgePackages/nuget/v3/index.json' which is the released version of the packages. You will need to add the following Nuget references to your custom mapping project: Levridge.EntityFramework Levridge.Integration.IntegrationService.Abstractions Levridge.Integration.IntegrationService.Mapping Microsoft.Extensions.DependencyInjection System.ComponentModel.Composition Note: The Levridge packages are available from our Azure DevOps Artifacts repository. The URL for released packages is https://pkgs.dev.azure.com/stoneridgesoftware/Levridge/_packaging/LevridgePackages/nuget/v3/index.json Add Source and Destination Datasource References to Custom Mapping Project You will need to reference the destination data source project in order to have access to any Field types and other types used during mapping. If your datasource is CRM you would include this reference: * Levridge.ODataDataSources.CRMODataDataSource If your datasource is F&O you would include this reference: * Levridge.DynamicsAxDataSource Update the references to be excluded from deployment These references will be needed for the build process, but we will want to use the libraries provided by the standard deployment and not deploy our own copy of the libraries with the custom mapping assemblies. To accomplish this, we let the msbuild system know to exclude these libraries from the runtime deployment by adding <ExcludeAssets>runtime</ExcludeAssets> to the <PackageReference> node for each package. You should have an <ItemGroup> node that looks somthing like this. <ItemGroup> <PackageReference Include=\"Levridge.EntityFramework\" Version=\"2.0.10\"> <ExcludeAssets>runtime</ExcludeAssets> </PackageReference> <PackageReference Include=\"Levridge.Integration.IntegrationService.Abstractions\" Version=\"1.0.0\"> <ExcludeAssets>runtime</ExcludeAssets> </PackageReference> <PackageReference Include=\"Levridge.Integration.IntegrationService.Mapping\" Version=\"1.0.0\"> <ExcludeAssets>runtime</ExcludeAssets> </PackageReference> <PackageReference Include=\"Levridge.ODataDataSources.CRMODataDataSource\" Version=\"2.1.15\"> <ExcludeAssets>runtime</ExcludeAssets> </PackageReference> <PackageReference Include=\"Microsoft.Extensions.DependencyInjection\" Version=\"3.0.2\"> <ExcludeAssets>runtime</ExcludeAssets> </PackageReference> </ItemGroup> Add a reference to the two datasource proxy projects Your custom mapping methods will need to access the source and data entities so you will need to add a reference to the two proxy projects you created in sections Create the Source Proxy Project and Create the Destination Proxy Project If you do not need a custom proxy project you should add a reference package to the correct Levridge nuget package. The F&O and CRM packages are: * Levridge.DynamicsAxDataSource * Levridge.ODataDataSources.DynamicsCRM Create An EntityMapBuilderExtension class We need to have a class that will contain the necessary mapping methods for the custom fields being added to the integration. Create a public class This can not be a static class because Microsoft MEF won't export a static class. Add the [Export] attribute to the class The Export attribute is in the System.ComponentModel.Composition that was added in the section Add References to the Custom Mapping Project Name the class We recommend using something like [client]CustomFieldsEntityMapping. The actual name is not important, orther than to be clear the purpose of the class. In our example we named the class ContosoCustomFieldsEntityMapping . Add a public static void method that takes a single IServiceCollection parameter. In our example we added the following method: public static void AddContosoCustomMaps(IServiceCollection services) Add a private static method that takes a single IEntityMapBuilder parameter and returns an IEntityMapBuilder and takes two generic parameters. In our example we added the following method: private static IEntityMapBuilder AddContosoCustomFieldsToMaps<TSource, TTarget>(IEntityMapBuilder builder) Add the following code to the public static method created in step 3 above: var builder = new EntityMapBuilder(services); ContosoCustomFieldsEntityMapping.AddContosoCustomFieldsToMaps (builder);` The second line should reference the current class and the method you created in step 4 above. Here is our complete method in the example: public static void AddLandusCustomMaps(IServiceCollection services) { var builder = new EntityMapBuilder(services); ContosoCustomFieldsEntityMapping.AddContosoCustomFieldsToMaps (builder); } Note: The namespaces AxEntities and CRMEntities should reference your custom proxies if you created them otherwise it should reference the packaged Levridge provided proxy Add the following code to the private static method created in step 4 above: if (builder == null) { throw new ArgumentNullException(nameof(builder)); } return EntityMapBuilderHelper.InvokeMappingMethods(builder, typeof(ContosoCustomFieldsEntityMapping)); The first block validates the parameter. The last line calls a method on a helper class to invoke all the mapping methods on the class we are now building. Here is our complete method in the example: private static IEntityMapBuilder AddContosoCustomFieldsToMaps (IEntityMapBuilder builder) { if (builder == null) { throw new ArgumentNullException(nameof(builder)); } return EntityMapBuilderHelper.InvokeMappingMethods(builder, typeof(ContosoCustomFieldsEntityMapping)); } Create mapping methods Create a private static method that a single IEntityMapBuilder parameter and returns an IEntityMapBuilder Name the method Map[SourceEntity]_[DestinationEntity]. In our example we added the following method: private static IEntityMapBuilder MapCustomerV3_lev_customer(IEntityMapBuilder builder) This method will add the custom field mapping for CustomerV3 to lev_customer Add an [EntityMapMethod] Attribute to the map method Add mapping code in the map method The mapping code is split into two levels: - AddEntityMap The structure of the AddEntityMap is an extension method on IEntityMapBuilder that takes two generic parameters for the source and target entities and an Action method parameter used to configure the EntityMap by adding field maps for the entities. - AddFieldMap AddFieldMap is a method on EntityMap . There are several that takes two generic parameters for the source field type and target field type and an Action method parameter used to configure the EntityMap by adding field maps for the entities. Here is an example method that adds mapping for CustomerV3 to lev_customer: private static IEntityMapBuilder MapCustomerV3_lev_customer(IEntityMapBuilder builder) { builder.AddEntityMap<AxEntities.CustomerV3, CRMEntities.lev_customer>(em => { em.AddFieldMap<String, String>( new JSONField<String>(nameof(AxEntities.CustomerV3.LevPrintName)), new CRMODataField<String>(nameof(CRMEntities.lev_customer.lev_name)), (a, b) => { String printName = a.Value; String organizationName = (String)a.FieldEntity[nameof(AxEntities.CustomerV3.OrganizationName)].Value; return String.IsNullOrEmpty(printName) ? organizationName : printName; }, (b, a) => { throw new IntegrationNotSupportedException(); }); em.AddFieldMap<String, String>( new JSONField<String>(nameof(AxEntities.CustomerV3.LevInCareOf)), new CRMODataField<String>(nameof(CRMEntities.lev_customer.lev_incareof))); em.AddMapReferencedField(nameof(AxEntities.CustomerV3), nameof(AxEntities.CustomerV3.LevPrintName)); }); return builder; } Example Mapping using System; using Microsoft.Extensions.DependencyInjection; using Levridge.EntityFramework; using Levridge.ODataDataSources; using Levridge.Integration.IntegrationService.Abstractions; using AxEntities = ContosoCustomFields.Microsoft.Dynamics.DataEntities; using CRMEntities = ContosoCustomFields.ODataDataSources.DynamicsCRM; using System.ComponentModel.Composition; namespace Levridge.Integration.IntegrationService.Mapping.ContosoCustomFields { [Export] public class ContosoCustomFieldsEntityMapBuilderExtensions { public static void AddContosoCustomMaps(IServiceCollection services) { var builder = new EntityMapBuilder(services); ContosoCustomFieldsEntityMapBuilderExtensions.AddContosoCustomFieldsToMaps<AxEntities.CustomerV3, CRMEntities.lev_customer>(builder); } private static IEntityMapBuilder AddContosoCustomFieldsToMaps<TSource, TTarget>(IEntityMapBuilder builder) { if (builder == null) { throw new ArgumentNullException(nameof(builder)); } return EntityMapBuilderHelper.InvokeMappingMethods(builder, typeof(ContosoCustomFieldsEntityMapBuilderExtensions)); } [EntityMapMethod] private static IEntityMapBuilder MapCustomerV3_lev_customer(IEntityMapBuilder builder) { builder.AddEntityMap<AxEntities.CustomerV3, CRMEntities.lev_customer>(em => { em.AddFieldMap<String, String>( new JSONField<String>(nameof(AxEntities.CustomerV3.LevPrintName)), new CRMODataField<String>(nameof(CRMEntities.lev_customer.lev_name)), (a, b) => { String printName = a.Value; String organizationName = (String)a.FieldEntity[nameof(AxEntities.CustomerV3.OrganizationName)].Value; return String.IsNullOrEmpty(printName) ? organizationName : printName; }, (b, a) => { throw new IntegrationNotSupportedException(); }); em.AddFieldMap<String, String>( new JSONField<String>(nameof(AxEntities.CustomerV3.LevInCareOf)), new CRMODataField<String>(nameof(CRMEntities.lev_customer.lev_incareof))); em.AddMapReferencedField(nameof(AxEntities.CustomerV3), nameof(AxEntities.CustomerV3.LevPrintName)); }); return builder; } [EntityMapMethod] private static IEntityMapBuilder MapCustomerV3_account(IEntityMapBuilder builder) { builder.AddEntityMap<AxEntities.CustomerV3, CRMEntities.account>(em => { em.AddFieldMap<String, String>( new JSONField<String>(nameof(AxEntities.CustomerV3.LevPrintName)), new CRMODataField<String>(nameof(CRMEntities.account.name)), (a, b) => { String printName = a.Value; String organizationName = (String)a.FieldEntity[nameof(AxEntities.CustomerV3.OrganizationName)].Value; return String.IsNullOrEmpty(printName) ? organizationName : printName; }, (b, a) => { throw new IntegrationNotSupportedException(); }); }); return builder; } [EntityMapMethod] private static IEntityMapBuilder MapInventSite_lev_companysite(IEntityMapBuilder builder) { builder.AddEntityMap<AxEntities.InventSite, CRMEntities.lev_companysite>(em => { em.AddFieldMap<String, String>( new JSONField<String>(nameof(AxEntities.InventSite.LevRegionId)), new CRMODataField<String>(nameof(CRMEntities.lev_companysite.stn_region))); }); return builder; } } } Deploying the Custom Mapping Assemblies To deploy the custom assembly you simply copy the output of the mapping assembly to the same directory that contains the Levridge.Integration.Host assemblies. See Deploying Custom Field Mapping Assemblies for more information.","title":"Integrating Custom Fields"},{"location":"IntegratingCustomFields/#introduction","text":"In many implementations a client will have specific needs that require custom fields to be added to existing entities or custom behavior for an existing field in an integration. This document shows how to create a custom field extension to have those custom fields integrated when the entity is being integrated.","title":"Introduction"},{"location":"IntegratingCustomFields/#overview","text":"In order to integrate custom fields you will need to create a Visual Studio solution with upto three projects: Custom Source Proxy Project Contains the proxy for the source datasource with the custom fields in the entities This project is only necessary if there are custom fields in the source. If your customizations only include custom behavior for existing fields then a custom source proxy will not be needed. Custom Destination Proxy Project Contains the proxy for the desitination datasource with the custom fields in the entities This project is only necessary if there are custom fields in the destination. If your customizations only include custom behavior for existing fields then a custom destination proxy will not be needed. Custom Mapping Project Contains the code need to map the integration for the custom fields between the source and destination entities Note: If there are no custom fields involved and you are only defining custom behavior for existing fields you will not need new proxies. You only need a proxy to define the metadata for custom fields. Please see Deploying Custom Field Mapping Assemblies for information on deploying the custom mapping assembly.","title":"Overview"},{"location":"IntegratingCustomFields/#tutorial","text":"","title":"Tutorial"},{"location":"IntegratingCustomFields/#create-the-solution","text":"The first step is to create a Visual Studio solution for the custom mapping. In Visual Studio, create a new C# Class Library (.NET Core). You may want to give the solution a different name than the project. In our example, the customer is Comstock so we will name the solution \"ComstockCustomMapping\" and the project Levridge.Integration.IntegrationService.Mapping.Comstock. We recommend you use this naming convention for your custom mapping project: Levridge.Integration.IntegrationService.Mapping.[clientname].","title":"Create the Solution"},{"location":"IntegratingCustomFields/#create-the-source-proxy-project","text":"","title":"Create the Source Proxy Project"},{"location":"IntegratingCustomFields/#create-the-destination-proxy-project","text":"","title":"Create the Destination Proxy Project"},{"location":"IntegratingCustomFields/#add-references-to-the-custom-mapping-project","text":"The Levridge packages are published to our DevOps Artifacts so you will need to add a Nuget source for those packages. This is done in Visual Studio from Tools/Options/NuGet Package Manager/Package Sources. The source is https://pkgs.dev.azure.com/stoneridgesoftware/Levridge/_packaging/LevridgePackages-beta/nuget/v3/index.json If you are working the LevDevelopment branch. If you are working in the customer's repository you should use 'https://pkgs.dev.azure.com/stoneridgesoftware/Levridge/_packaging/LevridgePackages/nuget/v3/index.json' which is the released version of the packages. You will need to add the following Nuget references to your custom mapping project: Levridge.EntityFramework Levridge.Integration.IntegrationService.Abstractions Levridge.Integration.IntegrationService.Mapping Microsoft.Extensions.DependencyInjection System.ComponentModel.Composition Note: The Levridge packages are available from our Azure DevOps Artifacts repository. The URL for released packages is https://pkgs.dev.azure.com/stoneridgesoftware/Levridge/_packaging/LevridgePackages/nuget/v3/index.json","title":"Add References to the Custom Mapping Project"},{"location":"IntegratingCustomFields/#add-source-and-destination-datasource-references-to-custom-mapping-project","text":"You will need to reference the destination data source project in order to have access to any Field types and other types used during mapping. If your datasource is CRM you would include this reference: * Levridge.ODataDataSources.CRMODataDataSource If your datasource is F&O you would include this reference: * Levridge.DynamicsAxDataSource","title":"Add Source and Destination Datasource References to Custom Mapping Project"},{"location":"IntegratingCustomFields/#update-the-references-to-be-excluded-from-deployment","text":"These references will be needed for the build process, but we will want to use the libraries provided by the standard deployment and not deploy our own copy of the libraries with the custom mapping assemblies. To accomplish this, we let the msbuild system know to exclude these libraries from the runtime deployment by adding <ExcludeAssets>runtime</ExcludeAssets> to the <PackageReference> node for each package. You should have an <ItemGroup> node that looks somthing like this. <ItemGroup> <PackageReference Include=\"Levridge.EntityFramework\" Version=\"2.0.10\"> <ExcludeAssets>runtime</ExcludeAssets> </PackageReference> <PackageReference Include=\"Levridge.Integration.IntegrationService.Abstractions\" Version=\"1.0.0\"> <ExcludeAssets>runtime</ExcludeAssets> </PackageReference> <PackageReference Include=\"Levridge.Integration.IntegrationService.Mapping\" Version=\"1.0.0\"> <ExcludeAssets>runtime</ExcludeAssets> </PackageReference> <PackageReference Include=\"Levridge.ODataDataSources.CRMODataDataSource\" Version=\"2.1.15\"> <ExcludeAssets>runtime</ExcludeAssets> </PackageReference> <PackageReference Include=\"Microsoft.Extensions.DependencyInjection\" Version=\"3.0.2\"> <ExcludeAssets>runtime</ExcludeAssets> </PackageReference> </ItemGroup>","title":"Update the references to be excluded from deployment"},{"location":"IntegratingCustomFields/#add-a-reference-to-the-two-datasource-proxy-projects","text":"Your custom mapping methods will need to access the source and data entities so you will need to add a reference to the two proxy projects you created in sections Create the Source Proxy Project and Create the Destination Proxy Project If you do not need a custom proxy project you should add a reference package to the correct Levridge nuget package. The F&O and CRM packages are: * Levridge.DynamicsAxDataSource * Levridge.ODataDataSources.DynamicsCRM","title":"Add a reference to the two datasource proxy projects"},{"location":"IntegratingCustomFields/#create-an-entitymapbuilderextension-class","text":"We need to have a class that will contain the necessary mapping methods for the custom fields being added to the integration. Create a public class This can not be a static class because Microsoft MEF won't export a static class. Add the [Export] attribute to the class The Export attribute is in the System.ComponentModel.Composition that was added in the section Add References to the Custom Mapping Project Name the class We recommend using something like [client]CustomFieldsEntityMapping. The actual name is not important, orther than to be clear the purpose of the class. In our example we named the class ContosoCustomFieldsEntityMapping . Add a public static void method that takes a single IServiceCollection parameter. In our example we added the following method: public static void AddContosoCustomMaps(IServiceCollection services) Add a private static method that takes a single IEntityMapBuilder parameter and returns an IEntityMapBuilder and takes two generic parameters. In our example we added the following method: private static IEntityMapBuilder AddContosoCustomFieldsToMaps<TSource, TTarget>(IEntityMapBuilder builder) Add the following code to the public static method created in step 3 above: var builder = new EntityMapBuilder(services); ContosoCustomFieldsEntityMapping.AddContosoCustomFieldsToMaps (builder);` The second line should reference the current class and the method you created in step 4 above. Here is our complete method in the example: public static void AddLandusCustomMaps(IServiceCollection services) { var builder = new EntityMapBuilder(services); ContosoCustomFieldsEntityMapping.AddContosoCustomFieldsToMaps (builder); } Note: The namespaces AxEntities and CRMEntities should reference your custom proxies if you created them otherwise it should reference the packaged Levridge provided proxy Add the following code to the private static method created in step 4 above: if (builder == null) { throw new ArgumentNullException(nameof(builder)); } return EntityMapBuilderHelper.InvokeMappingMethods(builder, typeof(ContosoCustomFieldsEntityMapping)); The first block validates the parameter. The last line calls a method on a helper class to invoke all the mapping methods on the class we are now building. Here is our complete method in the example: private static IEntityMapBuilder AddContosoCustomFieldsToMaps (IEntityMapBuilder builder) { if (builder == null) { throw new ArgumentNullException(nameof(builder)); } return EntityMapBuilderHelper.InvokeMappingMethods(builder, typeof(ContosoCustomFieldsEntityMapping)); } Create mapping methods Create a private static method that a single IEntityMapBuilder parameter and returns an IEntityMapBuilder Name the method Map[SourceEntity]_[DestinationEntity]. In our example we added the following method: private static IEntityMapBuilder MapCustomerV3_lev_customer(IEntityMapBuilder builder) This method will add the custom field mapping for CustomerV3 to lev_customer Add an [EntityMapMethod] Attribute to the map method Add mapping code in the map method The mapping code is split into two levels: - AddEntityMap The structure of the AddEntityMap is an extension method on IEntityMapBuilder that takes two generic parameters for the source and target entities and an Action method parameter used to configure the EntityMap by adding field maps for the entities. - AddFieldMap AddFieldMap is a method on EntityMap . There are several that takes two generic parameters for the source field type and target field type and an Action method parameter used to configure the EntityMap by adding field maps for the entities. Here is an example method that adds mapping for CustomerV3 to lev_customer: private static IEntityMapBuilder MapCustomerV3_lev_customer(IEntityMapBuilder builder) { builder.AddEntityMap<AxEntities.CustomerV3, CRMEntities.lev_customer>(em => { em.AddFieldMap<String, String>( new JSONField<String>(nameof(AxEntities.CustomerV3.LevPrintName)), new CRMODataField<String>(nameof(CRMEntities.lev_customer.lev_name)), (a, b) => { String printName = a.Value; String organizationName = (String)a.FieldEntity[nameof(AxEntities.CustomerV3.OrganizationName)].Value; return String.IsNullOrEmpty(printName) ? organizationName : printName; }, (b, a) => { throw new IntegrationNotSupportedException(); }); em.AddFieldMap<String, String>( new JSONField<String>(nameof(AxEntities.CustomerV3.LevInCareOf)), new CRMODataField<String>(nameof(CRMEntities.lev_customer.lev_incareof))); em.AddMapReferencedField(nameof(AxEntities.CustomerV3), nameof(AxEntities.CustomerV3.LevPrintName)); }); return builder; }","title":"Create An EntityMapBuilderExtension class"},{"location":"IntegratingCustomFields/#example-mapping","text":"using System; using Microsoft.Extensions.DependencyInjection; using Levridge.EntityFramework; using Levridge.ODataDataSources; using Levridge.Integration.IntegrationService.Abstractions; using AxEntities = ContosoCustomFields.Microsoft.Dynamics.DataEntities; using CRMEntities = ContosoCustomFields.ODataDataSources.DynamicsCRM; using System.ComponentModel.Composition; namespace Levridge.Integration.IntegrationService.Mapping.ContosoCustomFields { [Export] public class ContosoCustomFieldsEntityMapBuilderExtensions { public static void AddContosoCustomMaps(IServiceCollection services) { var builder = new EntityMapBuilder(services); ContosoCustomFieldsEntityMapBuilderExtensions.AddContosoCustomFieldsToMaps<AxEntities.CustomerV3, CRMEntities.lev_customer>(builder); } private static IEntityMapBuilder AddContosoCustomFieldsToMaps<TSource, TTarget>(IEntityMapBuilder builder) { if (builder == null) { throw new ArgumentNullException(nameof(builder)); } return EntityMapBuilderHelper.InvokeMappingMethods(builder, typeof(ContosoCustomFieldsEntityMapBuilderExtensions)); } [EntityMapMethod] private static IEntityMapBuilder MapCustomerV3_lev_customer(IEntityMapBuilder builder) { builder.AddEntityMap<AxEntities.CustomerV3, CRMEntities.lev_customer>(em => { em.AddFieldMap<String, String>( new JSONField<String>(nameof(AxEntities.CustomerV3.LevPrintName)), new CRMODataField<String>(nameof(CRMEntities.lev_customer.lev_name)), (a, b) => { String printName = a.Value; String organizationName = (String)a.FieldEntity[nameof(AxEntities.CustomerV3.OrganizationName)].Value; return String.IsNullOrEmpty(printName) ? organizationName : printName; }, (b, a) => { throw new IntegrationNotSupportedException(); }); em.AddFieldMap<String, String>( new JSONField<String>(nameof(AxEntities.CustomerV3.LevInCareOf)), new CRMODataField<String>(nameof(CRMEntities.lev_customer.lev_incareof))); em.AddMapReferencedField(nameof(AxEntities.CustomerV3), nameof(AxEntities.CustomerV3.LevPrintName)); }); return builder; } [EntityMapMethod] private static IEntityMapBuilder MapCustomerV3_account(IEntityMapBuilder builder) { builder.AddEntityMap<AxEntities.CustomerV3, CRMEntities.account>(em => { em.AddFieldMap<String, String>( new JSONField<String>(nameof(AxEntities.CustomerV3.LevPrintName)), new CRMODataField<String>(nameof(CRMEntities.account.name)), (a, b) => { String printName = a.Value; String organizationName = (String)a.FieldEntity[nameof(AxEntities.CustomerV3.OrganizationName)].Value; return String.IsNullOrEmpty(printName) ? organizationName : printName; }, (b, a) => { throw new IntegrationNotSupportedException(); }); }); return builder; } [EntityMapMethod] private static IEntityMapBuilder MapInventSite_lev_companysite(IEntityMapBuilder builder) { builder.AddEntityMap<AxEntities.InventSite, CRMEntities.lev_companysite>(em => { em.AddFieldMap<String, String>( new JSONField<String>(nameof(AxEntities.InventSite.LevRegionId)), new CRMODataField<String>(nameof(CRMEntities.lev_companysite.stn_region))); }); return builder; } } }","title":"Example Mapping"},{"location":"IntegratingCustomFields/#deploying-the-custom-mapping-assemblies","text":"To deploy the custom assembly you simply copy the output of the mapping assembly to the same directory that contains the Levridge.Integration.Host assemblies. See Deploying Custom Field Mapping Assemblies for more information.","title":"Deploying the Custom Mapping Assemblies"},{"location":"Integration%20Template/","text":"Title of Integration This should be a short, one paragraph description of the integration being documented. Overview A more detailed overview should included here. This overview should describe the integration and any unique details for this integration. API This section should include detailed documentation of the API. Configuration This section should include detailed documentation for configuring the integration. (Optional) Deployment This section should include any instructions necessary to deploy this integration. This section may not be needed if the deployment follows a standard deployment process that is documented elsewhere. In this case it would be helpful to direct the reader to that standard documentation.","title":"Title of Integration"},{"location":"Integration%20Template/#title-of-integration","text":"This should be a short, one paragraph description of the integration being documented.","title":"Title of Integration"},{"location":"Integration%20Template/#overview","text":"A more detailed overview should included here. This overview should describe the integration and any unique details for this integration.","title":"Overview"},{"location":"Integration%20Template/#api","text":"This section should include detailed documentation of the API.","title":"API"},{"location":"Integration%20Template/#configuration","text":"This section should include detailed documentation for configuring the integration.","title":"Configuration"},{"location":"Integration%20Template/#optional-deployment","text":"This section should include any instructions necessary to deploy this integration. This section may not be needed if the deployment follows a standard deployment process that is documented elsewhere. In this case it would be helpful to direct the reader to that standard documentation.","title":"(Optional) Deployment"},{"location":"Integration-Overview/","text":"Introduction The Levridge integration framework provides integration between Dynamics 365 Finance and Dynamics 365 Customer Engagement and 3rd party applications. This document provides and overview of the integration framework and links to the documents that exist for the framework. Overview Levridge has created an integration framework to handle all the integrations between systems. This framework uses json entities to exchange information. The integration framework uses Microsoft Azure Service Bus to provide a Publish and Subscribe messaging pattern . Levridge has created an event framework in D365 Finance that will publish entity data to the service bus based on Creates, Deletes and Updates. Information/messages sent to the service bus can be subscribed to by 3rd party applications or by the Levridge integration framework and sent to 3rd party applications like AgSync. The data entities in D365 Finance and can be filtered so only certain records are sent. All integrations that use the framework follow the same pattern: 1. A data source has an integration event 2. The data source responds to the integration event by sending one or more entities to the service bus. 3. The service bus publishes the message(s) to each subscription 4. An instance of the integration framework receives the message(s) from a subscription 5. The integration framework transforms the message if needed 6. The integration framework sends the message to the target data source The Levridge Integration Framework is most commonly run in the cloud as an Azure App Serivce . It can also run as a windows service or as an IIS application. See this article to learn more about the deployment options. Integrations Currently we support the following integrations: D365 F&O to D365 CE D365 CE to D365 F&O Scale Agsync Kahler Surety oneWeigh Field Reveal Levridge CRM Remote Printing Service Environment Planning A standard D365 implementation is used when launching a Levridge environment plan. The D365 F&O System Requirements and what one needs to start a project are outlined in Environment Planning . Levridge Integration System Requirements Azure App Registrations One each for the Production and Test environments Azure App Registrations Azure Service Bus Namespace One each for the Production and Test environments Configure both using the Standard Pricing Tier Topics/Subscriptions will be determined based on integration within scope for the project Documentation Azure Service Bus Overview Azure Service Bus topics and subscriptions Azure App Service Plan One each for Production and Test environments Use a Standard tier for the Test environment Use a Premium V2 tier for the Production environment Additional sizing will be reviewed during the implemenation Azure App Service Plan overview Individual Azure App Services will be deployed to these App Service Plans Azure App Services Separate App Services will be deployed for each integrating application Once deployed and configured we will then deploy the Levridge Integration Framework to each Separate Levridge Integration Framework configuration will be required App Services pricing is included in the App Service Plan tier pricing (see App Service Plan overview above) Azure Storage Account Levridge Integrations utilize Azure Storage Accounts in multiple areas One each required for the Production and Test environments Use the General Purpose V2 account type Azure Storage Account overview Azure Key Vault One key vault is required. Can be used by both the Production and Test environments Azure Key Vault overview","title":"Overview"},{"location":"Integration-Overview/#introduction","text":"The Levridge integration framework provides integration between Dynamics 365 Finance and Dynamics 365 Customer Engagement and 3rd party applications. This document provides and overview of the integration framework and links to the documents that exist for the framework.","title":"Introduction"},{"location":"Integration-Overview/#overview","text":"Levridge has created an integration framework to handle all the integrations between systems. This framework uses json entities to exchange information. The integration framework uses Microsoft Azure Service Bus to provide a Publish and Subscribe messaging pattern . Levridge has created an event framework in D365 Finance that will publish entity data to the service bus based on Creates, Deletes and Updates. Information/messages sent to the service bus can be subscribed to by 3rd party applications or by the Levridge integration framework and sent to 3rd party applications like AgSync. The data entities in D365 Finance and can be filtered so only certain records are sent. All integrations that use the framework follow the same pattern: 1. A data source has an integration event 2. The data source responds to the integration event by sending one or more entities to the service bus. 3. The service bus publishes the message(s) to each subscription 4. An instance of the integration framework receives the message(s) from a subscription 5. The integration framework transforms the message if needed 6. The integration framework sends the message to the target data source The Levridge Integration Framework is most commonly run in the cloud as an Azure App Serivce . It can also run as a windows service or as an IIS application. See this article to learn more about the deployment options.","title":"Overview"},{"location":"Integration-Overview/#integrations","text":"Currently we support the following integrations: D365 F&O to D365 CE D365 CE to D365 F&O Scale Agsync Kahler Surety oneWeigh Field Reveal Levridge CRM Remote Printing Service","title":"Integrations"},{"location":"Integration-Overview/#environment-planning","text":"A standard D365 implementation is used when launching a Levridge environment plan. The D365 F&O System Requirements and what one needs to start a project are outlined in Environment Planning .","title":"Environment Planning"},{"location":"Integration-Overview/#levridge-integration-system-requirements","text":"Azure App Registrations One each for the Production and Test environments Azure App Registrations Azure Service Bus Namespace One each for the Production and Test environments Configure both using the Standard Pricing Tier Topics/Subscriptions will be determined based on integration within scope for the project Documentation Azure Service Bus Overview Azure Service Bus topics and subscriptions Azure App Service Plan One each for Production and Test environments Use a Standard tier for the Test environment Use a Premium V2 tier for the Production environment Additional sizing will be reviewed during the implemenation Azure App Service Plan overview Individual Azure App Services will be deployed to these App Service Plans Azure App Services Separate App Services will be deployed for each integrating application Once deployed and configured we will then deploy the Levridge Integration Framework to each Separate Levridge Integration Framework configuration will be required App Services pricing is included in the App Service Plan tier pricing (see App Service Plan overview above) Azure Storage Account Levridge Integrations utilize Azure Storage Accounts in multiple areas One each required for the Production and Test environments Use the General Purpose V2 account type Azure Storage Account overview Azure Key Vault One key vault is required. Can be used by both the Production and Test environments Azure Key Vault overview","title":"Levridge Integration System Requirements"},{"location":"Kahler/","text":"Kahler Integration The Kahler integration is a bidirectional integration that consists of a Topic for Dispensing Work Orders that go from D365 F&O to Kahler and Dispensing Work Records that go from Kahler to D365 F&O with the goal of margin control and recognizing revenue across site locations. Overview Because this is a bidirectional integration there are two instances of the integration running to handle the entire integration. There is one integration instance for each direction. One aspect of Kahler that is different from other integrations is the D365 F&O to Kahler instance must run on premise because the Kahler system runs behind the firewall. This should be deployed as a service. Another aspect that is different is that each location should only get the messages from the topic that apply to that location. This is done with a filter on the service bus topic subscription. In order for the filter to work the Levridge Entity Event must be configured to expose the branch property on the message. Required Resources Security permissions to access sales orders, transfer orders, and transportation maangement system in D365 F&O Security permissions and access to Kahler's system Setup To integrate to and from Kahler and D365 F&O you will need to: Create an Azure Service bus topic for Dispensing Work Orders (D365 F&O to Kahler) Create an Azure Service bus topic for Dispensing Work Records (Kahler to D365 F&O) Create a subscription on the Dispensing Work Order topic for each Branch that has a Kahler mixer Create a filter on the subscription for each Branch Create a subscription on the Dispensing Work Record topic for integration back to F&O Configure Event Endpoint in F&O Configure Levridge Entity Events You will need to be sure to provide properties on the event to allow filtering by Branch Create an application ID for the integration framework to authenticate to D365 F&O Create an Azure Active Directory Application in D365 F&O Set up Azure Key Vault / Overview Deploy the Levridge Integration Framework as a service at each Branch that has a Kahler mixer Configuration for Kahler on Premise This configuration will need to be on premise with the Kahler mixer. The on-premise instance will handle the Dispensing Work Order from D365 F&O to Kahler and the Webhook that receives Dispensing Work Records from Kahler. The configuration of D365 F&O is required to release to Kahler. Within the release product itself, default warehouses must be defined to ensure that under Manage Inventory default warehouses are set up with a dispensing method to be able to ship to a Kahler. The dispensing method is a 1:1 ration (1 warehouse 1 dispensing method). This configuration produces a URL that is an identifier for each Kahler. Each Kahler instance has its own URL and that URL must be attached to the product to ensure it is sent to the correct web hub. In the appsettings.json you will need to define the InstanceConfig SourceConfig and TargetConfig nodes as follows: \"InstanceConfig\": { \"AzureTableConfiguration\": \"[section name to Azure Table Configuration\", \"LogRequestsAndResponses\": [true or false] \"EnableAppInsightsAdaptiveSampling\": [true or false] }, \"SourceConfig\": { \"ServiceBusConfigName\": \"[section name with Dispensing Work Order service bus topic]\", \"ODataConfigName\": \"[section name with F&O data configuration]\", \"SystemName\": \"DynamicsAX\", \"Direction\": \"Source\" }, \"TargetConfig\": { \"ODataConfigName\": \"[section name with Kahler data configuration]\", \"CDSConfigName\": \"[section name with CDS data configuration]\", \"SystemName\": \"Kahler\", \"Direction\": \"Target\" } Here is a sample template for the entire appsettings.json file used for the on-premise deployment of the integration from FinOps to Kahler: { \"Controllers\": { \"HostController\": \"Levridge.Integration.Host.DefaultController\", \"KahlerConroller\": \"Levridge.Integration.Host.KahlerController\" }, \"Logging\": { \"Debug\": { \"LogLevel\": { \"Default\": \"Information\" } }, \"Console\": { \"IncludeScopes\": true, \"LogLevel\": { \"Default\": \"Information\" } }, \"LogLevel\": { \"Default\": \"Information\" } }, \"AllowedHosts\": \"*\", \"ApplicationInsights\": { \"InstrumentationKey\": \"08f05bc5-e901-4c19-8358-286bdcedf35e\" }, \"InstanceConfig\": { \"AzureTableConfiguration\": \"[section name to Azure Table Configuration\", \"LogRequestsAndResponses\": [true or false] \"EnableAppInsightsAdaptiveSampling\": [true or false] }, \"SourceConfig\": { \"ServiceBusConfigName\": \"Dispensing Work Order\", //[section name with Dispensing Work Order service bus topic] \"ODataConfigName\": \"DynamicsAX\", //[section name with F&O data configuration] \"SystemName\": \"DynamicsAX\", \"Direction\": \"Source\" }, \"TargetConfig\": { \"ODataConfigName\": \"Kahler End Point\", //[section name with Kahler data configuration], \"CDSConfigName\": \"[section name with CDS data configuration]\", \"SystemName\": \"Kahler\", \"Direction\": \"Target\" }, \"DynamicsAX\": { \"UriString\": \"[URL to D365 F&O]\", \"ActiveDirectoryResource\": \"[URL to D365 F&O]\", \"ActiveDirectoryTenant\": \"https://login.microsoftonline.com/[Customer_Tenant_ID]\", \"ActiveDirectoryClientAppId\": \"[Application ID used to register the application in AD]\", \"ActiveDirectoryClientAppSecret\": \"[Client Secret generated for the Application ID in AD]\", \"ODataEntityPath\": \"[URL to D365 F&O]/data/\" }, \"Kahler End Point\": { \"UriString\": \"[URL to Local Kahler]\" }, \"Dispensing Work Order\": { \"ConnectionString\": \"[connection string to Dispensing Work Order Topic]\", \"TopicName\": \"[Dispensing Work Order Topic Name]\", \"SubscriptionName\": \"[Subscription Name for the Branch]\", \"RequiresSession\": true }, \"CDS\": { \"UriString\": \"[URL to CDS or Localhost]\", \"ActiveDirectoryResource\": \"[URL to CDS]\", \"ActiveDirectoryTenant\": \"https://login.microsoftonline.com/[Customer_Tenant_ID]\", \"ActiveDirectoryClientAppId\": \"[Application ID used to register the application in AD]\", \"ActiveDirectoryClientAppSecret\": \"[Client Secret generated for the Application ID in AD]\", \"ODataEntityPath\": \"[URL to CDS]/api/data/v9.0/\", \"AssemblyName\": \"Levridge.ODataDataSources.CDS\", \"ClientClassesNameSpace\": \"Levridge.ODataDataSources.CDS\", \"MetadataResource\": \"CDSMetadata.xml\" }, \"Levridge.Integration.Host.KahlerController\": { \"ConnectionString\": \"[connection string to Dispensing Work Record Topic]\", \"TopicName\": \"[Dispensing Work Record Topic Name]\", \"RequiresSession\": true } } Controllers This section contains a list of controllers that will be loaded by the current instance. In addition to the default controller that we always want to load, we want the system to load the Kahler controller. The names (on the left) are not significant and are used only for debugging. The values (on the right) are significant. It must be the name of the assembly that should be loaded for the controller. InstanceConfig InstanceConfig is an object in the appsettings.json file used by the Levridge Integration Framework to define the configuration for the Current Instance of the integration framework. SourceConfig The source config will represent the data being send from D365 F&O. Currently, the Dispensing Work Order is the only entity sent from F&O. In the future the topic may also include master data that is sent to Kahler. The ODataConfigName is not currently being used but is a required value. So point it to the section that contains the connection information to D365 F&O. TargetConfig The target config will represent the data endpoint for the local Kahler mixer. The \"ODataConfigName\" should point to a section that contains the Kahler REST endpoint. Levridge.Integration.Host.KahlerController This section is used by the Kahler controller to be able to send messages to the proper topic to be handled by the integration framework and written to D365 F&O. Configuration for Kahler in Azure This instance can be a single instance running in the cloud. This instance will handle the Dispensing Work Record from Kahler to D365 F&O. In the appsettings.json you will need to define the InstanceConfig SourceConfig and TargetConfig nodes as follows: \"InstanceConfig\": { \"AzureTableConfiguration\": \"[section name to Azure Table Configuration\", \"LogRequestsAndResponses\": [true or false] \"EnableAppInsightsAdaptiveSampling\": [true or false] }, \"SourceConfig\": { \"ServiceBusConfigName\": \"[section name with Dispensing Work Record service bus topic]\" \"ODataConfigName\": \"\", \"SystemName\": \"Kahler\", \"Direction\": \"Source\" }, \"TargetConfig\": { \"ODataConfigName\": \"[section name with F&O data configuration]\", \"CDSConfigName\": \"[section name with CDS configuration]\", \"SystemName\": \"DynamicsAX\", \"Direction\": \"Target\" } Here is a template of the full appsettings.json file used for the Kahler integration from the Webhook to FinOps: { \"Controllers\": { \"HostController\": \"Levridge.Integration.Host.DefaultController\" }, \"Logging\": { \"ApplicationInsights\": { \"LogLevel\": { \"Default\": \"Trace\" } }, \"Debug\": { \"LogLevel\": { \"Default\": \"Information\" } }, \"Console\": { \"IncludeScopes\": true, \"LogLevel\": { \"Default\": \"Information\" } }, \"LogLevel\": { \"Default\": \"Information\" } }, \"AllowedHosts\": \"*\", \"InstanceConfig\": { \"AzureTableConfiguration\": \"[section name to Azure Table Configuration\", \"LogRequestsAndResponses\": [true or false] \"EnableAppInsightsAdaptiveSampling\": [true or false] }, \"SourceConfig\": { \"ServiceBusConfigName\": \"Dispensing Work Record\", //[section name with Dispensing Work Record service bus topic] \"ODataConfigName\": \"\", \"SystemName\": \"Kahler\", \"Direction\": \"Source\" }, \"TargetConfig\": { \"ODataConfigName\": \"DynamicsAX\", //[section name with F&O data configuration] \"CDSConfigName\": \"CDS\", \"SystemName\": \"DynamicsAX\", \"Direction\": \"Target\" }, \"DynamicsAX\": { \"UriString\": \"[URL to D365 F&O]\", \"ActiveDirectoryResource\": \"[URL to D365 F&O]\", \"ActiveDirectoryTenant\": \"https://login.microsoftonline.com/[Customer_Tenant_ID]\", \"ActiveDirectoryClientAppId\": \"[Application ID used to register the application in AD]\", \"ActiveDirectoryClientAppSecret\": \"[Client Secret generated for the Application ID in AD]\", \"ODataEntityPath\": \"[URL to D365 F&O]/data/\" }, \"Dispensing Work Record\": { \"ConnectionString\": \"[connection string to Dispensing Work Record Topic]\", \"TopicName\": \"[Dispensing Work Record Topic Name]\", \"SubscriptionName\": \"[Subscription Name for Integration to D365 F&O]\", \"RequiresSession\": true } } Dispensing Method Configuration Release Products Grid Highlight line Manage Inventory Warehouse Tab Warehouse Items Dispensing Method field Select the warehouse that should be tied to the dispensing method Select the appropriate configure dispensing method Add Kahler specific URL If the URL is not setup properly for each product, the product could be sent to an incorrect Kahler location. This step is critical to ensure the setup and configurations are accurate. Sales Order Statuses Planned (generated sales order) Booked Released (order is sent to dispatcher work board) Scheduled (generates dispensing work record and assigns application site) Completed Once the order is in a scheduled status, it will generate a dispnsing work record which kicks off the integration to send the product to Kahler. Once Kahler completes the dispatching of product and the onsite work is completed, the work order status is updated to either completed, pending, or review. Data is populated in the work order completion tab in F&O. Work Order Completion Rolling stock ID (task completion equipment) Weather information Humidity Temperature Wind speed Wind direction Pests Acreage completed Start and end times Acmount of actual product applied Once the work is completed by the dispatcher, the work order status updated to the final completion stage (complete/verify). Picking List Within each order, there is a possibility of multiple picking lists. Picking lists annotate the amount of batches/truck loads left Kahler and the mix in each truck. Picking lists allow one to know how many loads are being used. There are two bill options: 1. Bill what Kahler said was used 2. Bill for what the machine stated was used Transportation Management System (TMS) has the ability to generate a freight bill after Kahler sends the picking lists to ensure the third party contractor delivering the product is paid. The steps below outline the process: 1. Data from Kahler creates a picking list 2. Load creation in TMS 3. Generate freight bill invoiceable to the third party contractor Packing Order A packing slip is generated for the work order in an invoiceable status. Packing orders can take place either in the complete, pending, review status or in the complete, verify status. Generate Order Manually The below outlines the steps required to generate a manual order. 1. Create a sales order. The information collected in the sales order includes: - Who the grower is - The inventory location the product is being picked up at 2. Generate product to be shipped to Kahler - Individual products - Custom configuration BOM - Select configuration BOM - Product and supply - Configure line - Configure selected item (product type and amount) - Click OK (BOM is generated) - Select blending site - Save - Explode BOM back out 3. Warehouse items tab - Release for dispensing - Generates dispensing ID - Order is sent to Kahler 4. Manually pack, slip, and post Manual Transfer Order Transfer orders allow product to be moved from one site to another. The below outlines the process: 1. Inventory management 2. Transfer order 3. Populate \"From\" and \"To\" warehouses 4. Add line 5. Select product and amont of product to be transferred 6. Save Ship 7. Release for dispensing 8. Transfer order is sent to Kahler to act on physical shipping of the product.","title":"Kahler"},{"location":"Kahler/#kahler-integration","text":"The Kahler integration is a bidirectional integration that consists of a Topic for Dispensing Work Orders that go from D365 F&O to Kahler and Dispensing Work Records that go from Kahler to D365 F&O with the goal of margin control and recognizing revenue across site locations.","title":"Kahler Integration"},{"location":"Kahler/#overview","text":"Because this is a bidirectional integration there are two instances of the integration running to handle the entire integration. There is one integration instance for each direction. One aspect of Kahler that is different from other integrations is the D365 F&O to Kahler instance must run on premise because the Kahler system runs behind the firewall. This should be deployed as a service. Another aspect that is different is that each location should only get the messages from the topic that apply to that location. This is done with a filter on the service bus topic subscription. In order for the filter to work the Levridge Entity Event must be configured to expose the branch property on the message.","title":"Overview"},{"location":"Kahler/#required-resources","text":"Security permissions to access sales orders, transfer orders, and transportation maangement system in D365 F&O Security permissions and access to Kahler's system","title":"Required Resources"},{"location":"Kahler/#setup","text":"To integrate to and from Kahler and D365 F&O you will need to: Create an Azure Service bus topic for Dispensing Work Orders (D365 F&O to Kahler) Create an Azure Service bus topic for Dispensing Work Records (Kahler to D365 F&O) Create a subscription on the Dispensing Work Order topic for each Branch that has a Kahler mixer Create a filter on the subscription for each Branch Create a subscription on the Dispensing Work Record topic for integration back to F&O Configure Event Endpoint in F&O Configure Levridge Entity Events You will need to be sure to provide properties on the event to allow filtering by Branch Create an application ID for the integration framework to authenticate to D365 F&O Create an Azure Active Directory Application in D365 F&O Set up Azure Key Vault / Overview Deploy the Levridge Integration Framework as a service at each Branch that has a Kahler mixer","title":"Setup"},{"location":"Kahler/#configuration-for-kahler-on-premise","text":"This configuration will need to be on premise with the Kahler mixer. The on-premise instance will handle the Dispensing Work Order from D365 F&O to Kahler and the Webhook that receives Dispensing Work Records from Kahler. The configuration of D365 F&O is required to release to Kahler. Within the release product itself, default warehouses must be defined to ensure that under Manage Inventory default warehouses are set up with a dispensing method to be able to ship to a Kahler. The dispensing method is a 1:1 ration (1 warehouse 1 dispensing method). This configuration produces a URL that is an identifier for each Kahler. Each Kahler instance has its own URL and that URL must be attached to the product to ensure it is sent to the correct web hub. In the appsettings.json you will need to define the InstanceConfig SourceConfig and TargetConfig nodes as follows: \"InstanceConfig\": { \"AzureTableConfiguration\": \"[section name to Azure Table Configuration\", \"LogRequestsAndResponses\": [true or false] \"EnableAppInsightsAdaptiveSampling\": [true or false] }, \"SourceConfig\": { \"ServiceBusConfigName\": \"[section name with Dispensing Work Order service bus topic]\", \"ODataConfigName\": \"[section name with F&O data configuration]\", \"SystemName\": \"DynamicsAX\", \"Direction\": \"Source\" }, \"TargetConfig\": { \"ODataConfigName\": \"[section name with Kahler data configuration]\", \"CDSConfigName\": \"[section name with CDS data configuration]\", \"SystemName\": \"Kahler\", \"Direction\": \"Target\" } Here is a sample template for the entire appsettings.json file used for the on-premise deployment of the integration from FinOps to Kahler: { \"Controllers\": { \"HostController\": \"Levridge.Integration.Host.DefaultController\", \"KahlerConroller\": \"Levridge.Integration.Host.KahlerController\" }, \"Logging\": { \"Debug\": { \"LogLevel\": { \"Default\": \"Information\" } }, \"Console\": { \"IncludeScopes\": true, \"LogLevel\": { \"Default\": \"Information\" } }, \"LogLevel\": { \"Default\": \"Information\" } }, \"AllowedHosts\": \"*\", \"ApplicationInsights\": { \"InstrumentationKey\": \"08f05bc5-e901-4c19-8358-286bdcedf35e\" }, \"InstanceConfig\": { \"AzureTableConfiguration\": \"[section name to Azure Table Configuration\", \"LogRequestsAndResponses\": [true or false] \"EnableAppInsightsAdaptiveSampling\": [true or false] }, \"SourceConfig\": { \"ServiceBusConfigName\": \"Dispensing Work Order\", //[section name with Dispensing Work Order service bus topic] \"ODataConfigName\": \"DynamicsAX\", //[section name with F&O data configuration] \"SystemName\": \"DynamicsAX\", \"Direction\": \"Source\" }, \"TargetConfig\": { \"ODataConfigName\": \"Kahler End Point\", //[section name with Kahler data configuration], \"CDSConfigName\": \"[section name with CDS data configuration]\", \"SystemName\": \"Kahler\", \"Direction\": \"Target\" }, \"DynamicsAX\": { \"UriString\": \"[URL to D365 F&O]\", \"ActiveDirectoryResource\": \"[URL to D365 F&O]\", \"ActiveDirectoryTenant\": \"https://login.microsoftonline.com/[Customer_Tenant_ID]\", \"ActiveDirectoryClientAppId\": \"[Application ID used to register the application in AD]\", \"ActiveDirectoryClientAppSecret\": \"[Client Secret generated for the Application ID in AD]\", \"ODataEntityPath\": \"[URL to D365 F&O]/data/\" }, \"Kahler End Point\": { \"UriString\": \"[URL to Local Kahler]\" }, \"Dispensing Work Order\": { \"ConnectionString\": \"[connection string to Dispensing Work Order Topic]\", \"TopicName\": \"[Dispensing Work Order Topic Name]\", \"SubscriptionName\": \"[Subscription Name for the Branch]\", \"RequiresSession\": true }, \"CDS\": { \"UriString\": \"[URL to CDS or Localhost]\", \"ActiveDirectoryResource\": \"[URL to CDS]\", \"ActiveDirectoryTenant\": \"https://login.microsoftonline.com/[Customer_Tenant_ID]\", \"ActiveDirectoryClientAppId\": \"[Application ID used to register the application in AD]\", \"ActiveDirectoryClientAppSecret\": \"[Client Secret generated for the Application ID in AD]\", \"ODataEntityPath\": \"[URL to CDS]/api/data/v9.0/\", \"AssemblyName\": \"Levridge.ODataDataSources.CDS\", \"ClientClassesNameSpace\": \"Levridge.ODataDataSources.CDS\", \"MetadataResource\": \"CDSMetadata.xml\" }, \"Levridge.Integration.Host.KahlerController\": { \"ConnectionString\": \"[connection string to Dispensing Work Record Topic]\", \"TopicName\": \"[Dispensing Work Record Topic Name]\", \"RequiresSession\": true } }","title":"Configuration for Kahler on Premise"},{"location":"Kahler/#controllers","text":"This section contains a list of controllers that will be loaded by the current instance. In addition to the default controller that we always want to load, we want the system to load the Kahler controller. The names (on the left) are not significant and are used only for debugging. The values (on the right) are significant. It must be the name of the assembly that should be loaded for the controller.","title":"Controllers"},{"location":"Kahler/#instanceconfig","text":"InstanceConfig is an object in the appsettings.json file used by the Levridge Integration Framework to define the configuration for the Current Instance of the integration framework.","title":"InstanceConfig"},{"location":"Kahler/#sourceconfig","text":"The source config will represent the data being send from D365 F&O. Currently, the Dispensing Work Order is the only entity sent from F&O. In the future the topic may also include master data that is sent to Kahler. The ODataConfigName is not currently being used but is a required value. So point it to the section that contains the connection information to D365 F&O.","title":"SourceConfig"},{"location":"Kahler/#targetconfig","text":"The target config will represent the data endpoint for the local Kahler mixer. The \"ODataConfigName\" should point to a section that contains the Kahler REST endpoint.","title":"TargetConfig"},{"location":"Kahler/#levridgeintegrationhostkahlercontroller","text":"This section is used by the Kahler controller to be able to send messages to the proper topic to be handled by the integration framework and written to D365 F&O.","title":"Levridge.Integration.Host.KahlerController"},{"location":"Kahler/#configuration-for-kahler-in-azure","text":"This instance can be a single instance running in the cloud. This instance will handle the Dispensing Work Record from Kahler to D365 F&O. In the appsettings.json you will need to define the InstanceConfig SourceConfig and TargetConfig nodes as follows: \"InstanceConfig\": { \"AzureTableConfiguration\": \"[section name to Azure Table Configuration\", \"LogRequestsAndResponses\": [true or false] \"EnableAppInsightsAdaptiveSampling\": [true or false] }, \"SourceConfig\": { \"ServiceBusConfigName\": \"[section name with Dispensing Work Record service bus topic]\" \"ODataConfigName\": \"\", \"SystemName\": \"Kahler\", \"Direction\": \"Source\" }, \"TargetConfig\": { \"ODataConfigName\": \"[section name with F&O data configuration]\", \"CDSConfigName\": \"[section name with CDS configuration]\", \"SystemName\": \"DynamicsAX\", \"Direction\": \"Target\" } Here is a template of the full appsettings.json file used for the Kahler integration from the Webhook to FinOps: { \"Controllers\": { \"HostController\": \"Levridge.Integration.Host.DefaultController\" }, \"Logging\": { \"ApplicationInsights\": { \"LogLevel\": { \"Default\": \"Trace\" } }, \"Debug\": { \"LogLevel\": { \"Default\": \"Information\" } }, \"Console\": { \"IncludeScopes\": true, \"LogLevel\": { \"Default\": \"Information\" } }, \"LogLevel\": { \"Default\": \"Information\" } }, \"AllowedHosts\": \"*\", \"InstanceConfig\": { \"AzureTableConfiguration\": \"[section name to Azure Table Configuration\", \"LogRequestsAndResponses\": [true or false] \"EnableAppInsightsAdaptiveSampling\": [true or false] }, \"SourceConfig\": { \"ServiceBusConfigName\": \"Dispensing Work Record\", //[section name with Dispensing Work Record service bus topic] \"ODataConfigName\": \"\", \"SystemName\": \"Kahler\", \"Direction\": \"Source\" }, \"TargetConfig\": { \"ODataConfigName\": \"DynamicsAX\", //[section name with F&O data configuration] \"CDSConfigName\": \"CDS\", \"SystemName\": \"DynamicsAX\", \"Direction\": \"Target\" }, \"DynamicsAX\": { \"UriString\": \"[URL to D365 F&O]\", \"ActiveDirectoryResource\": \"[URL to D365 F&O]\", \"ActiveDirectoryTenant\": \"https://login.microsoftonline.com/[Customer_Tenant_ID]\", \"ActiveDirectoryClientAppId\": \"[Application ID used to register the application in AD]\", \"ActiveDirectoryClientAppSecret\": \"[Client Secret generated for the Application ID in AD]\", \"ODataEntityPath\": \"[URL to D365 F&O]/data/\" }, \"Dispensing Work Record\": { \"ConnectionString\": \"[connection string to Dispensing Work Record Topic]\", \"TopicName\": \"[Dispensing Work Record Topic Name]\", \"SubscriptionName\": \"[Subscription Name for Integration to D365 F&O]\", \"RequiresSession\": true } }","title":"Configuration for Kahler in Azure"},{"location":"Kahler/#dispensing-method-configuration","text":"Release Products Grid Highlight line Manage Inventory Warehouse Tab Warehouse Items Dispensing Method field Select the warehouse that should be tied to the dispensing method Select the appropriate configure dispensing method Add Kahler specific URL If the URL is not setup properly for each product, the product could be sent to an incorrect Kahler location. This step is critical to ensure the setup and configurations are accurate.","title":"Dispensing Method Configuration"},{"location":"Kahler/#sales-order-statuses","text":"Planned (generated sales order) Booked Released (order is sent to dispatcher work board) Scheduled (generates dispensing work record and assigns application site) Completed Once the order is in a scheduled status, it will generate a dispnsing work record which kicks off the integration to send the product to Kahler. Once Kahler completes the dispatching of product and the onsite work is completed, the work order status is updated to either completed, pending, or review. Data is populated in the work order completion tab in F&O.","title":"Sales Order Statuses"},{"location":"Kahler/#work-order-completion","text":"Rolling stock ID (task completion equipment) Weather information Humidity Temperature Wind speed Wind direction Pests Acreage completed Start and end times Acmount of actual product applied Once the work is completed by the dispatcher, the work order status updated to the final completion stage (complete/verify).","title":"Work Order Completion"},{"location":"Kahler/#picking-list","text":"Within each order, there is a possibility of multiple picking lists. Picking lists annotate the amount of batches/truck loads left Kahler and the mix in each truck. Picking lists allow one to know how many loads are being used. There are two bill options: 1. Bill what Kahler said was used 2. Bill for what the machine stated was used Transportation Management System (TMS) has the ability to generate a freight bill after Kahler sends the picking lists to ensure the third party contractor delivering the product is paid. The steps below outline the process: 1. Data from Kahler creates a picking list 2. Load creation in TMS 3. Generate freight bill invoiceable to the third party contractor","title":"Picking List"},{"location":"Kahler/#packing-order","text":"A packing slip is generated for the work order in an invoiceable status. Packing orders can take place either in the complete, pending, review status or in the complete, verify status.","title":"Packing Order"},{"location":"Kahler/#generate-order-manually","text":"The below outlines the steps required to generate a manual order. 1. Create a sales order. The information collected in the sales order includes: - Who the grower is - The inventory location the product is being picked up at 2. Generate product to be shipped to Kahler - Individual products - Custom configuration BOM - Select configuration BOM - Product and supply - Configure line - Configure selected item (product type and amount) - Click OK (BOM is generated) - Select blending site - Save - Explode BOM back out 3. Warehouse items tab - Release for dispensing - Generates dispensing ID - Order is sent to Kahler 4. Manually pack, slip, and post","title":"Generate Order Manually"},{"location":"Kahler/#manual-transfer-order","text":"Transfer orders allow product to be moved from one site to another. The below outlines the process: 1. Inventory management 2. Transfer order 3. Populate \"From\" and \"To\" warehouses 4. Add line 5. Select product and amont of product to be transferred 6. Save Ship 7. Release for dispensing 8. Transfer order is sent to Kahler to act on physical shipping of the product.","title":"Manual Transfer Order"},{"location":"KeyVault/","text":"Key Vault Overview","title":"Key Vault"},{"location":"KeyVault/#key-vault","text":"","title":"Key Vault"},{"location":"KeyVault/#overview","text":"","title":"Overview"},{"location":"Levridge-CRM-Remote-Printing-Service/","text":"","title":"Levridge CRM Remote Printing Service"},{"location":"Logging/","text":"Logging Settings This document describes how to configure logging for the Levridge Integration Framework Overview The Levridge Integration Framework utilizes the standard ASP.NET Core logging capabilities. Our primary target for logging is Azure Application Insights however we can also log to the Windows Event Log. This can be usefull if the integration framework is running as a windows service and does not have highspeed internet access to send log data to Application Insights. Log Settings In the appsettings.json file there is a \"Logging\" section that defines the log settings for the various logging providers. A typical logging section may look like this. \"Logging\": { \"ApplicationInsights\": { \"LogLevel\": { \"Default\": \"Trace\" } }, \"Debug\": { \"LogLevel\": { \"Default\": \"Trace\" } }, \"Console\": { \"IncludeScopes\": true, \"LogLevel\": { \"Default\": \"Trace\" } }, \"LogLevel\": { \"Default\": \"Warning\" } }, Unless you are using Debug or Console logging the only setting you need to be concerned with is the ApplicationInsights object. In a typical production environment you will want this set to \"Information\" or \"Warning\" rather than trace. However, to troubleshoot issues you may need to set this to \"Trace\". Valid Log Levels Here are the valid log level settings: Setting Description Trace Logs that contain the most detailed messages. These messages may contain sensitive application data. These messages are disabled by default and should never be enabled in a production environment. Debug Logs that are used for interactive investigation during development. These logs should primarily contain information useful for debugging and have no long-term value. Information Logs that track the general flow of the application. These logs should have long-term value. Warning Logs that highlight an abnormal or unexpected event in the application flow, but do not otherwise cause the application execution to stop. Error Logs that highlight when the current flow of execution is stopped due to a failure. These should indicate a failure in the current activity, not an application-wide failure. Critical Logs that describe an unrecoverable application or system crash, or a catastrophic failure that requires immediate attention. None Not used for writing log messages. Specifies that a logging category should not write any messages. Levridge Guidance for using Log Levels In order to provide a consistent user experience when using the configuration to adjust the log levels we need to have a consistent practice accross all of our code for what information is logged at what level. Generally speaking we would like to have logging set to Information in production unless there is a need to troubleshoot a specific issue. At this level we should be able to see all errors and warnings along with usefull information for basic troubleshooting. Here is what Levridge is logging at the different levels: Trace Log variable states, parameters & request and response payloads as Trace items. Use trace to log verbose information and large payloads. These items may can cause performance issues during normal operation. Debug Log any information that inidates the basic flow of the code. Examples include something like \"Entering POST method\" or \"Enabling authentication.\" This is general information that helps someone follow the flow of the code and can be useful in determining what is happening when an error occures. Information Log specific state information. For example \"Recieved Order 1234\" or \"Enabling AzureAd authentication scheme.\" These messages provide specific instance and state information. For example, in the trace section we showed an example \"Enabling authenticaction\". The represents the current code that is executing but not any instance or state information. On the other hand, logging \"Enabling AzureAd authentication scheme\" tells us which type of authenticaion is being enabled. Warning Log abnormal situations that do not warrent stopping the application and do not leave the application in an unkown state. Error Log error conditions from which we can recover. Critical Log all exceptions as critical. Include as much state information as possible to help with troubleshooting the root cause of the exception. Application Insights Instrumentation Key Application Insights uses an Intrumentation Key to specify the Application Insights resource to use for logging. If the integration framework is deployed to Azure and Application Insights is enabled, the Instrumentation Key will be specified in the environment variables. However, if the Integration is deployed on premise an Instrumentation Key must be specified for the Application Insights logging provider to send the log data to the correct Application Insights resource. \"ApplicationInsights\": { \"InstrumentationKey\": \"[Application Insights Instrumentation Key]\" }, This section is at the top level and not in the Logging section. Here is an example of a partial appsettings.json file. \"Logging\": { \"ApplicationInsights\": { \"LogLevel\": { \"Default\": \"Trace\" } \"LogLevel\": { \"Default\": \"Information\" } }, \"AllowedHosts\": \"*\", \"ApplicationInsights\": { \"InstrumentationKey\": \"1a2b3c4d-5e6f-4b24-9308-4dff05f1cd02\" },","title":"Logging Settings"},{"location":"Logging/#logging-settings","text":"This document describes how to configure logging for the Levridge Integration Framework","title":"Logging Settings"},{"location":"Logging/#overview","text":"The Levridge Integration Framework utilizes the standard ASP.NET Core logging capabilities. Our primary target for logging is Azure Application Insights however we can also log to the Windows Event Log. This can be usefull if the integration framework is running as a windows service and does not have highspeed internet access to send log data to Application Insights.","title":"Overview"},{"location":"Logging/#log-settings","text":"In the appsettings.json file there is a \"Logging\" section that defines the log settings for the various logging providers. A typical logging section may look like this. \"Logging\": { \"ApplicationInsights\": { \"LogLevel\": { \"Default\": \"Trace\" } }, \"Debug\": { \"LogLevel\": { \"Default\": \"Trace\" } }, \"Console\": { \"IncludeScopes\": true, \"LogLevel\": { \"Default\": \"Trace\" } }, \"LogLevel\": { \"Default\": \"Warning\" } }, Unless you are using Debug or Console logging the only setting you need to be concerned with is the ApplicationInsights object. In a typical production environment you will want this set to \"Information\" or \"Warning\" rather than trace. However, to troubleshoot issues you may need to set this to \"Trace\".","title":"Log Settings"},{"location":"Logging/#valid-log-levels","text":"Here are the valid log level settings: Setting Description Trace Logs that contain the most detailed messages. These messages may contain sensitive application data. These messages are disabled by default and should never be enabled in a production environment. Debug Logs that are used for interactive investigation during development. These logs should primarily contain information useful for debugging and have no long-term value. Information Logs that track the general flow of the application. These logs should have long-term value. Warning Logs that highlight an abnormal or unexpected event in the application flow, but do not otherwise cause the application execution to stop. Error Logs that highlight when the current flow of execution is stopped due to a failure. These should indicate a failure in the current activity, not an application-wide failure. Critical Logs that describe an unrecoverable application or system crash, or a catastrophic failure that requires immediate attention. None Not used for writing log messages. Specifies that a logging category should not write any messages.","title":"Valid Log Levels"},{"location":"Logging/#levridge-guidance-for-using-log-levels","text":"In order to provide a consistent user experience when using the configuration to adjust the log levels we need to have a consistent practice accross all of our code for what information is logged at what level. Generally speaking we would like to have logging set to Information in production unless there is a need to troubleshoot a specific issue. At this level we should be able to see all errors and warnings along with usefull information for basic troubleshooting. Here is what Levridge is logging at the different levels:","title":"Levridge Guidance for using Log Levels"},{"location":"Logging/#trace","text":"Log variable states, parameters & request and response payloads as Trace items. Use trace to log verbose information and large payloads. These items may can cause performance issues during normal operation.","title":"Trace"},{"location":"Logging/#debug","text":"Log any information that inidates the basic flow of the code. Examples include something like \"Entering POST method\" or \"Enabling authentication.\" This is general information that helps someone follow the flow of the code and can be useful in determining what is happening when an error occures.","title":"Debug"},{"location":"Logging/#information","text":"Log specific state information. For example \"Recieved Order 1234\" or \"Enabling AzureAd authentication scheme.\" These messages provide specific instance and state information. For example, in the trace section we showed an example \"Enabling authenticaction\". The represents the current code that is executing but not any instance or state information. On the other hand, logging \"Enabling AzureAd authentication scheme\" tells us which type of authenticaion is being enabled.","title":"Information"},{"location":"Logging/#warning","text":"Log abnormal situations that do not warrent stopping the application and do not leave the application in an unkown state.","title":"Warning"},{"location":"Logging/#error","text":"Log error conditions from which we can recover.","title":"Error"},{"location":"Logging/#critical","text":"Log all exceptions as critical. Include as much state information as possible to help with troubleshooting the root cause of the exception.","title":"Critical"},{"location":"Logging/#application-insights-instrumentation-key","text":"Application Insights uses an Intrumentation Key to specify the Application Insights resource to use for logging. If the integration framework is deployed to Azure and Application Insights is enabled, the Instrumentation Key will be specified in the environment variables. However, if the Integration is deployed on premise an Instrumentation Key must be specified for the Application Insights logging provider to send the log data to the correct Application Insights resource. \"ApplicationInsights\": { \"InstrumentationKey\": \"[Application Insights Instrumentation Key]\" }, This section is at the top level and not in the Logging section. Here is an example of a partial appsettings.json file. \"Logging\": { \"ApplicationInsights\": { \"LogLevel\": { \"Default\": \"Trace\" } \"LogLevel\": { \"Default\": \"Information\" } }, \"AllowedHosts\": \"*\", \"ApplicationInsights\": { \"InstrumentationKey\": \"1a2b3c4d-5e6f-4b24-9308-4dff05f1cd02\" },","title":"Application Insights Instrumentation Key"},{"location":"ManagingSplits_SalesContracts_Prepayments/","text":"Managing Splits, Sales Contracts, and Prepayments Overview Within Levridge, when a sales order or invoice is created, the system automatically selects the sales contract according to the items and categories identified. It also selects a prepayment against it, eliminating the need to manually go in and figure out the process, creating fewer invoicing issues sent to growers. Target Audience: This functionality applies to individuals in Ag Retailers who manage splits, contracts, and prepayments on behalf of customers. Product Functionality Create Split Groups in F&O The split group setup is located under Create Split Group in F&O . Sales Agreement Entry In Accounts Receivable > Orders > Sales agreements > Create a new Sales agreement under the \u201cSales Agreement\u201d tab by clicking the New button. On the new sales agreement, select the customer the agreement is for and the item(s) or a sales category(s) the agreement is for. The sales agreement can include any number of items or sales categories for that particular contract. Prepayment Entry In Accounts Receivable > Payments > Customer payment journal > Click the New button to create a new prepayment then click the \u201cEnter customer prepayments\u201d button. Enter the customer the prepayment is for. Enter in the amount of the prepayment. If the prepayment is being used on a sales agreement, the Sales agreement will show up in the \u201cSales agreement\u201d table. Mark the sales agreement the prepayment should be applied against. If there is no sales agreement/contract, you can still take a prepayment and enter it as an Uncontracted amount to ensure the funds are accounted for. A prepayment can have some portion of it applied against an agreement and some set aside as Uncontracted. Once the amounts and agreements have been selected, click \u201cSave in Journal\u201d, annotating the prepayment has been accepted. Click \u201cPost\u201d to process and post the prepayment. Sales Order Entry In Accounts Receivable > Orders > All sales orders > Click the New button to create a new sales order. Enter the customer Once selected, the customer\u2019s account information will auto-populate Choose any split group for that customer on your sales order The field will default to a split group that is 100% but you can select whichever one you would like Choose specific period Enter the items for the sales order and their quantities. Additional order lines can be created. Click Save. Click the Post packing slip button to indicate the product has been sent out to the customer. Once delivered, one is now able to view the sales order with a sales agreement and prepayment allocated towards it according to the items that match between the sales order and the sales agreement and/or prepayment.","title":"Managing Splits, Sales Contracts, and Prepayments"},{"location":"ManagingSplits_SalesContracts_Prepayments/#managing-splits-sales-contracts-and-prepayments","text":"","title":"Managing Splits, Sales Contracts, and Prepayments"},{"location":"ManagingSplits_SalesContracts_Prepayments/#overview","text":"Within Levridge, when a sales order or invoice is created, the system automatically selects the sales contract according to the items and categories identified. It also selects a prepayment against it, eliminating the need to manually go in and figure out the process, creating fewer invoicing issues sent to growers. Target Audience: This functionality applies to individuals in Ag Retailers who manage splits, contracts, and prepayments on behalf of customers.","title":"Overview"},{"location":"ManagingSplits_SalesContracts_Prepayments/#product-functionality","text":"","title":"Product Functionality"},{"location":"ManagingSplits_SalesContracts_Prepayments/#create-split-groups-in-fo","text":"The split group setup is located under Create Split Group in F&O .","title":"Create Split Groups in F&amp;O"},{"location":"ManagingSplits_SalesContracts_Prepayments/#sales-agreement-entry","text":"In Accounts Receivable > Orders > Sales agreements > Create a new Sales agreement under the \u201cSales Agreement\u201d tab by clicking the New button. On the new sales agreement, select the customer the agreement is for and the item(s) or a sales category(s) the agreement is for. The sales agreement can include any number of items or sales categories for that particular contract.","title":"Sales Agreement Entry"},{"location":"ManagingSplits_SalesContracts_Prepayments/#prepayment-entry","text":"In Accounts Receivable > Payments > Customer payment journal > Click the New button to create a new prepayment then click the \u201cEnter customer prepayments\u201d button. Enter the customer the prepayment is for. Enter in the amount of the prepayment. If the prepayment is being used on a sales agreement, the Sales agreement will show up in the \u201cSales agreement\u201d table. Mark the sales agreement the prepayment should be applied against. If there is no sales agreement/contract, you can still take a prepayment and enter it as an Uncontracted amount to ensure the funds are accounted for. A prepayment can have some portion of it applied against an agreement and some set aside as Uncontracted. Once the amounts and agreements have been selected, click \u201cSave in Journal\u201d, annotating the prepayment has been accepted. Click \u201cPost\u201d to process and post the prepayment.","title":"Prepayment Entry"},{"location":"ManagingSplits_SalesContracts_Prepayments/#sales-order-entry","text":"In Accounts Receivable > Orders > All sales orders > Click the New button to create a new sales order. Enter the customer Once selected, the customer\u2019s account information will auto-populate Choose any split group for that customer on your sales order The field will default to a split group that is 100% but you can select whichever one you would like Choose specific period Enter the items for the sales order and their quantities. Additional order lines can be created. Click Save. Click the Post packing slip button to indicate the product has been sent out to the customer. Once delivered, one is now able to view the sales order with a sales agreement and prepayment allocated towards it according to the items that match between the sales order and the sales agreement and/or prepayment.","title":"Sales Order Entry"},{"location":"ODataConfig/","text":"Introduction Brief introduction of the module, component or feature being documented. This document explains ... Overview Main Point 1 Sub Point 1.1","title":"Introduction"},{"location":"ODataConfig/#introduction","text":"Brief introduction of the module, component or feature being documented. This document explains ...","title":"Introduction"},{"location":"ODataConfig/#overview","text":"","title":"Overview"},{"location":"ODataConfig/#main-point-1","text":"","title":"Main Point 1"},{"location":"ODataConfig/#sub-point-11","text":"","title":"Sub Point 1.1"},{"location":"Patronage/","text":"Patronage Overview Patronage is a dividend or distribution that a co-operative pays to its members or investors. Patronage dividends are given based on a proportion of profit that the business makes. Once this amount is determined, management calculates the dividend according to how much each member has used the co-op's services. It is typically an annual process of generating dividends and equity balances. Managing this process has been streamlined and simplified within the Levridge solution. In a few simple steps, you can accurately set up splits, distributions, equity, reports, and cut checks. Implementation Activities The Implementation Activities for Patronage provides an overview of the implementation activities necessary for getting started with processing Patronage. Base Type Functionality The Patronage solutions is operated on Dynamics 365 Customer Engagement (CE) functionality, with an integration from F&O. There is certain functionality that is reliant on the integration between F&O and CE Patronage to get the data into the system, one of those being \u201cProducts\u201d under the Setup navigation area. All the products listed are integrated over from F&O. D365 CE Setup Functionality Products The Product form includes multiple views. This is accomplished by clicking on a certain product line. The two main factors related to Patronage are: 1. Eligible for Patronage: This field is set to either yes or no. 2. Patronage unit: The goal is to know how Patronage is paid out on a product. Products can be sold in different types of unit, for example: ounce, pound, ton, vs. bushel, bag, individually. The default sales unit can be different than the Patronage itself. For example, a product sold in pounds but paid in tons. The Item Category, Relationship to the Product, and Products under Site Navigator are reliant on the integration between F&O and CE Patronage. They are areas integrated from F&O. An important item to note is outlining what item category relates to which Patronage category. The Patronage category is the determining factor of what items will be calculated and what items will be pulled in at different rates based. Stock Classes This section allows a cooperative to set up stock classes. Some cooperatives will have many stock classes, some will have none. Periods Periods include calendar and fiscal periods. These are not integrated from F&O and set up on CE Patronage. - Fiscal period: 1099PATR reporting is run on calendar year, requiring the fiscal period to include a year calendar. - Split due date: A Patronage Split is how a grower would like their patronage eligible transactions to be split to another grower. The Split due date is the date the annual member letters must be returned by. This used when cooperatives send their annual letters to their members and the date they request the letter to be returned. - Minimum Spend: This is the minimum spend amount within a period to receive patronage. There is currently no functionality around this field and more for informational purposes. - Minimum Voting Activity: There is currently no functionality around this field and more for informational purposes. Sources Sources is where the transactional data comes from including previous years. You could have multiple sources if you have had mergers or acquisitions with other cooperatives. They include rules and regulations from mergers and acquisitions with other cooperatives. With those cooperatives comes different rules and regulations, and this is where cooperatives can identify where those transactions come from, different payout rates, and what rules should apply. If a cooperative has not undergone a merger or acquisition, there will more than likely be only one source information. - Primary retirement age: This field is used to set up the retirement age that members will get paid out. This annotates what age the member is eligible to receive Patronage. - Secondary retirement age: This field is used as a secondary option to pay out Patronage. This is the age where Patronage pay outs are mandatory. There is currently no functionality around this field and more for informational purposes. - Default stock class: There is currently no functionality around this field and more for informational purposes. - Minimum payout threshold: Can enter the minimum amount the company will pay patronage on. This is informational and can be used building the payment summary export if included in the query that is built. - Minimum Dividend percentage: Can enter the minimum percentage to receive dividends. This is informational and can be included when building queries. - ERP System: This field is to track where the ERP system information comes from. There is currently no functionality around this field and more for informational purposes. - 1099PATR Filing Entity: This field annotates if a member is filing a 1099PATR. If set to yes, the source is available to create 1099-PATR records for and you can populate the information on the Filing Details tab. - The Filings Details tab outlines the information required on a 1099PATR electronic filing report. - Address, Phone number, Tax ID number, Name, Contact Imported Transaction An Imported Transaction includes the Transaction Details, outlining the Invoice Number, Branch, Source, and the ability to mark whether it is an eligible transaction. The \u201cCreate Patronage Detail\u201d process needs to be run to create eligible transactions utilizing the patronage splits defined on the account record. There is the traceability functionality under \u201cEligible Transactions\u201d where you can view where the transactions came from and how they are being processed and utilized throughout the system. Customer Information Account Form Account Information: The fields are integrated over from F&O. Patronage Contact: is who would be receiving the retirement benefits. Estate Contact: in the case if a member dies and want to identify who we should be working out on behalf of their estate. No functional relevance as what is calculated. Purely who we should talk to if there are questions on payouts. Membership: These fields are integrated over from F&O and include: Membership Type, Tax ID Number, Tax ID Type. Patronage Split Lines: Patronage Split Lines are stored in the Patronage CE and used to calculate the eligible transactions which patronage can be calculated from. Patronage Summary: Displays the equity and payment summary for the account. Summarized by period, payout type, distribution type, and source Stock Summary: Displays the total stock subscriptions by stock class for the account. Reports There are several reporting statements available per account, to include: allocation and payment by account. Stocks Stock Subscriptions outline how many shares one has in a certain stock along with payment date, amount, and certificate number. Stock transfers and adjustments can be completed within the system. There are two stock types: 1. Account type: A transfer of stock to another account. There is the option for an approval process under this type. 2. Stock class type: Changing stocks into another type of stock. Adjustments: if there was a data entry error or, one can make adjustments. This field is highly auditable with several restrictions D365 Finance and Operations Patronage Customers The Customer Account information includes a Membership tab for informational data. It does not drive functionality. It includes the following fields: - Membership Type, Patron, Membership Date, Member ID, and Legal Classification - Legal classification is utilized with 1099PATR reporting Released Product Details includes the Patronage unit where fields on the product need to be set up. The unit conversions come from the Patronage unit field and needs to be set up through F&O to be able to calculate the right unit when you switch over to D365 CE. Payment Details By clicking on \u201cProcess patronage\u201d, one can pull invoices within a specific date range, reading the invoice lines not creating. Converting the invoiced units to patronage units if the patronage unit is different on the released items. It also pulls in charge codes or discounts. Charge codes can be set up to either increase in price or decrease in price depending on how your rules are set up. When a sales order is generated, the F&O will break up the sales order into split allocations, creating an invoice to the customers financially responsible for what was delivered. From the information generated, one can export the data into an Excel file or CSV file to import into CE. This is done by \"Imported Transactions\" under the standard import functionality. You can then filter the list and \"Create Patronage Detail\". This takes the information you have from your patronage splits and whether this item is chargeable and nonchargeable and creates your eligible transactions. The data needs to be in the right format to be imported into CE and can be imported from any system, not just F&O. Eligible Transactions Patronage processing is generated once all eligible transactions have been created. One does not need to bring in imported transactions if they do not use splits or are already split out, they can import right into Eligible Transactions. These are the records that will be used when calculating the equity credit and payments through Patronage Category allocations. Patronage Categories Patronage categories are set up based on what is being paid out and based on sources. If a cooperative has had mergers or acquisitions, or importing categories from previous years, there will likely be a variety of categories Patronage has been processed under. The categories can be calculated by revenue or by units. Direct products are not associated directly to a patronage category instead they choose the item categories of those products. Based on that relationship between item category and product that is what is determined which eligible transactions to pull in when it runs that calculation. An item category can only belong to one patronage category. All the eligible transactions are siphoned into the Patronage Category Allocation. Eligible transactions need to be listed as a sale or purchase. The Percent of Revenue is the total percent of revenue and dictates the Cash Percentage, creating a payment detail. The remaining of that would create an Equity Credit paid out later. These calculations can apply to certain branches. If a Patronage Category is calculated by Units, instead of reference how much is purchased, you are referencing quantity. It is classified as a rate if based on units. The Cash percentage would create a payment detail record with the rest going to an equity credit. Which the traceability you can view were created from that process. If the Patronage Category is calculated by units, Section199A Allocations can be set up which appear on the 1099PATR Deductions will be generated based on units purchased and the payout rate under a Specific Section1 allocation. This feeds into the 1099PATR processing. Once the allocation is run, one can no longer generate a new transaction. It becomes inactive. The other section that is created from this allocation is an equity credit. Equity Equity Credits If an equity is not paid out in the current calendar year, it moves into an equity credit value that would later determine what would be paid out This is created from the Patronage Calculation. These credits are run every year based on the items purchased or sold within that year to see how much would be paid out in the current year along with future years. An equity credit can be adjusted. Equity Revolvements Equity Revolvements are used to determine what to pay out on existing equity credits from prior years. Here is where you annotate what percentage to pay out for a certain source, class, distribution type, or period. Based on that criteria, payment detail records are created. You can set up a Payment Schedule for the credits to be paid out in multiple years. Retirement Equity This can be a manual or run process. The equity summary shows how much equity is available to be paid out and is a tracking mechanism to view whether one qualifies for a payout and if they request one. If a payout is requested, one can create payment details and send over to the system being utilized to for payouts. This process is traceable. Annual Allocations The 1099 information is located under the Annual Allocations navigation. It includes: - Deductions - Non-Patronage Distribution - 1099 Credits - 1099 PATR ##### 1099 PATR The 1099 PATR is a form a cooperative files for each person whom: - The cooperative has paid at least $10 in patronage dividends and other distributions described in IRS tax section 6044(b). - The cooperative withheld any federal income tax under the backup withholding rules regardless of the amount of the payment. When a cooperative collects new information for a 1099 year, they would create a new 1099 PATR form. Under 1099 PATR, click \u201cBuild 1099 PATR\u201d - A dialog box will open asking to select the following: - 1099 Year (a calendar year) - Source (where you want the information to be coming from) - This is the information collected previously under Source. - Click Generate. This generates the records requested and pulls in the details required for the form. - Additional items that make up a 1099 PATR form include: - General Account Information - 1099 Credit - This includes a variety of credit types available. - Deductions - Records created from Patronage categories - Non-Patronage Distributions - Includes all the source data that made up the record. This is necessary for auditing purposes - Payments - Equity Credits - Related The 1099 PATR form is used for electronic filing. The cooperative can print either 1099 PATR-B or 1099 PATR-C versions of the 1099 PATR to provide to their members for tax purposes. The 1099 PATR form is also available online and can be accessed through a portal for members to download. Patronage Power BI Dashboards There are several dashboards available for Patronage through Power BI. These are powerful reporting tools to analyze and view key Patronage data and summaries. They are located: Customer Info navigation tab > Customers > Customer Info > Add Dashboards > Patronage Dashboard The following dashboards are already pre-loaded: - 1099A Deductions - Allocation Summary - Eligible Transactions - Equity Credit - Equity Revolvements - Patronage Dashboard Payment Details and Summary The Patronage Category Process creates an individual payment detail record for each eligible transaction. There is a summarize payment option that if you use the Advanced Find functionality received with CE, you can put in any sort of query you want to show payment details and from there can \"Generate Payment Summary\". This would summarize all the records. Once you have that summary detail, you can export it to an Excel file. This is the data you would bring into your financial system. Once the data is exported, you can go into F&O under Accounts Payable and import the data. This will create a vendor invoice for each record. Once you have the vendor invoices, you can create a journal specific to Patronage. There are designations for specific invoice journals for Patronage. The Patronage journal is different from a standard invoice journal due to bringing in and identifying the customer account. A core function in F&O is when those transactions are processed and imported from CE, F&O can pay out on a different account (a financial split vs payment split). Once in F&O, the customer can decide where they would like the distribution to be sent to and the allocated percentage. There are variety of payment journals one can choose from. When journals are set up to pay out, ensure equity payment is specified along with the check and device format. There are up to five options to choose from. There is the ability for multiple check formats and the ability to include additional information on check stubs. This is located under Payment Proposal.","title":"Equity Management"},{"location":"Patronage/#patronage","text":"","title":"Patronage"},{"location":"Patronage/#overview","text":"Patronage is a dividend or distribution that a co-operative pays to its members or investors. Patronage dividends are given based on a proportion of profit that the business makes. Once this amount is determined, management calculates the dividend according to how much each member has used the co-op's services. It is typically an annual process of generating dividends and equity balances. Managing this process has been streamlined and simplified within the Levridge solution. In a few simple steps, you can accurately set up splits, distributions, equity, reports, and cut checks.","title":"Overview"},{"location":"Patronage/#implementation-activities","text":"The Implementation Activities for Patronage provides an overview of the implementation activities necessary for getting started with processing Patronage.","title":"Implementation Activities"},{"location":"Patronage/#base-type-functionality","text":"The Patronage solutions is operated on Dynamics 365 Customer Engagement (CE) functionality, with an integration from F&O. There is certain functionality that is reliant on the integration between F&O and CE Patronage to get the data into the system, one of those being \u201cProducts\u201d under the Setup navigation area. All the products listed are integrated over from F&O.","title":"Base Type Functionality"},{"location":"Patronage/#d365-ce-setup-functionality","text":"","title":"D365 CE Setup Functionality"},{"location":"Patronage/#products","text":"The Product form includes multiple views. This is accomplished by clicking on a certain product line. The two main factors related to Patronage are: 1. Eligible for Patronage: This field is set to either yes or no. 2. Patronage unit: The goal is to know how Patronage is paid out on a product. Products can be sold in different types of unit, for example: ounce, pound, ton, vs. bushel, bag, individually. The default sales unit can be different than the Patronage itself. For example, a product sold in pounds but paid in tons. The Item Category, Relationship to the Product, and Products under Site Navigator are reliant on the integration between F&O and CE Patronage. They are areas integrated from F&O. An important item to note is outlining what item category relates to which Patronage category. The Patronage category is the determining factor of what items will be calculated and what items will be pulled in at different rates based.","title":"Products"},{"location":"Patronage/#stock-classes","text":"This section allows a cooperative to set up stock classes. Some cooperatives will have many stock classes, some will have none.","title":"Stock Classes"},{"location":"Patronage/#periods","text":"Periods include calendar and fiscal periods. These are not integrated from F&O and set up on CE Patronage. - Fiscal period: 1099PATR reporting is run on calendar year, requiring the fiscal period to include a year calendar. - Split due date: A Patronage Split is how a grower would like their patronage eligible transactions to be split to another grower. The Split due date is the date the annual member letters must be returned by. This used when cooperatives send their annual letters to their members and the date they request the letter to be returned. - Minimum Spend: This is the minimum spend amount within a period to receive patronage. There is currently no functionality around this field and more for informational purposes. - Minimum Voting Activity: There is currently no functionality around this field and more for informational purposes.","title":"Periods"},{"location":"Patronage/#sources","text":"Sources is where the transactional data comes from including previous years. You could have multiple sources if you have had mergers or acquisitions with other cooperatives. They include rules and regulations from mergers and acquisitions with other cooperatives. With those cooperatives comes different rules and regulations, and this is where cooperatives can identify where those transactions come from, different payout rates, and what rules should apply. If a cooperative has not undergone a merger or acquisition, there will more than likely be only one source information. - Primary retirement age: This field is used to set up the retirement age that members will get paid out. This annotates what age the member is eligible to receive Patronage. - Secondary retirement age: This field is used as a secondary option to pay out Patronage. This is the age where Patronage pay outs are mandatory. There is currently no functionality around this field and more for informational purposes. - Default stock class: There is currently no functionality around this field and more for informational purposes. - Minimum payout threshold: Can enter the minimum amount the company will pay patronage on. This is informational and can be used building the payment summary export if included in the query that is built. - Minimum Dividend percentage: Can enter the minimum percentage to receive dividends. This is informational and can be included when building queries. - ERP System: This field is to track where the ERP system information comes from. There is currently no functionality around this field and more for informational purposes. - 1099PATR Filing Entity: This field annotates if a member is filing a 1099PATR. If set to yes, the source is available to create 1099-PATR records for and you can populate the information on the Filing Details tab. - The Filings Details tab outlines the information required on a 1099PATR electronic filing report. - Address, Phone number, Tax ID number, Name, Contact","title":"Sources"},{"location":"Patronage/#imported-transaction","text":"An Imported Transaction includes the Transaction Details, outlining the Invoice Number, Branch, Source, and the ability to mark whether it is an eligible transaction. The \u201cCreate Patronage Detail\u201d process needs to be run to create eligible transactions utilizing the patronage splits defined on the account record. There is the traceability functionality under \u201cEligible Transactions\u201d where you can view where the transactions came from and how they are being processed and utilized throughout the system.","title":"Imported Transaction"},{"location":"Patronage/#customer-information","text":"","title":"Customer Information"},{"location":"Patronage/#account-form","text":"Account Information: The fields are integrated over from F&O. Patronage Contact: is who would be receiving the retirement benefits. Estate Contact: in the case if a member dies and want to identify who we should be working out on behalf of their estate. No functional relevance as what is calculated. Purely who we should talk to if there are questions on payouts. Membership: These fields are integrated over from F&O and include: Membership Type, Tax ID Number, Tax ID Type. Patronage Split Lines: Patronage Split Lines are stored in the Patronage CE and used to calculate the eligible transactions which patronage can be calculated from. Patronage Summary: Displays the equity and payment summary for the account. Summarized by period, payout type, distribution type, and source Stock Summary: Displays the total stock subscriptions by stock class for the account.","title":"Account Form"},{"location":"Patronage/#reports","text":"There are several reporting statements available per account, to include: allocation and payment by account.","title":"Reports"},{"location":"Patronage/#stocks","text":"Stock Subscriptions outline how many shares one has in a certain stock along with payment date, amount, and certificate number. Stock transfers and adjustments can be completed within the system. There are two stock types: 1. Account type: A transfer of stock to another account. There is the option for an approval process under this type. 2. Stock class type: Changing stocks into another type of stock. Adjustments: if there was a data entry error or, one can make adjustments. This field is highly auditable with several restrictions","title":"Stocks"},{"location":"Patronage/#d365-finance-and-operations-patronage","text":"","title":"D365 Finance and Operations Patronage"},{"location":"Patronage/#customers","text":"The Customer Account information includes a Membership tab for informational data. It does not drive functionality. It includes the following fields: - Membership Type, Patron, Membership Date, Member ID, and Legal Classification - Legal classification is utilized with 1099PATR reporting Released Product Details includes the Patronage unit where fields on the product need to be set up. The unit conversions come from the Patronage unit field and needs to be set up through F&O to be able to calculate the right unit when you switch over to D365 CE.","title":"Customers"},{"location":"Patronage/#payment-details","text":"By clicking on \u201cProcess patronage\u201d, one can pull invoices within a specific date range, reading the invoice lines not creating. Converting the invoiced units to patronage units if the patronage unit is different on the released items. It also pulls in charge codes or discounts. Charge codes can be set up to either increase in price or decrease in price depending on how your rules are set up. When a sales order is generated, the F&O will break up the sales order into split allocations, creating an invoice to the customers financially responsible for what was delivered. From the information generated, one can export the data into an Excel file or CSV file to import into CE. This is done by \"Imported Transactions\" under the standard import functionality. You can then filter the list and \"Create Patronage Detail\". This takes the information you have from your patronage splits and whether this item is chargeable and nonchargeable and creates your eligible transactions. The data needs to be in the right format to be imported into CE and can be imported from any system, not just F&O.","title":"Payment Details"},{"location":"Patronage/#eligible-transactions","text":"Patronage processing is generated once all eligible transactions have been created. One does not need to bring in imported transactions if they do not use splits or are already split out, they can import right into Eligible Transactions. These are the records that will be used when calculating the equity credit and payments through Patronage Category allocations.","title":"Eligible Transactions"},{"location":"Patronage/#patronage-categories","text":"Patronage categories are set up based on what is being paid out and based on sources. If a cooperative has had mergers or acquisitions, or importing categories from previous years, there will likely be a variety of categories Patronage has been processed under. The categories can be calculated by revenue or by units. Direct products are not associated directly to a patronage category instead they choose the item categories of those products. Based on that relationship between item category and product that is what is determined which eligible transactions to pull in when it runs that calculation. An item category can only belong to one patronage category. All the eligible transactions are siphoned into the Patronage Category Allocation. Eligible transactions need to be listed as a sale or purchase. The Percent of Revenue is the total percent of revenue and dictates the Cash Percentage, creating a payment detail. The remaining of that would create an Equity Credit paid out later. These calculations can apply to certain branches. If a Patronage Category is calculated by Units, instead of reference how much is purchased, you are referencing quantity. It is classified as a rate if based on units. The Cash percentage would create a payment detail record with the rest going to an equity credit. Which the traceability you can view were created from that process. If the Patronage Category is calculated by units, Section199A Allocations can be set up which appear on the 1099PATR Deductions will be generated based on units purchased and the payout rate under a Specific Section1 allocation. This feeds into the 1099PATR processing. Once the allocation is run, one can no longer generate a new transaction. It becomes inactive. The other section that is created from this allocation is an equity credit.","title":"Patronage Categories"},{"location":"Patronage/#equity","text":"","title":"Equity"},{"location":"Patronage/#equity-credits","text":"If an equity is not paid out in the current calendar year, it moves into an equity credit value that would later determine what would be paid out This is created from the Patronage Calculation. These credits are run every year based on the items purchased or sold within that year to see how much would be paid out in the current year along with future years. An equity credit can be adjusted.","title":"Equity Credits"},{"location":"Patronage/#equity-revolvements","text":"Equity Revolvements are used to determine what to pay out on existing equity credits from prior years. Here is where you annotate what percentage to pay out for a certain source, class, distribution type, or period. Based on that criteria, payment detail records are created. You can set up a Payment Schedule for the credits to be paid out in multiple years.","title":"Equity Revolvements"},{"location":"Patronage/#retirement-equity","text":"This can be a manual or run process. The equity summary shows how much equity is available to be paid out and is a tracking mechanism to view whether one qualifies for a payout and if they request one. If a payout is requested, one can create payment details and send over to the system being utilized to for payouts. This process is traceable.","title":"Retirement Equity"},{"location":"Patronage/#annual-allocations","text":"The 1099 information is located under the Annual Allocations navigation. It includes: - Deductions - Non-Patronage Distribution - 1099 Credits - 1099 PATR ##### 1099 PATR The 1099 PATR is a form a cooperative files for each person whom: - The cooperative has paid at least $10 in patronage dividends and other distributions described in IRS tax section 6044(b). - The cooperative withheld any federal income tax under the backup withholding rules regardless of the amount of the payment. When a cooperative collects new information for a 1099 year, they would create a new 1099 PATR form. Under 1099 PATR, click \u201cBuild 1099 PATR\u201d - A dialog box will open asking to select the following: - 1099 Year (a calendar year) - Source (where you want the information to be coming from) - This is the information collected previously under Source. - Click Generate. This generates the records requested and pulls in the details required for the form. - Additional items that make up a 1099 PATR form include: - General Account Information - 1099 Credit - This includes a variety of credit types available. - Deductions - Records created from Patronage categories - Non-Patronage Distributions - Includes all the source data that made up the record. This is necessary for auditing purposes - Payments - Equity Credits - Related The 1099 PATR form is used for electronic filing. The cooperative can print either 1099 PATR-B or 1099 PATR-C versions of the 1099 PATR to provide to their members for tax purposes. The 1099 PATR form is also available online and can be accessed through a portal for members to download.","title":"Annual Allocations"},{"location":"Patronage/#patronage-power-bi-dashboards","text":"There are several dashboards available for Patronage through Power BI. These are powerful reporting tools to analyze and view key Patronage data and summaries. They are located: Customer Info navigation tab > Customers > Customer Info > Add Dashboards > Patronage Dashboard The following dashboards are already pre-loaded: - 1099A Deductions - Allocation Summary - Eligible Transactions - Equity Credit - Equity Revolvements - Patronage Dashboard","title":"Patronage Power BI Dashboards"},{"location":"Patronage/#payment-details-and-summary","text":"The Patronage Category Process creates an individual payment detail record for each eligible transaction. There is a summarize payment option that if you use the Advanced Find functionality received with CE, you can put in any sort of query you want to show payment details and from there can \"Generate Payment Summary\". This would summarize all the records. Once you have that summary detail, you can export it to an Excel file. This is the data you would bring into your financial system. Once the data is exported, you can go into F&O under Accounts Payable and import the data. This will create a vendor invoice for each record. Once you have the vendor invoices, you can create a journal specific to Patronage. There are designations for specific invoice journals for Patronage. The Patronage journal is different from a standard invoice journal due to bringing in and identifying the customer account. A core function in F&O is when those transactions are processed and imported from CE, F&O can pay out on a different account (a financial split vs payment split). Once in F&O, the customer can decide where they would like the distribution to be sent to and the allocated percentage. There are variety of payment journals one can choose from. When journals are set up to pay out, ensure equity payment is specified along with the check and device format. There are up to five options to choose from. There is the ability for multiple check formats and the ability to include additional information on check stubs. This is located under Payment Proposal.","title":"Payment Details and Summary"},{"location":"PatronageImplementationActivities/","text":"Patronage Implementation Activities Overview The following is an overview of the implementation activities necessary for getting started with processing Patronage. Initial Implementation Activities In FinOps, update released products with a patronage unit In FinOps, setup equity vendor journals In FinOps, setup equity vendor payments including checks In FinOps, setup any patronage payment distributions on customers From FinOps, push via the Levridge integration customers, contacts, item categories, units of measure, branches and addresses to CE In CE, setup patronage specific information: a. Stock classes Sources Periods Patronage split lines Into CE, import stock subscriptions Into CE, import stock subscrptions Into CE, import any equity credits from other systems (if applicable) In CE, create patronage categories a. Setup per unit and revenue allocations Setup 199A allocations In CE, setup requirements equity In CE, setup equity revolvements Year-End Activities In FinOps, export out patronage sales In CE, import imported transactions (these are what was exported from FinOps or other systems) In CE, run patronage detail to create eligible transactions a. Verify results (this takes most of the time) i. Eligible transactions were created properly ii. Split look as expected In CE, run patronage category allocations a. Verify results (this takes most of the time) i. Payments ii. Equity credits In CE, run equity revolvements a. Verify results i. Payment details ii. Equity credits updated accordingly In CE, run retirement equity summaries In CE, create payment summaries a. Verify results In CE, export payment summaries In FinOps, import payment summaries which become vendor journals In FinOps, post vendor journals In FinOps, pay equity vendor invoices Calendar Year-End Activities In CE, create non-patronage distributions for 1099 PATR In CE, create 1099 PATR credits In CE, create 1099 PATR records a. Verify results In CE, build 1099 PATR electronic file a. Verify results In CE, print copy B and copy C of the 1099 PATR records","title":"Patronage Implementation Activities"},{"location":"PatronageImplementationActivities/#patronage-implementation-activities","text":"","title":"Patronage Implementation Activities"},{"location":"PatronageImplementationActivities/#overview","text":"The following is an overview of the implementation activities necessary for getting started with processing Patronage.","title":"Overview"},{"location":"PatronageImplementationActivities/#initial-implementation-activities","text":"In FinOps, update released products with a patronage unit In FinOps, setup equity vendor journals In FinOps, setup equity vendor payments including checks In FinOps, setup any patronage payment distributions on customers From FinOps, push via the Levridge integration customers, contacts, item categories, units of measure, branches and addresses to CE In CE, setup patronage specific information: a. Stock classes Sources Periods Patronage split lines Into CE, import stock subscriptions Into CE, import stock subscrptions Into CE, import any equity credits from other systems (if applicable) In CE, create patronage categories a. Setup per unit and revenue allocations Setup 199A allocations In CE, setup requirements equity In CE, setup equity revolvements","title":"Initial Implementation Activities"},{"location":"PatronageImplementationActivities/#year-end-activities","text":"In FinOps, export out patronage sales In CE, import imported transactions (these are what was exported from FinOps or other systems) In CE, run patronage detail to create eligible transactions a. Verify results (this takes most of the time) i. Eligible transactions were created properly ii. Split look as expected In CE, run patronage category allocations a. Verify results (this takes most of the time) i. Payments ii. Equity credits In CE, run equity revolvements a. Verify results i. Payment details ii. Equity credits updated accordingly In CE, run retirement equity summaries In CE, create payment summaries a. Verify results In CE, export payment summaries In FinOps, import payment summaries which become vendor journals In FinOps, post vendor journals In FinOps, pay equity vendor invoices","title":"Year-End Activities"},{"location":"PatronageImplementationActivities/#calendar-year-end-activities","text":"In CE, create non-patronage distributions for 1099 PATR In CE, create 1099 PATR credits In CE, create 1099 PATR records a. Verify results In CE, build 1099 PATR electronic file a. Verify results In CE, print copy B and copy C of the 1099 PATR records","title":"Calendar Year-End Activities"},{"location":"PreparingCDSForDeployment/","text":"Introduction The Levridge Integration Framework uses the CDS repository to store reference data. We need to provide deployment packages that include solutions, applications and data that can be used to deploy the CDS components to the customer. This document explains the steps necessary to package CDS for deployment to a customer environment. Overview The steps necessary to prepare the CDS components for deployment are: 1. Create the Solution 2. Create the application packages 3. Create the export schemas 4. Export the data 5. Check assets into source control Create Solution Create Application Packages Create Export Schemas Export Data Check-In Assets","title":"Introduction"},{"location":"PreparingCDSForDeployment/#introduction","text":"The Levridge Integration Framework uses the CDS repository to store reference data. We need to provide deployment packages that include solutions, applications and data that can be used to deploy the CDS components to the customer. This document explains the steps necessary to package CDS for deployment to a customer environment.","title":"Introduction"},{"location":"PreparingCDSForDeployment/#overview","text":"The steps necessary to prepare the CDS components for deployment are: 1. Create the Solution 2. Create the application packages 3. Create the export schemas 4. Export the data 5. Check assets into source control","title":"Overview"},{"location":"PreparingCDSForDeployment/#create-solution","text":"","title":"Create Solution"},{"location":"PreparingCDSForDeployment/#create-application-packages","text":"","title":"Create Application Packages"},{"location":"PreparingCDSForDeployment/#create-export-schemas","text":"","title":"Create Export Schemas"},{"location":"PreparingCDSForDeployment/#export-data","text":"","title":"Export Data"},{"location":"PreparingCDSForDeployment/#check-in-assets","text":"","title":"Check-In Assets"},{"location":"PublishCatalog/","text":"Publish Catalog Brief introduction of the module, component or feature being documented. This document explains ... How to Publish a Catalog Go to Procurement and Sourcing > Catalogs > Procurement Catalogs. In the list, find and select the desired record. Click Publish Catalog.","title":"Publish Catalog"},{"location":"PublishCatalog/#publish-catalog","text":"Brief introduction of the module, component or feature being documented. This document explains ...","title":"Publish Catalog"},{"location":"PublishCatalog/#how-to-publish-a-catalog","text":"Go to Procurement and Sourcing > Catalogs > Procurement Catalogs. In the list, find and select the desired record. Click Publish Catalog.","title":"How to Publish a Catalog"},{"location":"PurchasingPolicyAssignment/","text":"Purchasing Policy Assignment Brief introduction of the module, component or feature being documented. This document explains ... How to Purchase Policy Assignments Close the page. Go to Procuremnet and Sourcing > Setup > Policies > Purchasing Policies. In the list, click the link in the selected row. In the list, click the link in the selected row. Click Cancel. In the list, find and select the desired record. In the list, click the link in the selected row. In the tree, select 'Landus\\Monsanto'. Click Add. Click OK. Click Save. Close the page.","title":"Purchasing Policy Assignment"},{"location":"PurchasingPolicyAssignment/#purchasing-policy-assignment","text":"Brief introduction of the module, component or feature being documented. This document explains ...","title":"Purchasing Policy Assignment"},{"location":"PurchasingPolicyAssignment/#how-to-purchase-policy-assignments","text":"Close the page. Go to Procuremnet and Sourcing > Setup > Policies > Purchasing Policies. In the list, click the link in the selected row. In the list, click the link in the selected row. Click Cancel. In the list, find and select the desired record. In the list, click the link in the selected row. In the tree, select 'Landus\\Monsanto'. Click Add. Click OK. Click Save. Close the page.","title":"How to Purchase Policy Assignments"},{"location":"RFQ/","text":"RFQ Brief introduction of the module, component or feature being documented. This document explains ... RFQ Go to Procurement and sourcing > Requests for quotations > All requests for quotations. Click New. In the Solicitation type field, enter or select a value. In the list, click the link in the selected row. In the Requesting department field, type a value. In the warehouse field, type a value. Click OK. In the list, mark the selected row. In the Item number field, type a value. In the Quantity field, enter a number. In the Lines or header field, select an option. In the list, mark the selected row. In the Vendor account field, enter or select a value. In the list, click the link in the selected row. Click Add. In the list, mark the selected row. In the Vendor account field, enter or select a value. Close the page. In the Vendor account field, enter or select a value. In the list, select row 5. In the list, click the link in the selected row. Click Save. Click Send. In the list, mark the selected row. Click OK. Close the page. Close the page. Close the page. Close the page. Click Manage replies. Click Edit. Click Edit RFQ reply. In the list, mark the selected row. In the Unit price field, enter a number. Expand the Line details section. Click Submit. In the list, find and select the desired record. Click Edit. Click Edit RFQ reply. In the list, mark the selected row. In the Unit price field, enter a number. Click Submit. Close the page. Refresh the page. Click Compare replies. In the Show field, select an option. Select the mark check box. Click Accept. In the list, mark the selected row. Click OK. In the list, mark the selected row. Click OK. Close the page. Close the page. Refresh the page. Close the page. Close the page. Go to Procurement and sourving > Purchase orders > All purchase orders. In the list, find and select the desired record. In the list, click the link in the selected row.","title":"RFQ"},{"location":"RFQ/#rfq","text":"Brief introduction of the module, component or feature being documented. This document explains ...","title":"RFQ"},{"location":"RFQ/#rfq_1","text":"Go to Procurement and sourcing > Requests for quotations > All requests for quotations. Click New. In the Solicitation type field, enter or select a value. In the list, click the link in the selected row. In the Requesting department field, type a value. In the warehouse field, type a value. Click OK. In the list, mark the selected row. In the Item number field, type a value. In the Quantity field, enter a number. In the Lines or header field, select an option. In the list, mark the selected row. In the Vendor account field, enter or select a value. In the list, click the link in the selected row. Click Add. In the list, mark the selected row. In the Vendor account field, enter or select a value. Close the page. In the Vendor account field, enter or select a value. In the list, select row 5. In the list, click the link in the selected row. Click Save. Click Send. In the list, mark the selected row. Click OK. Close the page. Close the page. Close the page. Close the page. Click Manage replies. Click Edit. Click Edit RFQ reply. In the list, mark the selected row. In the Unit price field, enter a number. Expand the Line details section. Click Submit. In the list, find and select the desired record. Click Edit. Click Edit RFQ reply. In the list, mark the selected row. In the Unit price field, enter a number. Click Submit. Close the page. Refresh the page. Click Compare replies. In the Show field, select an option. Select the mark check box. Click Accept. In the list, mark the selected row. Click OK. In the list, mark the selected row. Click OK. Close the page. Close the page. Refresh the page. Close the page. Close the page. Go to Procurement and sourving > Purchase orders > All purchase orders. In the list, find and select the desired record. In the list, click the link in the selected row.","title":"RFQ"},{"location":"ReadMe%20%282%29/","text":"Introduction The Levridge integration framework provides integration between Dynamics365 Finance and Operations and Dynamics 365 Customer Engagement and 3rd party applications. This document provides and overview of the integration framework and links to the document that exists for the framework. Overview The Levridge Integration Framework is an entity syncronization framework. It provides a means to synchronize data at an entity level between multiple data sources. All integrations that use the framework follow the same pattern: 1. A data source has an integration event 2. The data source responds to the integration event by sending one or more entities to the service bus. 3. The service bus publishes the message(s) to each subscription 4. An instance of the integration framework receives the message(s) from a subscription 5. The integration framework transforms the message it if needed 6. The integration framework sends the message to the target data source Integrations Currently we support the following integrations: - D365 F&O to D365 CRM - D365 CRM to D365 F&O - Kahler - Agsync - oneWeigh - Field Reveal - Surety - Levridge Scale House D365 F&O to D365 CRM Setup To integrate from D365 F&O to D365 CRM you will need to: - configure Levridge Entity Events - Create an application ID for the integration framework to authenticate to D365 CRM - Create an application user in D365 CRM and assign the proper role(s) - Create an Azure Service bus topic - Create a subscription on the topic above Configuration In the appsettings.json you will need to define the SourceConfig and TargetConfig nodes as follows: \"SourceConfig\": { \"ServiceBusConfigName\": [section name with service bus topic], \"ODataConfigName\": [section name with F&O data configuration], \"SystemName\": \"DynamicsAX\", \"Direction\": \"Source\" }, \"TargetConfig\": { \"ODataConfigName\": [section name with CRM data configuration], \"CDSConfigName\": [section name with CDS data configuration], \"SystemName\": \"DynamicsCRM\", \"Direction\": \"Target\" } D365 CRM to D365 F&O Setup To integrate from D365 CRM to D365 F&O you will need to: - Configure Azure Service Bus Endpoint in CRM - Configure Azure Service Bus plugin on the appropriate entities - Create an application ID for the integration framework to authenticate to D365 F&O - Create an Azure Active Directory Application in D365 F&O - Create an Azure Service bus topic - Create a subscription on the topic above Configuration In the appsettings.json you will need to define the SourceConfig and TargetConfig nodes as follows: \"SourceConfig\": { \"ServiceBusConfigName\": [section name with service bus topic], \"ODataConfigName\": [section name with CRM data configuration], \"SystemName\": \"DynamicsCRM\", \"Direction\": \"Source\" }, \"TargetConfig\": { \"ODataConfigName\": [section name with F&O data configuration], \"CDSConfigName\": [section name with CDS data configuration], \"SystemName\": \"DynamicsAX\", \"Direction\": \"Target\" } Kahler The Kahler integration is a bidirectional integration that consists of a Topic for Dispensing Work Orders that go from D365 F&O to Kahler and Dispensing Work Records that go from Kahler to D365 F&O. This means there are two instances of the integration running to handle the entire integration. One aspect of Kahler that is different from other integrations is the D365 F&O to Kahler instance must run on premise because the Kahler system runs behind the firewall. This should be deployed as a service. Another aspect that is different is that each location should only get the messages from the topic that apply to that location. This is done with a filter on the service bus topic subscription. In order for the filter to work the Levridge Entity Event must be configured to expose the branch property on the message. Setup To integrate to and from Kahler and D365 F&O you will need to: - Configure Levridge Entity Events - You will need to be sure to provide properties on the event to allow filtering by Branch - Create an application ID for the integration framework to authenticate to D365 F&O - Create an Azure Active Directory Application in D365 F&O - Create an Azure Service bus topic for Dispensing Work Orders (D365 F&O to Kahler) - Create an Azure Service bus topic for Dispensing Work Records (Kahler to D365 F&O) - Create a subscription on the Dispensing Work Order topic for each Branch that has a Kahler mixer - Create a filter on the subscription for each Branch - Create a subscription on the Dispensing Work Record topic for integration back to F&O - Deploy the Levridge Integration Framework as a service at each Branch that has a Kahler mixer Configuration for Kahler on Premise This configuration will need to be on premise with the Kahler mixer. In the appsettings.json you will need to define the SourceConfig and TargetConfig nodes as follows: \"Controllers\": { \"HostController\": \"Levridge.Integration.Host.DefaultController\", \"KahlerConroller\": \"Levridge.Integration.Host.KahlerController\" }, \"SourceConfig\": { \"ServiceBusConfigName\": \"Dispensing Work Order\", //[section name with Dispensing Work Order service bus topic] \"ODataConfigName\": \"DynamicsAX\", //[section name with F&O data configuration] \"SystemName\": \"DynamicsAX\", \"Direction\": \"Source\" }, \"TargetConfig\": { \"ODataConfigName\": \"Kahler End Point\", //[section name with Kahler data configuration], \"CDSConfigName\": [section name with CDS data configuration], \"SystemName\": \"Kahler\", \"Direction\": \"Target\" }, \"DynamicsAX\": { \"UriString\": [URL to D365 F&O], \"ActiveDirectoryResource\": [URL to D365 F&O], \"ActiveDirectoryTenant\": \"https://login.microsoftonline.com/[Customer_Tenant_ID]\", \"ActiveDirectoryClientAppId\": [Application ID used to register the application in AD], \"ActiveDirectoryClientAppSecret\": [Client Secret genrated for the Application ID in AD], \"ODataEntityPath\": \"[URL to D365 F&O]/data/\" }, \"Kahler End Point\": { \"UriString\": [URL to Local Kahler] }, \"Dispensing Work Order\": { \"ConnectionString\": [connection string to Dispensing Work Order Topic], \"TopicName\": [Dispensing Work Order Topic Name], \"SubscriptionName\": [Subscription Name for the Branch], \"RequiresSession\": true }, \"Levridge.Integration.Host.KahlerController\": { \"ConnectionString\": [connection string to Dispensing Work Record Topic], \"TopicName\": [Dispensing Work Record Topic Name], \"RequiresSession\": true } Controllers This section contains a list of controllers that will be loaded by the current instance. In addition to the default controller that we alwasy want to load, we want the system to load the Kahler controller. The names (on the left) are not significant and are used only for debugging. The values (on the right) are significant. It must be the name of the assembly that should be loaded for the controller. SourceConfig The source config will represent the data being send from D365 F&O. Currently, the Dispensing Work Order is the only entity sent from F&O. In the future the topic may also include master data that is sent to Kahler. The ODataConfigName is not currently being used but is a required value. So point it to the section that contains the connection information to D365 F&O. TargetConfig The target config will represent the data endpoint for the local Kahler mixer. The \"ODataConfigName\" should point to a section that contains the Kahler REST endpoint. Levridge.Integration.Host.KahlerController This section is used by the Kahler controller to be able to send messages to the proper topic to be handled by the integration framework and written to D365 F&O Configuration for Kahler in Azure This instance can be a single instance runing in the cloud. In the appsettings.json you will need to define the SourceConfig and TargetConfig nodes as follows: \"SourceConfig\": { \"ServiceBusConfigName\": \"Dispensing Work Record\", //[section name with Dispensing Work Record service bus topic] \"SystemName\": \"Kahler\", \"Direction\": \"Source\" }, \"TargetConfig\": { \"ODataConfigName\": \"DynamicsAX\", //[section name with F&O data configuration] \"SystemName\": \"DynamicsAX\", \"Direction\": \"Target\" }, \"DynamicsAX\": { \"UriString\": [URL to D365 F&O], \"ActiveDirectoryResource\": [URL to D365 F&O], \"ActiveDirectoryTenant\": \"https://login.microsoftonline.com/[Customer_Tenant_ID]\", \"ActiveDirectoryClientAppId\": [Application ID used to register the application in AD], \"ActiveDirectoryClientAppSecret\": [Client Secret genrated for the Application ID in AD], \"ODataEntityPath\": \"[URL to D365 F&O]/data/\" }, \"Dispensing Work Record\": { \"ConnectionString\": [connection string to Dispensing Work Record Topic], \"TopicName\": [Dispensing Work Record Topic Name], \"SubscriptionName\": [Subscription Name for Integration to D365 F&O], \"RequiresSession\": true } Agsync oneWeigh Field Reveal Surety Levridge Scale House","title":"Introduction"},{"location":"ReadMe%20%282%29/#introduction","text":"The Levridge integration framework provides integration between Dynamics365 Finance and Operations and Dynamics 365 Customer Engagement and 3rd party applications. This document provides and overview of the integration framework and links to the document that exists for the framework.","title":"Introduction"},{"location":"ReadMe%20%282%29/#overview","text":"The Levridge Integration Framework is an entity syncronization framework. It provides a means to synchronize data at an entity level between multiple data sources. All integrations that use the framework follow the same pattern: 1. A data source has an integration event 2. The data source responds to the integration event by sending one or more entities to the service bus. 3. The service bus publishes the message(s) to each subscription 4. An instance of the integration framework receives the message(s) from a subscription 5. The integration framework transforms the message it if needed 6. The integration framework sends the message to the target data source","title":"Overview"},{"location":"ReadMe%20%282%29/#integrations","text":"Currently we support the following integrations: - D365 F&O to D365 CRM - D365 CRM to D365 F&O - Kahler - Agsync - oneWeigh - Field Reveal - Surety - Levridge Scale House","title":"Integrations"},{"location":"ReadMe%20%282%29/#d365-fo-to-d365-crm","text":"","title":"D365 F&amp;O to D365 CRM"},{"location":"ReadMe%20%282%29/#setup","text":"To integrate from D365 F&O to D365 CRM you will need to: - configure Levridge Entity Events - Create an application ID for the integration framework to authenticate to D365 CRM - Create an application user in D365 CRM and assign the proper role(s) - Create an Azure Service bus topic - Create a subscription on the topic above","title":"Setup"},{"location":"ReadMe%20%282%29/#configuration","text":"In the appsettings.json you will need to define the SourceConfig and TargetConfig nodes as follows: \"SourceConfig\": { \"ServiceBusConfigName\": [section name with service bus topic], \"ODataConfigName\": [section name with F&O data configuration], \"SystemName\": \"DynamicsAX\", \"Direction\": \"Source\" }, \"TargetConfig\": { \"ODataConfigName\": [section name with CRM data configuration], \"CDSConfigName\": [section name with CDS data configuration], \"SystemName\": \"DynamicsCRM\", \"Direction\": \"Target\" }","title":"Configuration"},{"location":"ReadMe%20%282%29/#d365-crm-to-d365-fo","text":"","title":"D365 CRM to D365 F&amp;O"},{"location":"ReadMe%20%282%29/#setup_1","text":"To integrate from D365 CRM to D365 F&O you will need to: - Configure Azure Service Bus Endpoint in CRM - Configure Azure Service Bus plugin on the appropriate entities - Create an application ID for the integration framework to authenticate to D365 F&O - Create an Azure Active Directory Application in D365 F&O - Create an Azure Service bus topic - Create a subscription on the topic above","title":"Setup"},{"location":"ReadMe%20%282%29/#configuration_1","text":"In the appsettings.json you will need to define the SourceConfig and TargetConfig nodes as follows: \"SourceConfig\": { \"ServiceBusConfigName\": [section name with service bus topic], \"ODataConfigName\": [section name with CRM data configuration], \"SystemName\": \"DynamicsCRM\", \"Direction\": \"Source\" }, \"TargetConfig\": { \"ODataConfigName\": [section name with F&O data configuration], \"CDSConfigName\": [section name with CDS data configuration], \"SystemName\": \"DynamicsAX\", \"Direction\": \"Target\" }","title":"Configuration"},{"location":"ReadMe%20%282%29/#kahler","text":"The Kahler integration is a bidirectional integration that consists of a Topic for Dispensing Work Orders that go from D365 F&O to Kahler and Dispensing Work Records that go from Kahler to D365 F&O. This means there are two instances of the integration running to handle the entire integration. One aspect of Kahler that is different from other integrations is the D365 F&O to Kahler instance must run on premise because the Kahler system runs behind the firewall. This should be deployed as a service. Another aspect that is different is that each location should only get the messages from the topic that apply to that location. This is done with a filter on the service bus topic subscription. In order for the filter to work the Levridge Entity Event must be configured to expose the branch property on the message.","title":"Kahler"},{"location":"ReadMe%20%282%29/#setup_2","text":"To integrate to and from Kahler and D365 F&O you will need to: - Configure Levridge Entity Events - You will need to be sure to provide properties on the event to allow filtering by Branch - Create an application ID for the integration framework to authenticate to D365 F&O - Create an Azure Active Directory Application in D365 F&O - Create an Azure Service bus topic for Dispensing Work Orders (D365 F&O to Kahler) - Create an Azure Service bus topic for Dispensing Work Records (Kahler to D365 F&O) - Create a subscription on the Dispensing Work Order topic for each Branch that has a Kahler mixer - Create a filter on the subscription for each Branch - Create a subscription on the Dispensing Work Record topic for integration back to F&O - Deploy the Levridge Integration Framework as a service at each Branch that has a Kahler mixer","title":"Setup"},{"location":"ReadMe%20%282%29/#configuration-for-kahler-on-premise","text":"This configuration will need to be on premise with the Kahler mixer. In the appsettings.json you will need to define the SourceConfig and TargetConfig nodes as follows: \"Controllers\": { \"HostController\": \"Levridge.Integration.Host.DefaultController\", \"KahlerConroller\": \"Levridge.Integration.Host.KahlerController\" }, \"SourceConfig\": { \"ServiceBusConfigName\": \"Dispensing Work Order\", //[section name with Dispensing Work Order service bus topic] \"ODataConfigName\": \"DynamicsAX\", //[section name with F&O data configuration] \"SystemName\": \"DynamicsAX\", \"Direction\": \"Source\" }, \"TargetConfig\": { \"ODataConfigName\": \"Kahler End Point\", //[section name with Kahler data configuration], \"CDSConfigName\": [section name with CDS data configuration], \"SystemName\": \"Kahler\", \"Direction\": \"Target\" }, \"DynamicsAX\": { \"UriString\": [URL to D365 F&O], \"ActiveDirectoryResource\": [URL to D365 F&O], \"ActiveDirectoryTenant\": \"https://login.microsoftonline.com/[Customer_Tenant_ID]\", \"ActiveDirectoryClientAppId\": [Application ID used to register the application in AD], \"ActiveDirectoryClientAppSecret\": [Client Secret genrated for the Application ID in AD], \"ODataEntityPath\": \"[URL to D365 F&O]/data/\" }, \"Kahler End Point\": { \"UriString\": [URL to Local Kahler] }, \"Dispensing Work Order\": { \"ConnectionString\": [connection string to Dispensing Work Order Topic], \"TopicName\": [Dispensing Work Order Topic Name], \"SubscriptionName\": [Subscription Name for the Branch], \"RequiresSession\": true }, \"Levridge.Integration.Host.KahlerController\": { \"ConnectionString\": [connection string to Dispensing Work Record Topic], \"TopicName\": [Dispensing Work Record Topic Name], \"RequiresSession\": true }","title":"Configuration for Kahler on Premise"},{"location":"ReadMe%20%282%29/#controllers","text":"This section contains a list of controllers that will be loaded by the current instance. In addition to the default controller that we alwasy want to load, we want the system to load the Kahler controller. The names (on the left) are not significant and are used only for debugging. The values (on the right) are significant. It must be the name of the assembly that should be loaded for the controller.","title":"Controllers"},{"location":"ReadMe%20%282%29/#sourceconfig","text":"The source config will represent the data being send from D365 F&O. Currently, the Dispensing Work Order is the only entity sent from F&O. In the future the topic may also include master data that is sent to Kahler. The ODataConfigName is not currently being used but is a required value. So point it to the section that contains the connection information to D365 F&O.","title":"SourceConfig"},{"location":"ReadMe%20%282%29/#targetconfig","text":"The target config will represent the data endpoint for the local Kahler mixer. The \"ODataConfigName\" should point to a section that contains the Kahler REST endpoint.","title":"TargetConfig"},{"location":"ReadMe%20%282%29/#levridgeintegrationhostkahlercontroller","text":"This section is used by the Kahler controller to be able to send messages to the proper topic to be handled by the integration framework and written to D365 F&O","title":"Levridge.Integration.Host.KahlerController"},{"location":"ReadMe%20%282%29/#configuration-for-kahler-in-azure","text":"This instance can be a single instance runing in the cloud. In the appsettings.json you will need to define the SourceConfig and TargetConfig nodes as follows: \"SourceConfig\": { \"ServiceBusConfigName\": \"Dispensing Work Record\", //[section name with Dispensing Work Record service bus topic] \"SystemName\": \"Kahler\", \"Direction\": \"Source\" }, \"TargetConfig\": { \"ODataConfigName\": \"DynamicsAX\", //[section name with F&O data configuration] \"SystemName\": \"DynamicsAX\", \"Direction\": \"Target\" }, \"DynamicsAX\": { \"UriString\": [URL to D365 F&O], \"ActiveDirectoryResource\": [URL to D365 F&O], \"ActiveDirectoryTenant\": \"https://login.microsoftonline.com/[Customer_Tenant_ID]\", \"ActiveDirectoryClientAppId\": [Application ID used to register the application in AD], \"ActiveDirectoryClientAppSecret\": [Client Secret genrated for the Application ID in AD], \"ODataEntityPath\": \"[URL to D365 F&O]/data/\" }, \"Dispensing Work Record\": { \"ConnectionString\": [connection string to Dispensing Work Record Topic], \"TopicName\": [Dispensing Work Record Topic Name], \"SubscriptionName\": [Subscription Name for Integration to D365 F&O], \"RequiresSession\": true }","title":"Configuration for Kahler in Azure"},{"location":"ReadMe%20%282%29/#agsync","text":"","title":"Agsync"},{"location":"ReadMe%20%282%29/#oneweigh","text":"","title":"oneWeigh"},{"location":"ReadMe%20%282%29/#field-reveal","text":"","title":"Field Reveal"},{"location":"ReadMe%20%282%29/#surety","text":"","title":"Surety"},{"location":"ReadMe%20%282%29/#levridge-scale-house","text":"","title":"Levridge Scale House"},{"location":"Recommendation-Integration/","text":"Introduction Brief introduction of the module, component or feature being documented. This document explains ... Overview Main Point 1 Sub Point 1.1","title":"Introduction"},{"location":"Recommendation-Integration/#introduction","text":"Brief introduction of the module, component or feature being documented. This document explains ...","title":"Introduction"},{"location":"Recommendation-Integration/#overview","text":"","title":"Overview"},{"location":"Recommendation-Integration/#main-point-1","text":"","title":"Main Point 1"},{"location":"Recommendation-Integration/#sub-point-11","text":"","title":"Sub Point 1.1"},{"location":"Scale-Overview/","text":"Scale Overview The purpose of Levridge Scale is to receive all the data measured at the scales at ag retailer locations and incorporate it into a scale ticket which can be printed out and given to the grower, while also bringing the scale ticket into the Dynamics 365 ERP system. Within the accounting technology solution, the contents of the scale ticket can be accounted for and posted against contracts the grower has with the ag retailer. The LevridgeScaleApp structure is a browser-based Scale application with the functionality of being able to operate as a server or as a desktop. 1. IIS (Internet Information Services) which is a Microsoft web server. 2. A Windows desktop, which is a background Windows service. The functionality is chosen based on the specific requirements of the clients. If the client\u2019s functionality only includes a Windows desktop station, Levridge Scale App can be run as a service capability as not every scale will have a server. Scale Settings Configuration How to Install Levridge Scale How to Update Levridge Scale ScaleHead Hardware Setup for Rice Lake 920i and Rice Lake 1280 Scale Appsettings Configuration Scale Implementation Activities IIS Server Functionality If LevridgeScaleApp is ran as a server, it would operate with a Reverse Proxy. This is very important for implementation purposes. Required server information include the Printer Server and API Server. Clients are always classified as \u201cserver\u201d regardless if operating as a service or server. The intent is the client operates to serve content to the client. The two servers are: 1. LevScaleAPI - This server handles all the back-end calls. This is where the integration framework communications with the scale. O-Data calls which are in turn the integration framework. O-Data is very necessary due to the ability to operate with other ERP systems. 2. LevScalePrint - This server renders all the reports for printing. It communicates with the client printer service. This is within the .net framework. Desktop Environment Service Functionality The four services within a Windows desktop environment include: LevPrintService Can install the LevPrintService to any computer within a client\u2019s Intranet. It has the capability of rendering reports at any location if the LevPrintClient is registered at the specific location. This also includes the Zebra printing capability. LevHardwareService This is the service that handles communication with hardware such as the scale head. LevridgeAXToScale Along with LevridgeScaleToAX is the integration framework. The ratio would be one LevridgeAXToScale and one for the current desktop or LevScale instance. This enables F&O to broadcast all internal information that is necessary. LevridgeAXToScale would be looking at the specific service bus channel. It will then write into the LevScale through the LevScaleAPI. 4.LevScale LevScale is the defining factor between a server and service. LevScaleClient and LevScale are executing the same file path and processes. This is operating a straight web server without the reverse proxy capability. Scale Ticket Types Inbound Grain Outbound Grain Agronomy Sales Transfer Weight Only Trucks in Yard These will be shown \"in the yard\" since they are uncompleted tickets that have not been printed. Scale Ticket History This is the history of printed tickets from today and the previous day. Application Configuration This is where the settings to change Site, Print, Etc. are located. Delivery Sheets Here you can view and create delivery sheets for grain tickets. The scale tickets and delivery sheets will sync both ways between F&O and the scale app. Create Scale Tickets http://levscaledev.corp.stoneridgesoftware.com/","title":"Scale Overview"},{"location":"Scale-Overview/#scale-overview","text":"The purpose of Levridge Scale is to receive all the data measured at the scales at ag retailer locations and incorporate it into a scale ticket which can be printed out and given to the grower, while also bringing the scale ticket into the Dynamics 365 ERP system. Within the accounting technology solution, the contents of the scale ticket can be accounted for and posted against contracts the grower has with the ag retailer. The LevridgeScaleApp structure is a browser-based Scale application with the functionality of being able to operate as a server or as a desktop. 1. IIS (Internet Information Services) which is a Microsoft web server. 2. A Windows desktop, which is a background Windows service. The functionality is chosen based on the specific requirements of the clients. If the client\u2019s functionality only includes a Windows desktop station, Levridge Scale App can be run as a service capability as not every scale will have a server. Scale Settings Configuration How to Install Levridge Scale How to Update Levridge Scale ScaleHead Hardware Setup for Rice Lake 920i and Rice Lake 1280 Scale Appsettings Configuration Scale Implementation Activities","title":"Scale Overview"},{"location":"Scale-Overview/#iis-server-functionality","text":"If LevridgeScaleApp is ran as a server, it would operate with a Reverse Proxy. This is very important for implementation purposes. Required server information include the Printer Server and API Server. Clients are always classified as \u201cserver\u201d regardless if operating as a service or server. The intent is the client operates to serve content to the client. The two servers are: 1. LevScaleAPI - This server handles all the back-end calls. This is where the integration framework communications with the scale. O-Data calls which are in turn the integration framework. O-Data is very necessary due to the ability to operate with other ERP systems. 2. LevScalePrint - This server renders all the reports for printing. It communicates with the client printer service. This is within the .net framework.","title":"IIS Server Functionality"},{"location":"Scale-Overview/#desktop-environment-service-functionality","text":"The four services within a Windows desktop environment include: LevPrintService Can install the LevPrintService to any computer within a client\u2019s Intranet. It has the capability of rendering reports at any location if the LevPrintClient is registered at the specific location. This also includes the Zebra printing capability. LevHardwareService This is the service that handles communication with hardware such as the scale head. LevridgeAXToScale Along with LevridgeScaleToAX is the integration framework. The ratio would be one LevridgeAXToScale and one for the current desktop or LevScale instance. This enables F&O to broadcast all internal information that is necessary. LevridgeAXToScale would be looking at the specific service bus channel. It will then write into the LevScale through the LevScaleAPI. 4.LevScale LevScale is the defining factor between a server and service. LevScaleClient and LevScale are executing the same file path and processes. This is operating a straight web server without the reverse proxy capability.","title":"Desktop Environment Service Functionality"},{"location":"Scale-Overview/#scale-ticket-types","text":"Inbound Grain Outbound Grain Agronomy Sales Transfer Weight Only","title":"Scale Ticket Types"},{"location":"Scale-Overview/#trucks-in-yard","text":"These will be shown \"in the yard\" since they are uncompleted tickets that have not been printed.","title":"Trucks in Yard"},{"location":"Scale-Overview/#scale-ticket-history","text":"This is the history of printed tickets from today and the previous day.","title":"Scale Ticket History"},{"location":"Scale-Overview/#application-configuration","text":"This is where the settings to change Site, Print, Etc. are located.","title":"Application Configuration"},{"location":"Scale-Overview/#delivery-sheets","text":"Here you can view and create delivery sheets for grain tickets. The scale tickets and delivery sheets will sync both ways between F&O and the scale app.","title":"Delivery Sheets"},{"location":"Scale-Overview/#create-scale-tickets","text":"http://levscaledev.corp.stoneridgesoftware.com/","title":"Create Scale Tickets"},{"location":"ScaleHeadHardwareSetup/","text":"Scale Hardware Setup Rice Lake 920i Add the Equipment Information In Levridge Scale, click Application Configuration > Equipment Management Click Create New Set Is active = Yes Set Is scale = Yes Enter Equipment Name - this is the name that will show in the UI and on printed reports Equipment description - not used Location - not used Custom character = L Click Save, then edit for the next two fields to be visible Equipment response type ID = 4 Return response type ID = 5 Service URL = http://localhost:5000 Click Save Add the Equipment Field Details From Equipment Management, click Edit on the Equipment record Click Equipment Field Details Click Create New Name = \"weight\" SequenceId = 1 Check the box by OutputStreamTypeId FieldName = \"weight\" StringLength = 30 ReturnTypeResult = String Click Create Click Back to Equipment Add the Connection Details Click Connection Details Name = 1 Hostname = <ip address of scalehead> Port = 10001 TypeId = TcpIPClientStream Click Submit Create a Workstation In LevridgeScale, click Application Configuration > Workstation Management Click Create New Enter a Workstation name, ex. \"Ws1\" Click Create Click Edit Under Equipment, mark the box next to the Scale Click Save Configure the Hardware Service with the Workstation name Navigate to the folder LevridgeScaleHouse\\Services\\HardwareInterface Open appsettings.json Enter the WorkstationName, example: \"WorkstationName\" : \"Ws1\" Restart Hardware service Open (Windows) Services Restart LevScaleHardwareService Rice Lake 1280 Configure the 1280 Access the Scale UI in a browser. http://* :3000 In the Menu go to Configuration > Communications Select Ethernet, TCP Client 2 Remote Address = < ip address of Levridge Scale PC > Remote Port Number = 10001 Add the Equipment Information In Levridge Scale, click Application Configuration > Equipment Management Click Create New Set Is active = Yes Set Is scale = Yes Enter Equipment Name - this is the name that will show in the UI and on printed reports Equipment description - not used Location - not used Custom character = L Click Save, then edit for the next two fields to be visible Equipment response type ID = 4 Return response type ID = 5 Service URL = http://localhost:5000 Click Save Add the Equipment Field Details From Equipment Management, click Edit on the Equipment record Click Equipment Field Details Click Create New Name = \"weight\" SequenceId = 1 Check the box by OutputStreamTypeId FieldName = \"weight\" StringLength = 30 ReturnTypeResult = String Click Create Click Back to Equipment Add the Connection Details Click Connection Details Name = 1 Hostname = <ip address of scalehead> Port = 10001 TypeId = TcpIPCStream Click Submit Create a Workstation In LevridgeScale, click Application Configuration > Workstation Management Click Create New Enter a Workstation name, ex. \"Ws1\" Click Create Click Edit Under Equipment, mark the box next to the Scale Click Save Configure the Hardware Service with the Workstation name Navigate to the folder LevridgeScaleHouse\\Services\\HardwareInterface Open appsettings.json Enter the WorkstationName, example: \"WorkstationName\" : \"Ws1\" Restart Hardware service Open (Windows) Services Restart LevScaleHardwareService Serial Connection Add the Equipment Information In Levridge Scale, click Application Configuration > Equipment Management Click Create New Set Is active = Yes Set Is scale = Yes Enter Equipment Name - this is the name that will show in the UI and on printed reports Equipment description - not used Location - not used Custom character = L Click Save, then edit for the next two fields to be visible Equipment response type ID = 4 Return response type ID = 5 Service URL = http://localhost:5000 Click Save Add the Equipment Field Details From Equipment Management, click Edit on the Equipment record Click Equipment Field Details Click Create New Name = \"weight\" SequenceId = 1 Check the box by OutputStreamTypeId FieldName = \"weight\" StringLength = 30 ReturnTypeResult = String Click Create Click Back to Equipment Add the Connection Details Click Connection Details Name = COM1 Speed = 9600 DataBits = 8 StopBits = 1 Parity = N FlowControl = 0 TypeId = SerialStream Click Submit Create a Workstation In LevridgeScale, click Application Configuration > Workstation Management Click Create New Enter a Workstation name, ex. \"Ws1\" Click Create Click Edit Under Equipment, mark the box next to the Scale Click Save Configure the Hardware Service with the Workstation name Navigate to the folder LevridgeScaleHouse\\Services\\HardwareInterface Open appsettings.json Enter the WorkstationName, example: \"WorkstationName\" : \"Ws1\" Restart Hardware service Open (Windows) Services Restart LevScaleHardwareService","title":"Scale Hardware Setup"},{"location":"ScaleHeadHardwareSetup/#scale-hardware-setup","text":"","title":"Scale Hardware Setup"},{"location":"ScaleHeadHardwareSetup/#rice-lake-920i","text":"Add the Equipment Information In Levridge Scale, click Application Configuration > Equipment Management Click Create New Set Is active = Yes Set Is scale = Yes Enter Equipment Name - this is the name that will show in the UI and on printed reports Equipment description - not used Location - not used Custom character = L Click Save, then edit for the next two fields to be visible Equipment response type ID = 4 Return response type ID = 5 Service URL = http://localhost:5000 Click Save Add the Equipment Field Details From Equipment Management, click Edit on the Equipment record Click Equipment Field Details Click Create New Name = \"weight\" SequenceId = 1 Check the box by OutputStreamTypeId FieldName = \"weight\" StringLength = 30 ReturnTypeResult = String Click Create Click Back to Equipment Add the Connection Details Click Connection Details Name = 1 Hostname = <ip address of scalehead> Port = 10001 TypeId = TcpIPClientStream Click Submit Create a Workstation In LevridgeScale, click Application Configuration > Workstation Management Click Create New Enter a Workstation name, ex. \"Ws1\" Click Create Click Edit Under Equipment, mark the box next to the Scale Click Save Configure the Hardware Service with the Workstation name Navigate to the folder LevridgeScaleHouse\\Services\\HardwareInterface Open appsettings.json Enter the WorkstationName, example: \"WorkstationName\" : \"Ws1\" Restart Hardware service Open (Windows) Services Restart LevScaleHardwareService","title":"Rice Lake 920i"},{"location":"ScaleHeadHardwareSetup/#rice-lake-1280","text":"Configure the 1280 Access the Scale UI in a browser. http://* :3000 In the Menu go to Configuration > Communications Select Ethernet, TCP Client 2 Remote Address = < ip address of Levridge Scale PC > Remote Port Number = 10001 Add the Equipment Information In Levridge Scale, click Application Configuration > Equipment Management Click Create New Set Is active = Yes Set Is scale = Yes Enter Equipment Name - this is the name that will show in the UI and on printed reports Equipment description - not used Location - not used Custom character = L Click Save, then edit for the next two fields to be visible Equipment response type ID = 4 Return response type ID = 5 Service URL = http://localhost:5000 Click Save Add the Equipment Field Details From Equipment Management, click Edit on the Equipment record Click Equipment Field Details Click Create New Name = \"weight\" SequenceId = 1 Check the box by OutputStreamTypeId FieldName = \"weight\" StringLength = 30 ReturnTypeResult = String Click Create Click Back to Equipment Add the Connection Details Click Connection Details Name = 1 Hostname = <ip address of scalehead> Port = 10001 TypeId = TcpIPCStream Click Submit Create a Workstation In LevridgeScale, click Application Configuration > Workstation Management Click Create New Enter a Workstation name, ex. \"Ws1\" Click Create Click Edit Under Equipment, mark the box next to the Scale Click Save Configure the Hardware Service with the Workstation name Navigate to the folder LevridgeScaleHouse\\Services\\HardwareInterface Open appsettings.json Enter the WorkstationName, example: \"WorkstationName\" : \"Ws1\" Restart Hardware service Open (Windows) Services Restart LevScaleHardwareService","title":"Rice Lake 1280"},{"location":"ScaleHeadHardwareSetup/#serial-connection","text":"Add the Equipment Information In Levridge Scale, click Application Configuration > Equipment Management Click Create New Set Is active = Yes Set Is scale = Yes Enter Equipment Name - this is the name that will show in the UI and on printed reports Equipment description - not used Location - not used Custom character = L Click Save, then edit for the next two fields to be visible Equipment response type ID = 4 Return response type ID = 5 Service URL = http://localhost:5000 Click Save Add the Equipment Field Details From Equipment Management, click Edit on the Equipment record Click Equipment Field Details Click Create New Name = \"weight\" SequenceId = 1 Check the box by OutputStreamTypeId FieldName = \"weight\" StringLength = 30 ReturnTypeResult = String Click Create Click Back to Equipment Add the Connection Details Click Connection Details Name = COM1 Speed = 9600 DataBits = 8 StopBits = 1 Parity = N FlowControl = 0 TypeId = SerialStream Click Submit Create a Workstation In LevridgeScale, click Application Configuration > Workstation Management Click Create New Enter a Workstation name, ex. \"Ws1\" Click Create Click Edit Under Equipment, mark the box next to the Scale Click Save Configure the Hardware Service with the Workstation name Navigate to the folder LevridgeScaleHouse\\Services\\HardwareInterface Open appsettings.json Enter the WorkstationName, example: \"WorkstationName\" : \"Ws1\" Restart Hardware service Open (Windows) Services Restart LevScaleHardwareService","title":"Serial Connection"},{"location":"ScaleImplementationActivities/","text":"Scale Implementation Activities Overview The following is an overview of the implementation activities necessary for getting started with scale. Full Implementation - Includes Integration to FinOps At the scale: 1. Install the scale application 2. Setup Azure service bus 3. Configure the Azure service bus to connect to FinOps - Setup event endpoint in FinOps 4. Configure the appsettings.json files for the scale application 5. Connect hardware to the application 6. Configure the scale application settings - Customer short list - Gross quantity settings - Printer settings - Equipment mgmt - Main settings page In FinOps Set up the event framework events for integrating to the scale Configure data that will be sent back-and-forth to the scale (requires input from client on how they want to use the scale) Products Carrier services Use in scale flag Net weight Units of measure Create estimated ticket flag Use TMS parameters Scale operators from the workers table Rolling stock Equipment types and container weights (if using estimated tickets) Grade factors Grade factor sequencing Growing seasons Hazardous materials Freight carriers Customers Customer split groups Customer operations and sites Units of measure Inventory sites Pits Bins Setup number sequences Setup scale parameters under AR > Setup > Ag > Ag parameters All scale tickets Agronomy scale Configure batch processes for generating orders from tickets (sales orders, transfer orders, etc.) Configure TMS (requires input from the client on how they want to configure) Delivery terms Shipping carriers Grower and company Carrier services TMS parameters under TMS > Setup > TMS parameters > General > Agronomy scale Default settings for grower and company carriers Rating and routing Synch initial data to the scale prior to running","title":"Scale Implementation Activities"},{"location":"ScaleImplementationActivities/#scale-implementation-activities","text":"","title":"Scale Implementation Activities"},{"location":"ScaleImplementationActivities/#overview","text":"The following is an overview of the implementation activities necessary for getting started with scale.","title":"Overview"},{"location":"ScaleImplementationActivities/#full-implementation-includes-integration-to-finops","text":"At the scale: 1. Install the scale application 2. Setup Azure service bus 3. Configure the Azure service bus to connect to FinOps - Setup event endpoint in FinOps 4. Configure the appsettings.json files for the scale application 5. Connect hardware to the application 6. Configure the scale application settings - Customer short list - Gross quantity settings - Printer settings - Equipment mgmt - Main settings page","title":"Full Implementation - Includes Integration to FinOps"},{"location":"ScaleImplementationActivities/#in-finops","text":"Set up the event framework events for integrating to the scale Configure data that will be sent back-and-forth to the scale (requires input from client on how they want to use the scale) Products Carrier services Use in scale flag Net weight Units of measure Create estimated ticket flag Use TMS parameters Scale operators from the workers table Rolling stock Equipment types and container weights (if using estimated tickets) Grade factors Grade factor sequencing Growing seasons Hazardous materials Freight carriers Customers Customer split groups Customer operations and sites Units of measure Inventory sites Pits Bins Setup number sequences Setup scale parameters under AR > Setup > Ag > Ag parameters All scale tickets Agronomy scale Configure batch processes for generating orders from tickets (sales orders, transfer orders, etc.) Configure TMS (requires input from the client on how they want to configure) Delivery terms Shipping carriers Grower and company Carrier services TMS parameters under TMS > Setup > TMS parameters > General > Agronomy scale Default settings for grower and company carriers Rating and routing Synch initial data to the scale prior to running","title":"In FinOps"},{"location":"ServiceBusConfiguration/","text":"ServiceBusConfiguration The ServiceBusConfiguration object is used to configure a connection to a service bus queue or topic. Example \"prodagsyncmasterdata\": { \"ConnectionString\": \"Endpoint=[Customer Servicebus Connection String]\", \"TopicName\": \"[Customer Servicebus Topic]\", \"SubscriptionName\": \"[Customer Servicebus Topic Subscription]\", \"MaxConcurrentCount\":10, \"PrefetchCount\": 5 }, Definition ConnectionString TopicName SubscriptionName MaxConcurrentCount The maximum number of messages that will be allowed to be processed simultaneously. PrefetchCount","title":"ServiceBusConfiguration"},{"location":"ServiceBusConfiguration/#servicebusconfiguration","text":"The ServiceBusConfiguration object is used to configure a connection to a service bus queue or topic.","title":"ServiceBusConfiguration"},{"location":"ServiceBusConfiguration/#example","text":"\"prodagsyncmasterdata\": { \"ConnectionString\": \"Endpoint=[Customer Servicebus Connection String]\", \"TopicName\": \"[Customer Servicebus Topic]\", \"SubscriptionName\": \"[Customer Servicebus Topic Subscription]\", \"MaxConcurrentCount\":10, \"PrefetchCount\": 5 },","title":"Example"},{"location":"ServiceBusConfiguration/#definition","text":"","title":"Definition"},{"location":"ServiceBusConfiguration/#connectionstring","text":"","title":"ConnectionString"},{"location":"ServiceBusConfiguration/#topicname","text":"","title":"TopicName"},{"location":"ServiceBusConfiguration/#subscriptionname","text":"","title":"SubscriptionName"},{"location":"ServiceBusConfiguration/#maxconcurrentcount","text":"The maximum number of messages that will be allowed to be processed simultaneously.","title":"MaxConcurrentCount"},{"location":"ServiceBusConfiguration/#prefetchcount","text":"","title":"PrefetchCount"},{"location":"SetupCostVariances/","text":"Setup Cost Variances Brief introduction of the module, component or feature being documented. This document explains ... How to Setup Cost Variances Close the page. Go to Cost Management > Ledger Integration Policies Setup > Posting. Click the Standard cost variance tab. In the Select field, select an option.","title":"Setup Cost Variances"},{"location":"SetupCostVariances/#setup-cost-variances","text":"Brief introduction of the module, component or feature being documented. This document explains ...","title":"Setup Cost Variances"},{"location":"SetupCostVariances/#how-to-setup-cost-variances","text":"Close the page. Go to Cost Management > Ledger Integration Policies Setup > Posting. Click the Standard cost variance tab. In the Select field, select an option.","title":"How to Setup Cost Variances"},{"location":"SetupStandardCost/","text":"Setup a Standard Cost Brief introduction of the module, component or feature being documented. This document explains ... How to Setup a Standard Cost Use the Quick Filter to find records. For example, filter on the Item number field with a value of 'harness'. Click Item price. Click New. In the list, mark the selected row. In the Version field, enter or select a value. In the list, select row 2. In the list, click the link in the selected row. In the Site field, type a value. In the Price field, enter a number. Click Save. Click the Active prices tab. Click the Pending prices tab. On the Action Pane, click Options. Close the page. Refresh the page. Click Item price. Click Activate pending price(s). Click the Active prices tab.","title":"Setup a Standard Cost"},{"location":"SetupStandardCost/#setup-a-standard-cost","text":"Brief introduction of the module, component or feature being documented. This document explains ...","title":"Setup a Standard Cost"},{"location":"SetupStandardCost/#how-to-setup-a-standard-cost","text":"Use the Quick Filter to find records. For example, filter on the Item number field with a value of 'harness'. Click Item price. Click New. In the list, mark the selected row. In the Version field, enter or select a value. In the list, select row 2. In the list, click the link in the selected row. In the Site field, type a value. In the Price field, enter a number. Click Save. Click the Active prices tab. Click the Pending prices tab. On the Action Pane, click Options. Close the page. Refresh the page. Click Item price. Click Activate pending price(s). Click the Active prices tab.","title":"How to Setup a Standard Cost"},{"location":"SourceConfig/","text":"SourceConfig Settings SourceConfig is an object in the appsettings.json file used by the Levridge Integration Framework to define the configuration for the Source of an integration interaction. Example \"SourceConfig\": { \"ServiceBusConfigName\": \"Levridge.Integration.Host.SuretyController\", \"ODataConfigName\": \"DynamicsCRM\", \"SystemName\": \"Surety\", \"Direction\": \"Source\" } Definition SourceConfig The node in the appsettings.json file does not actually need to be named \"SourceConfig\". You can use a command line parameter to specify the node name (section name) that contains the SourceConfig data. No matter the name, the source config section must contain the following attributes: - ServiceBusConfigName - ODataConfigName - CDSConfigName - SystemName - Direction ServiceBusConfigName The ServiceBusConfigName attribute contains a string that specifies the json object (configuration section) that holds the service bus configuration used by the Source of the Integration Interaction. This must point to a node that is a ServiceBusConfig json object ODataConfigName The ODataConfigName attribute contains a string that specifies the json object (configuration section) that holds the data configuration used by the Source of the Integration Interaction. This must point to a node that is a ODataConfig json object CDSConfigName The CDSConfigName attribute contains a string that specifies the json object (configuration section) that holds the CDS configuration used by the Source of the Integration Interaction. This must point to a node that is a CDSConfig json object SystemName The SystemName attribute holds a string value that represents the source system name. It must be one of the names recognized by the SystemName enumeration . Direction The direction is either Soruce or Target. This must be specified since the SourceConfig Node can have any name so the direction of the configuration may not be clear from the name.","title":"SourceConfig Settings"},{"location":"SourceConfig/#sourceconfig-settings","text":"SourceConfig is an object in the appsettings.json file used by the Levridge Integration Framework to define the configuration for the Source of an integration interaction.","title":"SourceConfig Settings"},{"location":"SourceConfig/#example","text":"\"SourceConfig\": { \"ServiceBusConfigName\": \"Levridge.Integration.Host.SuretyController\", \"ODataConfigName\": \"DynamicsCRM\", \"SystemName\": \"Surety\", \"Direction\": \"Source\" }","title":"Example"},{"location":"SourceConfig/#definition","text":"","title":"Definition"},{"location":"SourceConfig/#sourceconfig","text":"The node in the appsettings.json file does not actually need to be named \"SourceConfig\". You can use a command line parameter to specify the node name (section name) that contains the SourceConfig data. No matter the name, the source config section must contain the following attributes: - ServiceBusConfigName - ODataConfigName - CDSConfigName - SystemName - Direction","title":"SourceConfig"},{"location":"SourceConfig/#servicebusconfigname","text":"The ServiceBusConfigName attribute contains a string that specifies the json object (configuration section) that holds the service bus configuration used by the Source of the Integration Interaction. This must point to a node that is a ServiceBusConfig json object","title":"ServiceBusConfigName"},{"location":"SourceConfig/#odataconfigname","text":"The ODataConfigName attribute contains a string that specifies the json object (configuration section) that holds the data configuration used by the Source of the Integration Interaction. This must point to a node that is a ODataConfig json object","title":"ODataConfigName"},{"location":"SourceConfig/#cdsconfigname","text":"The CDSConfigName attribute contains a string that specifies the json object (configuration section) that holds the CDS configuration used by the Source of the Integration Interaction. This must point to a node that is a CDSConfig json object","title":"CDSConfigName"},{"location":"SourceConfig/#systemname","text":"The SystemName attribute holds a string value that represents the source system name. It must be one of the names recognized by the SystemName enumeration .","title":"SystemName"},{"location":"SourceConfig/#direction","text":"The direction is either Soruce or Target. This must be specified since the SourceConfig Node can have any name so the direction of the configuration may not be clear from the name.","title":"Direction"},{"location":"Surety/","text":"Surety","title":"Surety"},{"location":"Surety/#surety","text":"","title":"Surety"},{"location":"SystemName/","text":"SystemName SystemName is an enumerated value that is used in the Source and Target configurations to configure which integrations get loaded into the current integration framework instance. The valid values are: None : No integrations will be loaded DynamicsAX : The integrations for Dynamics 365 Finance and Operations will be loaded as either the Source or the Target depending on which config node it appears. DynamicsCRM : The integrations for Dynamics 365 Customer Engagement will be loaded as either the Source or the Target depending on which config node it appears. ScaleHouse : The integrations for the Levridge Scale Application will be loaded as either the Source or the Target depending on which config node it appears. OneWeigh : The integrations for OneWeigh will be loaded as either the Source or the Target depending on which config node it appears. AgSync : The integrations for Agsync will be loaded as either the Source or the Target depending on which config node it appears. Recommendation : The integrations for Recommendations will be loaded as the Source. This is not a valid value for the Target configuration. Field : The integrations for Fields will be loaded as the Source. This is not a valid value for the Target configuration. Surety : The integrations for Surety will be loaded as the Source. This is not a valid value for the Target configuration. Kahler : The integrations for Kahler will be loaded as either the Source or the Target depending on which config node it appears.","title":"SystemName"},{"location":"SystemName/#systemname","text":"SystemName is an enumerated value that is used in the Source and Target configurations to configure which integrations get loaded into the current integration framework instance. The valid values are: None : No integrations will be loaded DynamicsAX : The integrations for Dynamics 365 Finance and Operations will be loaded as either the Source or the Target depending on which config node it appears. DynamicsCRM : The integrations for Dynamics 365 Customer Engagement will be loaded as either the Source or the Target depending on which config node it appears. ScaleHouse : The integrations for the Levridge Scale Application will be loaded as either the Source or the Target depending on which config node it appears. OneWeigh : The integrations for OneWeigh will be loaded as either the Source or the Target depending on which config node it appears. AgSync : The integrations for Agsync will be loaded as either the Source or the Target depending on which config node it appears. Recommendation : The integrations for Recommendations will be loaded as the Source. This is not a valid value for the Target configuration. Field : The integrations for Fields will be loaded as the Source. This is not a valid value for the Target configuration. Surety : The integrations for Surety will be loaded as the Source. This is not a valid value for the Target configuration. Kahler : The integrations for Kahler will be loaded as either the Source or the Target depending on which config node it appears.","title":"SystemName"},{"location":"TargetConfig/","text":"Target Config Settings TargetConfig is an object in the appsettings.json file used by the Levridge Integration Framework to define the configuration for the Source of an integration interaction. Example \"TargetConfig\": { \"ODataConfigName\": \"DynamicsCRM\", \"CDSConfigName\": \"CDS\", \"SystemName\": \"DynamicsCRM\", \"Direction\": \"Source\" } Definition TargetConfig The node in the appsettings.json file does not actually need to be named \"TargetConfig\". You can use a command line parameter to specify the node name (section name) that contains the TargetConfig data. No matter the name, the source config section must contain the following attributes: - ODataConfigName - CDSConfigName - SystemName - Direction ODataConfigName The ODataConfigName attribute contains a string that specifies the json object (configuration section) that holds the data configuration used by the Source of the Integration Interaction. This must point to a node that is a ODataConfig json object CDSConfigName The CDSConfigName attribute contains a string that specifies the json object (configuration section) that holds the CDS configuration used by the Target of the Integration Interaction. This must point to a node that is a CDSConfig json object SystemName The SystemName attribute holds a string value that represents the source system name. It must be one of the names recognized by the SystemName enumeration . Direction The direction is either Soruce or Target. This must be specified since the TargetConfig Node can have any name so the direction of the configuration may not be clear from the name.","title":"Target Config Settings"},{"location":"TargetConfig/#target-config-settings","text":"TargetConfig is an object in the appsettings.json file used by the Levridge Integration Framework to define the configuration for the Source of an integration interaction.","title":"Target Config Settings"},{"location":"TargetConfig/#example","text":"\"TargetConfig\": { \"ODataConfigName\": \"DynamicsCRM\", \"CDSConfigName\": \"CDS\", \"SystemName\": \"DynamicsCRM\", \"Direction\": \"Source\" }","title":"Example"},{"location":"TargetConfig/#definition","text":"","title":"Definition"},{"location":"TargetConfig/#targetconfig","text":"The node in the appsettings.json file does not actually need to be named \"TargetConfig\". You can use a command line parameter to specify the node name (section name) that contains the TargetConfig data. No matter the name, the source config section must contain the following attributes: - ODataConfigName - CDSConfigName - SystemName - Direction","title":"TargetConfig"},{"location":"TargetConfig/#odataconfigname","text":"The ODataConfigName attribute contains a string that specifies the json object (configuration section) that holds the data configuration used by the Source of the Integration Interaction. This must point to a node that is a ODataConfig json object","title":"ODataConfigName"},{"location":"TargetConfig/#cdsconfigname","text":"The CDSConfigName attribute contains a string that specifies the json object (configuration section) that holds the CDS configuration used by the Target of the Integration Interaction. This must point to a node that is a CDSConfig json object","title":"CDSConfigName"},{"location":"TargetConfig/#systemname","text":"The SystemName attribute holds a string value that represents the source system name. It must be one of the names recognized by the SystemName enumeration .","title":"SystemName"},{"location":"TargetConfig/#direction","text":"The direction is either Soruce or Target. This must be specified since the TargetConfig Node can have any name so the direction of the configuration may not be clear from the name.","title":"Direction"},{"location":"Template/","text":"Introduction Brief introduction of the module, component or feature being documented. This document explains ... Overview Main Point 1 Sub Point 1.1","title":"Introduction"},{"location":"Template/#introduction","text":"Brief introduction of the module, component or feature being documented. This document explains ...","title":"Introduction"},{"location":"Template/#overview","text":"","title":"Overview"},{"location":"Template/#main-point-1","text":"","title":"Main Point 1"},{"location":"Template/#sub-point-11","text":"","title":"Sub Point 1.1"},{"location":"TradeAgreementsPriceGroup/","text":"Trade Agreements Price Group Brief introduction of the module, component or feature being documented. This document explains ... Trade Agreements Price Group Go to Inventory Management > Setup > Price/Discount > Customer Price/Discount Groups. Click New. In the Price Groups field, type a value. In the Name field, type a value. On the Action Pane, click Trade Agreements. Click Save. On the Action Pane, click Trade Agreements. Click View Sales Prices. Close the page. One the Action Pane, click Trade Agreements. Click View Trade Agreements (sales). Close the page. On the Action Pane, click Trade Agreements. Close the page. Go to Product Information Management > Products > Released Products. In the list, click the link in the select row. Close the page. Go to Accounts Receivable > Customers > All customers. In the list, find and select the desired record. In the list, click the link in the selected row. Click Edit. In the Price field, enter or select a value. In the list, select row 2. In the list, click the link in the selected row. Click Save. Close the page. Go to Inventory Management > Setup > Price/Discount > Customer Price/Discount Groups. In the list, find and select the desired record. On the Action Pane, click Trade Agreements. Click Create Trade Agreements. Click New. In the Name field, enter or select a value. In the list, select row 5. In the list, click the link in the selected row. Click Lines. In the list, mark the selected row. In the Party code type field, select an option. In the Account selection field, enter or select a value. In the list, select row 2. In the list, click the link in the selected row. In the Item relation field, enter or select a value. Close the page. In the Item relation field, type a value. In the Site field, type a value. In the Warehouse field, type a value. In the Amount in currency field, enter a number. Click Post. Click OK. Refresh the page. Close the page. On the Action Pane, click Trade Agreements. Click View sales prices. Close the page. Close the page. Go to Accounts receivable > Orders > All sales orders. In the list, find and select the desired record. Click New.","title":"Trade Agreements Price Group"},{"location":"TradeAgreementsPriceGroup/#trade-agreements-price-group","text":"Brief introduction of the module, component or feature being documented. This document explains ...","title":"Trade Agreements Price Group"},{"location":"TradeAgreementsPriceGroup/#trade-agreements-price-group_1","text":"Go to Inventory Management > Setup > Price/Discount > Customer Price/Discount Groups. Click New. In the Price Groups field, type a value. In the Name field, type a value. On the Action Pane, click Trade Agreements. Click Save. On the Action Pane, click Trade Agreements. Click View Sales Prices. Close the page. One the Action Pane, click Trade Agreements. Click View Trade Agreements (sales). Close the page. On the Action Pane, click Trade Agreements. Close the page. Go to Product Information Management > Products > Released Products. In the list, click the link in the select row. Close the page. Go to Accounts Receivable > Customers > All customers. In the list, find and select the desired record. In the list, click the link in the selected row. Click Edit. In the Price field, enter or select a value. In the list, select row 2. In the list, click the link in the selected row. Click Save. Close the page. Go to Inventory Management > Setup > Price/Discount > Customer Price/Discount Groups. In the list, find and select the desired record. On the Action Pane, click Trade Agreements. Click Create Trade Agreements. Click New. In the Name field, enter or select a value. In the list, select row 5. In the list, click the link in the selected row. Click Lines. In the list, mark the selected row. In the Party code type field, select an option. In the Account selection field, enter or select a value. In the list, select row 2. In the list, click the link in the selected row. In the Item relation field, enter or select a value. Close the page. In the Item relation field, type a value. In the Site field, type a value. In the Warehouse field, type a value. In the Amount in currency field, enter a number. Click Post. Click OK. Refresh the page. Close the page. On the Action Pane, click Trade Agreements. Click View sales prices. Close the page. Close the page. Go to Accounts receivable > Orders > All sales orders. In the list, find and select the desired record. Click New.","title":"Trade Agreements Price Group"},{"location":"Unit-Testing-Entity-Mappings/","text":"Introduction This library provides the base classes and utility classes to help you bild unit tests for the integration framework. It contains classes in the following categories: * Mapping Unit Test Helper Classes * Service Bus Helper and Mock classes * Integration Test Helper Classes Mapping Unit Test Helper Classes EntityMapBuilderExtensionsTestHelper The EntityMapBuilderExtensionsTestHelper class contains static methods you can use to obtain an EntityMapProvider<> for a particular scenario and methods to execute A2B and B2A synchronization using the EntityMapProvider<> obtained. To create a unit test that can test a mapping scenario, reference this library in your test project. Then create a unit test using the following steps: Call GetEntityMapProvider (Type entityMapBuilderType, string methodName) passing the source target template parameter types as defined on the mapping method along with the class that contains the mapping method and the name of the mapping method that you want to test. For example, to test the mapping for AxEntities.ScaleTicketGradeFactor and ScaleHouseEntities.ScaleTicketGradeFactor make the following call: var mapProvider = EntityMapBuilderExtensionsTestHelper.GetEntityMapProvider<AxEntities.ScaleTicketGradeFactor, ScaleHouseEntities.ScaleTicketGradeFactor>(typeof(AxToScaleEntityMapBuilderExtensions), \"MapAXScaleTicketGradeFactor_ScaleHouseScaleTicketGradeFactor\"); Instantiate and populate the entities you want to use for mapping. I prefer to use the [MemberData] attribute to provide multiple instances for testing. See this article for example on providing data for unit tests. Use the EntityMapProvider<> to perform the mapping by calling one of the transform methods on The EntityMapBuilderExtensionsTestHelper class. For example, to test the B2A mapping for the previous example use the following code: AXentity = EntityMapBuilderExtensionsTestHelper.TransformB2A(mapProvider, ScaleEntity, AXentity); Assert - check the mapped values In order to test the expected values you should have an instance of the target object with the expected values and then get the transformed object from the target entity and compare the two. You can pass in an object with the expected values using the MemberData method. Example: // Get target object var targetObject = AXentity.GetEntityAsDotNetType<AxEntities.ScaleTicketGradeFactor>(); // Assert Assert.NotNull(targetObject); Assert.Equal(expectedResult.GradeFactorId, targetObject.GradeFactorId); Assert.Equal(expectedResult.GradeFactorValue, targetObject.GradeFactorValue); Assert.Equal(expectedResult.TicketNumber, targetObject.TicketNumber); Assert.Equal(expectedResult.LineNumber, targetObject.LineNumber); Using MemberData to Provide Unit Test Data If you want to use the MemberData method for building data to use in your unit tests (you can refer to this article for an over view) here are some ideas. 1. Create Methods to Instantiate and Populate the Source and Expected Target Objects I would recommend creating a private static method on your test class that recieves parameters that can be used to initialize the class with the desired values. If you need several parameters you can put them in an object array or Key Value pair or even a dictionary. Example: private static AxEntities.ScaleTicketGradeFactor GetAxTicketGradeFactorObject(int line, string id, string value, string ticketNumber) { return new AxEntities.ScaleTicketGradeFactor() { LineNumber = line, GradeFactorId = id, GradeFactorValue = decimal.Parse(value), TicketNumber = ticketNumber }; } 2. Create Methods to Instantiate and Populate the Source and Target Entities Once you have a populated source object you can use it to instantiate and initialize a source Entity. Example: private static Entity GetAxTicketGradeFactorEntity(AxEntities.ScaleTicketGradeFactor scaleTicketGradeFactor = null) { var axScaleTicketGradeFactor = scaleTicketGradeFactor ?? new AxEntities.ScaleTicketGradeFactor(); Entity entity = DynamicsAxEntities.Instance.GetScaleTicketGradeFactorEntity(); entity.CreateAndPopulateEntityFields(typeof(JSONField<>), axScaleTicketGradeFactor); return entity; } As you can see, if an object is provided, it is used to initialize the Entity. Otherwise a default object is used. 3. Implement the MemberData Method As described in this article for an over view) the MemberData method must return IEnumerable<object[]> . You can craete a method that uses the methods described in the previous steps to create source & target entities along with a target expected object. Then you can define that method using the [MemberData] attribute. Here is an example MemberData method: public static IEnumerable<object[]> GetSynchronizeA2BGradeFactorData() { return new List<object[]> { new object[] { GetAxTicketGradeFactorEntity(GetAxTicketGradeFactorObject(1, \"A\", \"8\", \"0000169\")), GetScaleHouseGradeFactorEntity(), GetScaleHouseTicketGradeFactorObject(1, \"A\", \"8\", \"0000169\") }, new object[] { GetAxTicketGradeFactorEntity(GetAxTicketGradeFactorObject(2, \"AB\", \"0.3\", \"0000169\")), GetScaleHouseGradeFactorEntity(), GetScaleHouseTicketGradeFactorObject(2, \"AB\", \"0.3\", \"0000169\") }, new object[] { GetAxTicketGradeFactorEntity(GetAxTicketGradeFactorObject(3, \"AS\", \"31.3\", \"0000169\")), GetScaleHouseGradeFactorEntity(), GetScaleHouseTicketGradeFactorObject(3, \"AS\", \"31.3\", \"0000169\") } }; } Example Unit Test Here is an example unit test that tests Ticket Grade Factor entities from the ScaleHouse application to F&O. [Theory] [MemberData(nameof(GetSynchronizeB2AGradeFactorData))] public void MapScaleTicketGradeFactor_SynchronizeB2AGradeFactor_CorrectMapping( Entity AXentity, // AX ticket grade factor Entity ScaleEntity, // Scale house ticket grade factor AxEntities.ScaleTicketGradeFactor expectedResult) { // Arrange var mapProvider = EntityMapBuilderExtensionsTestHelper.GetEntityMapProvider<AxEntities.ScaleTicketGradeFactor, ScaleHouseEntities.ScaleTicketGradeFactor> (typeof(AxToScaleEntityMapBuilderExtensions), \"MapAXScaleTicketGradeFactor_ScaleHouseScaleTicketGradeFactor\"); //Act // Transform AXentity = EntityMapBuilderExtensionsTestHelper.TransformB2A(mapProvider, ScaleEntity, AXentity); // Get target object var targetObject = AXentity.GetEntityAsDotNetType<AxEntities.ScaleTicketGradeFactor>(); // Assert Assert.NotNull(targetObject); Assert.Equal(expectedResult.GradeFactorId, targetObject.GradeFactorId); Assert.Equal(expectedResult.GradeFactorValue, targetObject.GradeFactorValue); Assert.Equal(expectedResult.TicketNumber, targetObject.TicketNumber); Assert.Equal(expectedResult.LineNumber, targetObject.LineNumber); }","title":"Introduction"},{"location":"Unit-Testing-Entity-Mappings/#introduction","text":"This library provides the base classes and utility classes to help you bild unit tests for the integration framework. It contains classes in the following categories: * Mapping Unit Test Helper Classes * Service Bus Helper and Mock classes * Integration Test Helper Classes","title":"Introduction"},{"location":"Unit-Testing-Entity-Mappings/#mapping-unit-test-helper-classes","text":"","title":"Mapping Unit Test Helper Classes"},{"location":"Unit-Testing-Entity-Mappings/#entitymapbuilderextensionstesthelper","text":"The EntityMapBuilderExtensionsTestHelper class contains static methods you can use to obtain an EntityMapProvider<> for a particular scenario and methods to execute A2B and B2A synchronization using the EntityMapProvider<> obtained. To create a unit test that can test a mapping scenario, reference this library in your test project. Then create a unit test using the following steps: Call GetEntityMapProvider (Type entityMapBuilderType, string methodName) passing the source target template parameter types as defined on the mapping method along with the class that contains the mapping method and the name of the mapping method that you want to test. For example, to test the mapping for AxEntities.ScaleTicketGradeFactor and ScaleHouseEntities.ScaleTicketGradeFactor make the following call: var mapProvider = EntityMapBuilderExtensionsTestHelper.GetEntityMapProvider<AxEntities.ScaleTicketGradeFactor, ScaleHouseEntities.ScaleTicketGradeFactor>(typeof(AxToScaleEntityMapBuilderExtensions), \"MapAXScaleTicketGradeFactor_ScaleHouseScaleTicketGradeFactor\"); Instantiate and populate the entities you want to use for mapping. I prefer to use the [MemberData] attribute to provide multiple instances for testing. See this article for example on providing data for unit tests. Use the EntityMapProvider<> to perform the mapping by calling one of the transform methods on The EntityMapBuilderExtensionsTestHelper class. For example, to test the B2A mapping for the previous example use the following code: AXentity = EntityMapBuilderExtensionsTestHelper.TransformB2A(mapProvider, ScaleEntity, AXentity); Assert - check the mapped values In order to test the expected values you should have an instance of the target object with the expected values and then get the transformed object from the target entity and compare the two. You can pass in an object with the expected values using the MemberData method. Example: // Get target object var targetObject = AXentity.GetEntityAsDotNetType<AxEntities.ScaleTicketGradeFactor>(); // Assert Assert.NotNull(targetObject); Assert.Equal(expectedResult.GradeFactorId, targetObject.GradeFactorId); Assert.Equal(expectedResult.GradeFactorValue, targetObject.GradeFactorValue); Assert.Equal(expectedResult.TicketNumber, targetObject.TicketNumber); Assert.Equal(expectedResult.LineNumber, targetObject.LineNumber);","title":"EntityMapBuilderExtensionsTestHelper"},{"location":"Unit-Testing-Entity-Mappings/#using-memberdata-to-provide-unit-test-data","text":"If you want to use the MemberData method for building data to use in your unit tests (you can refer to this article for an over view) here are some ideas.","title":"Using MemberData to Provide Unit Test Data"},{"location":"Unit-Testing-Entity-Mappings/#1-create-methods-to-instantiate-and-populate-the-source-and-expected-target-objects","text":"I would recommend creating a private static method on your test class that recieves parameters that can be used to initialize the class with the desired values. If you need several parameters you can put them in an object array or Key Value pair or even a dictionary. Example: private static AxEntities.ScaleTicketGradeFactor GetAxTicketGradeFactorObject(int line, string id, string value, string ticketNumber) { return new AxEntities.ScaleTicketGradeFactor() { LineNumber = line, GradeFactorId = id, GradeFactorValue = decimal.Parse(value), TicketNumber = ticketNumber }; }","title":"1. Create Methods to Instantiate and Populate the Source and Expected Target Objects"},{"location":"Unit-Testing-Entity-Mappings/#2-create-methods-to-instantiate-and-populate-the-source-and-target-entities","text":"Once you have a populated source object you can use it to instantiate and initialize a source Entity. Example: private static Entity GetAxTicketGradeFactorEntity(AxEntities.ScaleTicketGradeFactor scaleTicketGradeFactor = null) { var axScaleTicketGradeFactor = scaleTicketGradeFactor ?? new AxEntities.ScaleTicketGradeFactor(); Entity entity = DynamicsAxEntities.Instance.GetScaleTicketGradeFactorEntity(); entity.CreateAndPopulateEntityFields(typeof(JSONField<>), axScaleTicketGradeFactor); return entity; } As you can see, if an object is provided, it is used to initialize the Entity. Otherwise a default object is used.","title":"2. Create Methods to Instantiate and Populate the Source and Target Entities"},{"location":"Unit-Testing-Entity-Mappings/#3-implement-the-memberdata-method","text":"As described in this article for an over view) the MemberData method must return IEnumerable<object[]> . You can craete a method that uses the methods described in the previous steps to create source & target entities along with a target expected object. Then you can define that method using the [MemberData] attribute. Here is an example MemberData method: public static IEnumerable<object[]> GetSynchronizeA2BGradeFactorData() { return new List<object[]> { new object[] { GetAxTicketGradeFactorEntity(GetAxTicketGradeFactorObject(1, \"A\", \"8\", \"0000169\")), GetScaleHouseGradeFactorEntity(), GetScaleHouseTicketGradeFactorObject(1, \"A\", \"8\", \"0000169\") }, new object[] { GetAxTicketGradeFactorEntity(GetAxTicketGradeFactorObject(2, \"AB\", \"0.3\", \"0000169\")), GetScaleHouseGradeFactorEntity(), GetScaleHouseTicketGradeFactorObject(2, \"AB\", \"0.3\", \"0000169\") }, new object[] { GetAxTicketGradeFactorEntity(GetAxTicketGradeFactorObject(3, \"AS\", \"31.3\", \"0000169\")), GetScaleHouseGradeFactorEntity(), GetScaleHouseTicketGradeFactorObject(3, \"AS\", \"31.3\", \"0000169\") } }; }","title":"3. Implement the MemberData Method"},{"location":"Unit-Testing-Entity-Mappings/#example-unit-test","text":"Here is an example unit test that tests Ticket Grade Factor entities from the ScaleHouse application to F&O. [Theory] [MemberData(nameof(GetSynchronizeB2AGradeFactorData))] public void MapScaleTicketGradeFactor_SynchronizeB2AGradeFactor_CorrectMapping( Entity AXentity, // AX ticket grade factor Entity ScaleEntity, // Scale house ticket grade factor AxEntities.ScaleTicketGradeFactor expectedResult) { // Arrange var mapProvider = EntityMapBuilderExtensionsTestHelper.GetEntityMapProvider<AxEntities.ScaleTicketGradeFactor, ScaleHouseEntities.ScaleTicketGradeFactor> (typeof(AxToScaleEntityMapBuilderExtensions), \"MapAXScaleTicketGradeFactor_ScaleHouseScaleTicketGradeFactor\"); //Act // Transform AXentity = EntityMapBuilderExtensionsTestHelper.TransformB2A(mapProvider, ScaleEntity, AXentity); // Get target object var targetObject = AXentity.GetEntityAsDotNetType<AxEntities.ScaleTicketGradeFactor>(); // Assert Assert.NotNull(targetObject); Assert.Equal(expectedResult.GradeFactorId, targetObject.GradeFactorId); Assert.Equal(expectedResult.GradeFactorValue, targetObject.GradeFactorValue); Assert.Equal(expectedResult.TicketNumber, targetObject.TicketNumber); Assert.Equal(expectedResult.LineNumber, targetObject.LineNumber); }","title":"Example Unit Test"},{"location":"UploadStandardCostPricing/","text":"Upload Standard Cost Pricing Overview INSERT How to Upload Standard Cost Pricing Go to Cost Management > Predetermined cost policies setup > Costing Versions. In the list, find and select the desired record. Click Price Click Item Price. Click the Active Prices tab. Click the Pending Prices tab. Click New. In the list, mark the selected row. In the item number field, type a value. In the Site field, type a value. In the Price field, enter a number. Click Save. Click Open in Microsoft Office. Click Pending item prices (100). Click Download. Click Open in Microsoft Office. Click Pending Item Prices. Click Download. Refresh the page. In the list, mark or unmark all rows. Click Activate pending price(s). Click the Active prices tab. Close the page. Close the page. Go to Product Information Management > Products > Released Products. Use the Quick Filter to find records. For example, filter on the item number field with a value of 'harness'. On the Action Pane, click Manage Inventory. On the Action pane, click Manage Costs. Click Item price. In the list, find and select the desired record.","title":"Upload Standard Cost Pricing"},{"location":"UploadStandardCostPricing/#upload-standard-cost-pricing","text":"","title":"Upload Standard Cost Pricing"},{"location":"UploadStandardCostPricing/#overview","text":"INSERT","title":"Overview"},{"location":"UploadStandardCostPricing/#how-to-upload-standard-cost-pricing","text":"Go to Cost Management > Predetermined cost policies setup > Costing Versions. In the list, find and select the desired record. Click Price Click Item Price. Click the Active Prices tab. Click the Pending Prices tab. Click New. In the list, mark the selected row. In the item number field, type a value. In the Site field, type a value. In the Price field, enter a number. Click Save. Click Open in Microsoft Office. Click Pending item prices (100). Click Download. Click Open in Microsoft Office. Click Pending Item Prices. Click Download. Refresh the page. In the list, mark or unmark all rows. Click Activate pending price(s). Click the Active prices tab. Close the page. Close the page. Go to Product Information Management > Products > Released Products. Use the Quick Filter to find records. For example, filter on the item number field with a value of 'harness'. On the Action Pane, click Manage Inventory. On the Action pane, click Manage Costs. Click Item price. In the list, find and select the desired record.","title":"How to Upload Standard Cost Pricing"},{"location":"UuidCompositeRequest/","text":"UuidCompositeRequest The UuidCompositeRequest object is used by the AgsyncUUID controller to make a request for UUID values based on SyncId values passed in the request. JSON Defintion JSON Example { \"accountId\": \"ABC\", \"accountName\": \"Levridge - Barnesville\", \"growerId\": \"CUS-100442\", \"growerName\": \"Captain Cook Farms\", \"farmId\": \"COP100263\", \"farmeName\": \"Captain Cook Farms\", \"fieldId\": \"CST-400312\", \"fieldName\": \"CC Veggie Field NE\" } JSON Schema { \"$schema\": \"http://json-schema.org/draft-07/schema\", \"$id\": \"http://example.com/example.json\", \"type\": \"object\", \"readOnly\": false, \"writeOnly\": false, \"minProperties\": 0, \"title\": \"UuidCompositeRequest\", \"description\": \"This schema comprises the entire JSON document for a Composite Request\", \"additionalProperties\": true, \"required\": [ \"accountId\" ], \"properties\": { \"accountId\": { \"$id\": \"#/properties/accountId\", \"type\": \"string\", \"readOnly\": false, \"writeOnly\": false, \"minLength\": 0, \"title\": \"The AccountId Schema\", \"description\": \"The Sync Id for the Account (FinOps Branch)\", \"default\": \"\", \"examples\": [ \"SCG1\" ] }, \"accountName\": { \"$id\": \"#/properties/accountName\", \"type\": \"string\", \"readOnly\": false, \"writeOnly\": false, \"minLength\": 0, \"title\": \"The AccountName Schema\", \"description\": \"This is an optional name for the account\", \"default\": \"\", \"examples\": [ \"Alexandria\" ] }, \"growerId\": { \"$id\": \"#/properties/growerId\", \"type\": \"string\", \"readOnly\": false, \"writeOnly\": false, \"minLength\": 0, \"title\": \"The GrowerId Schema\", \"description\": \"The Sync Id for the Grower (FinOps Customer)\", \"default\": \"\", \"examples\": [ \"CUS-100347\" ] }, \"growerName\": { \"$id\": \"#/properties/growerName\", \"type\": \"string\", \"readOnly\": false, \"writeOnly\": false, \"minLength\": 0, \"title\": \"The GrowerName Schema\", \"description\": \"This is an optional name for the Grower\", \"default\": \"\", \"examples\": [ \"Bjorklund Land & Cattle\" ] }, \"farmId\": { \"$id\": \"#/properties/farmId\", \"type\": \"string\", \"readOnly\": false, \"writeOnly\": false, \"minLength\": 0, \"title\": \"The FarmId Schema\", \"description\": \"The Sync Id for the Farm (FinOps Customer Operation)\", \"default\": \"\", \"examples\": [ \"COP100208\" ] }, \"farmeName\": { \"$id\": \"#/properties/farmeName\", \"type\": \"string\", \"readOnly\": false, \"writeOnly\": false, \"minLength\": 0, \"title\": \"The FarmeName Schema\", \"description\": \"This is an optional name for the Farm\", \"default\": \"\", \"examples\": [ \"Bjorklund Land & Cattle\" ] }, \"fieldId\": { \"$id\": \"#/properties/fieldId\", \"type\": \"string\", \"readOnly\": false, \"writeOnly\": false, \"minLength\": 0, \"title\": \"The FieldId Schema\", \"description\": \"The Sync Id for the Field (FinOps Customer Site)\", \"default\": \"\", \"examples\": [ \"CST-000026\" ] }, \"fieldName\": { \"$id\": \"#/properties/fieldName\", \"type\": \"string\", \"readOnly\": false, \"writeOnly\": false, \"minLength\": 0, \"title\": \"The FieldName Schema\", \"description\": \"This is an optional name for the Field\", \"default\": \"\", \"examples\": [ \"NW 40\" ] } } }","title":"UuidCompositeRequest"},{"location":"UuidCompositeRequest/#uuidcompositerequest","text":"The UuidCompositeRequest object is used by the AgsyncUUID controller to make a request for UUID values based on SyncId values passed in the request.","title":"UuidCompositeRequest"},{"location":"UuidCompositeRequest/#json-defintion","text":"","title":"JSON Defintion"},{"location":"UuidCompositeRequest/#json-example","text":"{ \"accountId\": \"ABC\", \"accountName\": \"Levridge - Barnesville\", \"growerId\": \"CUS-100442\", \"growerName\": \"Captain Cook Farms\", \"farmId\": \"COP100263\", \"farmeName\": \"Captain Cook Farms\", \"fieldId\": \"CST-400312\", \"fieldName\": \"CC Veggie Field NE\" }","title":"JSON Example"},{"location":"UuidCompositeRequest/#json-schema","text":"{ \"$schema\": \"http://json-schema.org/draft-07/schema\", \"$id\": \"http://example.com/example.json\", \"type\": \"object\", \"readOnly\": false, \"writeOnly\": false, \"minProperties\": 0, \"title\": \"UuidCompositeRequest\", \"description\": \"This schema comprises the entire JSON document for a Composite Request\", \"additionalProperties\": true, \"required\": [ \"accountId\" ], \"properties\": { \"accountId\": { \"$id\": \"#/properties/accountId\", \"type\": \"string\", \"readOnly\": false, \"writeOnly\": false, \"minLength\": 0, \"title\": \"The AccountId Schema\", \"description\": \"The Sync Id for the Account (FinOps Branch)\", \"default\": \"\", \"examples\": [ \"SCG1\" ] }, \"accountName\": { \"$id\": \"#/properties/accountName\", \"type\": \"string\", \"readOnly\": false, \"writeOnly\": false, \"minLength\": 0, \"title\": \"The AccountName Schema\", \"description\": \"This is an optional name for the account\", \"default\": \"\", \"examples\": [ \"Alexandria\" ] }, \"growerId\": { \"$id\": \"#/properties/growerId\", \"type\": \"string\", \"readOnly\": false, \"writeOnly\": false, \"minLength\": 0, \"title\": \"The GrowerId Schema\", \"description\": \"The Sync Id for the Grower (FinOps Customer)\", \"default\": \"\", \"examples\": [ \"CUS-100347\" ] }, \"growerName\": { \"$id\": \"#/properties/growerName\", \"type\": \"string\", \"readOnly\": false, \"writeOnly\": false, \"minLength\": 0, \"title\": \"The GrowerName Schema\", \"description\": \"This is an optional name for the Grower\", \"default\": \"\", \"examples\": [ \"Bjorklund Land & Cattle\" ] }, \"farmId\": { \"$id\": \"#/properties/farmId\", \"type\": \"string\", \"readOnly\": false, \"writeOnly\": false, \"minLength\": 0, \"title\": \"The FarmId Schema\", \"description\": \"The Sync Id for the Farm (FinOps Customer Operation)\", \"default\": \"\", \"examples\": [ \"COP100208\" ] }, \"farmeName\": { \"$id\": \"#/properties/farmeName\", \"type\": \"string\", \"readOnly\": false, \"writeOnly\": false, \"minLength\": 0, \"title\": \"The FarmeName Schema\", \"description\": \"This is an optional name for the Farm\", \"default\": \"\", \"examples\": [ \"Bjorklund Land & Cattle\" ] }, \"fieldId\": { \"$id\": \"#/properties/fieldId\", \"type\": \"string\", \"readOnly\": false, \"writeOnly\": false, \"minLength\": 0, \"title\": \"The FieldId Schema\", \"description\": \"The Sync Id for the Field (FinOps Customer Site)\", \"default\": \"\", \"examples\": [ \"CST-000026\" ] }, \"fieldName\": { \"$id\": \"#/properties/fieldName\", \"type\": \"string\", \"readOnly\": false, \"writeOnly\": false, \"minLength\": 0, \"title\": \"The FieldName Schema\", \"description\": \"This is an optional name for the Field\", \"default\": \"\", \"examples\": [ \"NW 40\" ] } } }","title":"JSON Schema"},{"location":"UuidCompositeResponse/","text":"UuidCompositResponse The UuidCompositResponse object is used by the AgsyncUUID controller to return UUID values based on SyncId values passed in the request. JSON Defintion JSON Example { \"accountId\": \"BAR\", \"accountUuid\": \"336a8049-853c-4992-8382-e77e8868e208\", \"accountGuid\": \"000ad538-0000-0000-0000-000000000000\", \"accountName\": null, \"growerId\": \"CUS-100442\", \"growerUuid\": \"63ffb30a-aaf6-4cbe-83b7-95765aa2c5e4\", \"growerGuid\": \"000ceb06-0000-0000-0000-000000000000\", \"growerName\": null, \"farmId\": \"COP100263\", \"farmUuid\": \"d4334cd1-1a1f-4484-94d6-246645e07196\", \"farmGuid\": \"000ceb07-0000-0000-0000-000000000000\", \"farmName\": null, \"fieldId\": \"CST-400312\", \"fieldUuid\": \"9e0f0647-c21a-495a-92a2-9d0aaaf2bc8f\", \"fieldGuid\": \"001a5c8a-0000-0000-0000-000000000000\", \"fieldName\": null } JSON Schema { \"$schema\": \"http://json-schema.org/draft-07/schema\", \"$id\": \"http://example.com/example.json\", \"type\": \"object\", \"readOnly\": false, \"writeOnly\": false, \"minProperties\": 0, \"title\": \"The UuidCompositeResponse Schema\", \"description\": \"The root schema comprises the entire JSON document for Composite Response\", \"additionalProperties\": true, \"required\": [], \"properties\": { \"accountId\": { \"$id\": \"#/properties/accountId\", \"type\": \"string\", \"readOnly\": false, \"writeOnly\": false, \"minLength\": 0, \"title\": \"The AccountId Schema\", \"description\": \"The Sync Id for the Account (FinOps Branch)\", \"default\": \"\", \"examples\": [ \"SCG1\" ] }, \"accountUuid\": { \"$id\": \"#/properties/accountUuid\", \"type\": \"string\", \"readOnly\": false, \"writeOnly\": false, \"minLength\": 0, \"title\": \"The AccountUuid Schema\", \"description\": \"The UUID for the Account (FinOps Branch)\", \"default\": \"\", \"examples\": [ \"e0485a5d-f6c1-4a0e-824a-1abffbcfaa9\" ] }, \"accountGuid\": { \"$id\": \"#/properties/accountGuid\", \"type\": \"string\", \"readOnly\": false, \"writeOnly\": false, \"minLength\": 0, \"title\": \"The AccountGuid Schema\", \"description\": \"The GUID for the Account (FinOps Branch)\", \"default\": \"\", \"examples\": [ \"000c33a4-0000-0000-0000-000000000000\" ] }, \"accountName\": { \"$id\": \"#/properties/accountName\", \"type\": \"string\", \"readOnly\": false, \"writeOnly\": false, \"minLength\": 0, \"title\": \"The AccountName Schema\", \"description\": \"This is an optional name for the account\", \"default\": \"\", \"examples\": [ \"Alexandria\" ] }, \"growerId\": { \"$id\": \"#/properties/growerId\", \"type\": \"string\", \"readOnly\": false, \"writeOnly\": false, \"minLength\": 0, \"title\": \"The GrowerId Schema\", \"description\": \"The Sync Id for the Grower (FinOps Customer)\", \"default\": \"\", \"examples\": [ \"CUS-100347\" ] }, \"growerUuid\": { \"$id\": \"#/properties/growerUuid\", \"type\": \"string\", \"readOnly\": false, \"writeOnly\": false, \"minLength\": 0, \"title\": \"The GrowerUuid Schema\", \"description\": \"The UUID for the Grower (FinOps Customer)\", \"default\": \"\", \"examples\": [ \"ad5ee735-c783-45d3-9b7a-5e7b089b1fbf\" ] }, \"growerGuid\": { \"$id\": \"#/properties/growerGuid\", \"type\": \"string\", \"readOnly\": false, \"writeOnly\": false, \"minLength\": 0, \"title\": \"The GrowerGuid Schema\", \"description\": \"The GUID for the Grower (FinOps Customer)\", \"default\": \"\", \"examples\": [ \"000cd991-0000-0000-0000-000000000000\" ] }, \"growerName\": { \"$id\": \"#/properties/growerName\", \"type\": \"string\", \"readOnly\": false, \"writeOnly\": false, \"minLength\": 0, \"title\": \"The GrowerName Schema\", \"description\": \"This is an optional name for the grower\", \"default\": \"\", \"examples\": [ \"Bjorklund Land & Cattle\" ] }, \"farmId\": { \"$id\": \"#/properties/farmId\", \"type\": \"string\", \"readOnly\": false, \"writeOnly\": false, \"minLength\": 0, \"title\": \"The FarmId Schema\", \"description\": \"The Sync Id for the Farm (FinOps Customer Operation)\", \"default\": \"\", \"examples\": [ \"COP100208\" ] }, \"farmUuid\": { \"$id\": \"#/properties/farmUuid\", \"type\": \"string\", \"readOnly\": false, \"writeOnly\": false, \"minLength\": 0, \"title\": \"The FarmUuid Schema\", \"description\": \"The UUID for the Grower (FinOps Customer Operation)\", \"default\": \"\", \"examples\": [ \"1f7b3ecc-e2eb-4724-b9ec-f0b756d21566\" ] }, \"farmGuid\": { \"$id\": \"#/properties/farmGuid\", \"type\": \"string\", \"readOnly\": false, \"writeOnly\": false, \"minLength\": 0, \"title\": \"The FarmGuid Schema\", \"description\": \"The GUID for the Farm (FinOps Customer Operation)\", \"default\": \"\", \"examples\": [ \"000cd992-0000-0000-0000-000000000000\" ] }, \"farmeName\": { \"$id\": \"#/properties/farmeName\", \"type\": \"string\", \"readOnly\": false, \"writeOnly\": false, \"minLength\": 0, \"title\": \"The FarmeName Schema\", \"description\": \"This is an optional name for the farm\", \"default\": \"\", \"examples\": [ \"Bjorklund Land & Cattle\" ] }, \"fieldId\": { \"$id\": \"#/properties/fieldId\", \"type\": \"string\", \"readOnly\": false, \"writeOnly\": false, \"minLength\": 0, \"title\": \"The FieldId Schema\", \"description\": \"The Sync Id for the Field (FinOps Customer Site)\", \"default\": \"\", \"examples\": [ \"CST-000026\" ] }, \"fieldUuid\": { \"$id\": \"#/properties/fieldUuid\", \"type\": \"string\", \"readOnly\": false, \"writeOnly\": false, \"minLength\": 0, \"title\": \"The FieldUuid Schema\", \"description\": \"The UUID for the Field (FinOps Customer Site)\", \"default\": \"\", \"examples\": [ \"2ac15a5a-2c8a-4f65-b5fe-3706637937ec\" ] }, \"fieldGuid\": { \"$id\": \"#/properties/fieldGuid\", \"type\": \"string\", \"readOnly\": false, \"writeOnly\": false, \"minLength\": 0, \"title\": \"The FieldGuid Schema\", \"description\": \"The GUID for the Field (FinOps Customer Site)\", \"default\": \"\", \"examples\": [ \"001a4769-0000-0000-0000-000000000000\" ] }, \"fieldName\": { \"$id\": \"#/properties/fieldName\", \"type\": \"string\", \"readOnly\": false, \"writeOnly\": false, \"minLength\": 0, \"title\": \"The FieldName Schema\", \"description\": \"This is an optional name for the field\", \"default\": \"\", \"examples\": [ \"NW 40\" ] } } }","title":"UuidCompositResponse"},{"location":"UuidCompositeResponse/#uuidcompositresponse","text":"The UuidCompositResponse object is used by the AgsyncUUID controller to return UUID values based on SyncId values passed in the request.","title":"UuidCompositResponse"},{"location":"UuidCompositeResponse/#json-defintion","text":"","title":"JSON Defintion"},{"location":"UuidCompositeResponse/#json-example","text":"{ \"accountId\": \"BAR\", \"accountUuid\": \"336a8049-853c-4992-8382-e77e8868e208\", \"accountGuid\": \"000ad538-0000-0000-0000-000000000000\", \"accountName\": null, \"growerId\": \"CUS-100442\", \"growerUuid\": \"63ffb30a-aaf6-4cbe-83b7-95765aa2c5e4\", \"growerGuid\": \"000ceb06-0000-0000-0000-000000000000\", \"growerName\": null, \"farmId\": \"COP100263\", \"farmUuid\": \"d4334cd1-1a1f-4484-94d6-246645e07196\", \"farmGuid\": \"000ceb07-0000-0000-0000-000000000000\", \"farmName\": null, \"fieldId\": \"CST-400312\", \"fieldUuid\": \"9e0f0647-c21a-495a-92a2-9d0aaaf2bc8f\", \"fieldGuid\": \"001a5c8a-0000-0000-0000-000000000000\", \"fieldName\": null }","title":"JSON Example"},{"location":"UuidCompositeResponse/#json-schema","text":"{ \"$schema\": \"http://json-schema.org/draft-07/schema\", \"$id\": \"http://example.com/example.json\", \"type\": \"object\", \"readOnly\": false, \"writeOnly\": false, \"minProperties\": 0, \"title\": \"The UuidCompositeResponse Schema\", \"description\": \"The root schema comprises the entire JSON document for Composite Response\", \"additionalProperties\": true, \"required\": [], \"properties\": { \"accountId\": { \"$id\": \"#/properties/accountId\", \"type\": \"string\", \"readOnly\": false, \"writeOnly\": false, \"minLength\": 0, \"title\": \"The AccountId Schema\", \"description\": \"The Sync Id for the Account (FinOps Branch)\", \"default\": \"\", \"examples\": [ \"SCG1\" ] }, \"accountUuid\": { \"$id\": \"#/properties/accountUuid\", \"type\": \"string\", \"readOnly\": false, \"writeOnly\": false, \"minLength\": 0, \"title\": \"The AccountUuid Schema\", \"description\": \"The UUID for the Account (FinOps Branch)\", \"default\": \"\", \"examples\": [ \"e0485a5d-f6c1-4a0e-824a-1abffbcfaa9\" ] }, \"accountGuid\": { \"$id\": \"#/properties/accountGuid\", \"type\": \"string\", \"readOnly\": false, \"writeOnly\": false, \"minLength\": 0, \"title\": \"The AccountGuid Schema\", \"description\": \"The GUID for the Account (FinOps Branch)\", \"default\": \"\", \"examples\": [ \"000c33a4-0000-0000-0000-000000000000\" ] }, \"accountName\": { \"$id\": \"#/properties/accountName\", \"type\": \"string\", \"readOnly\": false, \"writeOnly\": false, \"minLength\": 0, \"title\": \"The AccountName Schema\", \"description\": \"This is an optional name for the account\", \"default\": \"\", \"examples\": [ \"Alexandria\" ] }, \"growerId\": { \"$id\": \"#/properties/growerId\", \"type\": \"string\", \"readOnly\": false, \"writeOnly\": false, \"minLength\": 0, \"title\": \"The GrowerId Schema\", \"description\": \"The Sync Id for the Grower (FinOps Customer)\", \"default\": \"\", \"examples\": [ \"CUS-100347\" ] }, \"growerUuid\": { \"$id\": \"#/properties/growerUuid\", \"type\": \"string\", \"readOnly\": false, \"writeOnly\": false, \"minLength\": 0, \"title\": \"The GrowerUuid Schema\", \"description\": \"The UUID for the Grower (FinOps Customer)\", \"default\": \"\", \"examples\": [ \"ad5ee735-c783-45d3-9b7a-5e7b089b1fbf\" ] }, \"growerGuid\": { \"$id\": \"#/properties/growerGuid\", \"type\": \"string\", \"readOnly\": false, \"writeOnly\": false, \"minLength\": 0, \"title\": \"The GrowerGuid Schema\", \"description\": \"The GUID for the Grower (FinOps Customer)\", \"default\": \"\", \"examples\": [ \"000cd991-0000-0000-0000-000000000000\" ] }, \"growerName\": { \"$id\": \"#/properties/growerName\", \"type\": \"string\", \"readOnly\": false, \"writeOnly\": false, \"minLength\": 0, \"title\": \"The GrowerName Schema\", \"description\": \"This is an optional name for the grower\", \"default\": \"\", \"examples\": [ \"Bjorklund Land & Cattle\" ] }, \"farmId\": { \"$id\": \"#/properties/farmId\", \"type\": \"string\", \"readOnly\": false, \"writeOnly\": false, \"minLength\": 0, \"title\": \"The FarmId Schema\", \"description\": \"The Sync Id for the Farm (FinOps Customer Operation)\", \"default\": \"\", \"examples\": [ \"COP100208\" ] }, \"farmUuid\": { \"$id\": \"#/properties/farmUuid\", \"type\": \"string\", \"readOnly\": false, \"writeOnly\": false, \"minLength\": 0, \"title\": \"The FarmUuid Schema\", \"description\": \"The UUID for the Grower (FinOps Customer Operation)\", \"default\": \"\", \"examples\": [ \"1f7b3ecc-e2eb-4724-b9ec-f0b756d21566\" ] }, \"farmGuid\": { \"$id\": \"#/properties/farmGuid\", \"type\": \"string\", \"readOnly\": false, \"writeOnly\": false, \"minLength\": 0, \"title\": \"The FarmGuid Schema\", \"description\": \"The GUID for the Farm (FinOps Customer Operation)\", \"default\": \"\", \"examples\": [ \"000cd992-0000-0000-0000-000000000000\" ] }, \"farmeName\": { \"$id\": \"#/properties/farmeName\", \"type\": \"string\", \"readOnly\": false, \"writeOnly\": false, \"minLength\": 0, \"title\": \"The FarmeName Schema\", \"description\": \"This is an optional name for the farm\", \"default\": \"\", \"examples\": [ \"Bjorklund Land & Cattle\" ] }, \"fieldId\": { \"$id\": \"#/properties/fieldId\", \"type\": \"string\", \"readOnly\": false, \"writeOnly\": false, \"minLength\": 0, \"title\": \"The FieldId Schema\", \"description\": \"The Sync Id for the Field (FinOps Customer Site)\", \"default\": \"\", \"examples\": [ \"CST-000026\" ] }, \"fieldUuid\": { \"$id\": \"#/properties/fieldUuid\", \"type\": \"string\", \"readOnly\": false, \"writeOnly\": false, \"minLength\": 0, \"title\": \"The FieldUuid Schema\", \"description\": \"The UUID for the Field (FinOps Customer Site)\", \"default\": \"\", \"examples\": [ \"2ac15a5a-2c8a-4f65-b5fe-3706637937ec\" ] }, \"fieldGuid\": { \"$id\": \"#/properties/fieldGuid\", \"type\": \"string\", \"readOnly\": false, \"writeOnly\": false, \"minLength\": 0, \"title\": \"The FieldGuid Schema\", \"description\": \"The GUID for the Field (FinOps Customer Site)\", \"default\": \"\", \"examples\": [ \"001a4769-0000-0000-0000-000000000000\" ] }, \"fieldName\": { \"$id\": \"#/properties/fieldName\", \"type\": \"string\", \"readOnly\": false, \"writeOnly\": false, \"minLength\": 0, \"title\": \"The FieldName Schema\", \"description\": \"This is an optional name for the field\", \"default\": \"\", \"examples\": [ \"NW 40\" ] } } }","title":"JSON Schema"},{"location":"about/","text":"","title":"About"},{"location":"agsyncConfigObject/","text":"agsync Settings agsync is an object in the appsettings.json file used by the Levridge Integration Framework to define the configuration for the integration of Agsync Workorders to FinOps. Example \"agsync\": { \"MustUseWktProcessor\": true, \"ConnectionString\": \"Endpoint=[Customer Servicebus Connection String]\", \"TopicName\": \"[Customer Servicebus Topic]\", \"SubscriptionName\": \"[Customer Servicebus Topic Subscription]\", \"PrefetchCount\": 5, \"RequiresSession\": [true/false], \"RedirectUri\": \"[Agsync Integration Base URL]/api/AgsyncAuth\", \"TokenUrl\": \"https://auth.agsync.com/core/connect/token\", \"AuthorizeUrl\": \"https://auth.agsync.com/core/connect/authorize\", \"baseUri\": \"https://fields.agsync.com/api/\", \"rejectUri\": \"https://orders.agsync.com/api/orders/\", \"ClientId\": \"[Agsync assigned client ID]\", \"ClientPass\": \"[Agsync assigned client password]\", \"VaultURL\": \"[Integration Framework Key Vault URL]\", \"AgSyncTokenKey\": \"[Vault Key Used for Access Token]\" \"WktUrl\": \"[Agsync Integration Base URL]/api/WktProcessor\", \"ProcessStatuses\": \"Plan,Released,Canceled,Scheduled,Rejected,Completed,Accepted\", \"MustUseCustomerSite\": true } Definition MustUseWktProcessor The MustUseWktProcessor attribute contains a bollean that specifies whether or not to process the WKT sent in the workorder from Agsync to FinOps. ConnectionString TopicName SubscriptionName PrefetchCount RequiresSession RedirectUri TokenUrl AuthorizeUrl baseUri rejectUri ClientId ClientPass VaultURL AgSyncTokenKey ProcessStatuses Not Required Place a comma delimited list of statuses that should be processed by the controller. It is important to make sure the capitalization is correct (Title Case) because it does an exact match. Deafult: \"Planned,Released,Canceled,Scheduled,Rejected,Completed,Accepted\" MustUseCustomerSite Not Required Set this value to true to require a valid customer site on work orders. Set this value to false to ignore the customer site on work orders. Default: true","title":"agsync Settings"},{"location":"agsyncConfigObject/#agsync-settings","text":"agsync is an object in the appsettings.json file used by the Levridge Integration Framework to define the configuration for the integration of Agsync Workorders to FinOps.","title":"agsync Settings"},{"location":"agsyncConfigObject/#example","text":"\"agsync\": { \"MustUseWktProcessor\": true, \"ConnectionString\": \"Endpoint=[Customer Servicebus Connection String]\", \"TopicName\": \"[Customer Servicebus Topic]\", \"SubscriptionName\": \"[Customer Servicebus Topic Subscription]\", \"PrefetchCount\": 5, \"RequiresSession\": [true/false], \"RedirectUri\": \"[Agsync Integration Base URL]/api/AgsyncAuth\", \"TokenUrl\": \"https://auth.agsync.com/core/connect/token\", \"AuthorizeUrl\": \"https://auth.agsync.com/core/connect/authorize\", \"baseUri\": \"https://fields.agsync.com/api/\", \"rejectUri\": \"https://orders.agsync.com/api/orders/\", \"ClientId\": \"[Agsync assigned client ID]\", \"ClientPass\": \"[Agsync assigned client password]\", \"VaultURL\": \"[Integration Framework Key Vault URL]\", \"AgSyncTokenKey\": \"[Vault Key Used for Access Token]\" \"WktUrl\": \"[Agsync Integration Base URL]/api/WktProcessor\", \"ProcessStatuses\": \"Plan,Released,Canceled,Scheduled,Rejected,Completed,Accepted\", \"MustUseCustomerSite\": true }","title":"Example"},{"location":"agsyncConfigObject/#definition","text":"","title":"Definition"},{"location":"agsyncConfigObject/#mustusewktprocessor","text":"The MustUseWktProcessor attribute contains a bollean that specifies whether or not to process the WKT sent in the workorder from Agsync to FinOps.","title":"MustUseWktProcessor"},{"location":"agsyncConfigObject/#connectionstring","text":"","title":"ConnectionString"},{"location":"agsyncConfigObject/#topicname","text":"","title":"TopicName"},{"location":"agsyncConfigObject/#subscriptionname","text":"","title":"SubscriptionName"},{"location":"agsyncConfigObject/#prefetchcount","text":"","title":"PrefetchCount"},{"location":"agsyncConfigObject/#requiressession","text":"","title":"RequiresSession"},{"location":"agsyncConfigObject/#redirecturi","text":"","title":"RedirectUri"},{"location":"agsyncConfigObject/#tokenurl","text":"","title":"TokenUrl"},{"location":"agsyncConfigObject/#authorizeurl","text":"","title":"AuthorizeUrl"},{"location":"agsyncConfigObject/#baseuri","text":"","title":"baseUri"},{"location":"agsyncConfigObject/#rejecturi","text":"","title":"rejectUri"},{"location":"agsyncConfigObject/#clientid","text":"","title":"ClientId"},{"location":"agsyncConfigObject/#clientpass","text":"","title":"ClientPass"},{"location":"agsyncConfigObject/#vaulturl","text":"","title":"VaultURL"},{"location":"agsyncConfigObject/#agsynctokenkey","text":"","title":"AgSyncTokenKey"},{"location":"agsyncConfigObject/#processstatuses","text":"Not Required Place a comma delimited list of statuses that should be processed by the controller. It is important to make sure the capitalization is correct (Title Case) because it does an exact match. Deafult: \"Planned,Released,Canceled,Scheduled,Rejected,Completed,Accepted\"","title":"ProcessStatuses"},{"location":"agsyncConfigObject/#mustusecustomersite","text":"Not Required Set this value to true to require a valid customer site on work orders. Set this value to false to ignore the customer site on work orders. Default: true","title":"MustUseCustomerSite"},{"location":"appsettings.json/","text":"appsettings.json The appsettings.json is a configuration file that is used in standard Microsoft Configuration for ASP.NET Core to provide application settings. This document explains the various json objects used in the appsettings.json file. Overview AzureAd The AzureAd section provides the necesary information for authentication. see AzureAd Settings Controllers The controllers section provides a list of json objects that define which controllers to load for the current Levridge Integration Framework instance. See Controllers Settings Logging The Levridge Integration Framework utilizes the standard ASP.NET Core logging capabilities. The Logging section defines the logging settings for the various logging providers being used. See Logging Settings InstanceConfig The InstanceConfig section is a json object used by the Levridge Integration Framework to define the configuration for the Current Instance of the integration framework. See InstanceConfig Settings SourceConfig The SourceConfig section is a json object used by the Levridge Integration Framework to define the configuration for the Source of an integration interaction. See SourceConfig Settings TargetConfig The TargetConfig section is a json object used by the Levridge Integration Framework to define the configuration for the Target of an integration interaction. See TargetConfig Settings","title":"appsettings.json"},{"location":"appsettings.json/#appsettingsjson","text":"The appsettings.json is a configuration file that is used in standard Microsoft Configuration for ASP.NET Core to provide application settings. This document explains the various json objects used in the appsettings.json file.","title":"appsettings.json"},{"location":"appsettings.json/#overview","text":"","title":"Overview"},{"location":"appsettings.json/#azuread","text":"The AzureAd section provides the necesary information for authentication. see AzureAd Settings","title":"AzureAd"},{"location":"appsettings.json/#controllers","text":"The controllers section provides a list of json objects that define which controllers to load for the current Levridge Integration Framework instance. See Controllers Settings","title":"Controllers"},{"location":"appsettings.json/#logging","text":"The Levridge Integration Framework utilizes the standard ASP.NET Core logging capabilities. The Logging section defines the logging settings for the various logging providers being used. See Logging Settings","title":"Logging"},{"location":"appsettings.json/#instanceconfig","text":"The InstanceConfig section is a json object used by the Levridge Integration Framework to define the configuration for the Current Instance of the integration framework. See InstanceConfig Settings","title":"InstanceConfig"},{"location":"appsettings.json/#sourceconfig","text":"The SourceConfig section is a json object used by the Levridge Integration Framework to define the configuration for the Source of an integration interaction. See SourceConfig Settings","title":"SourceConfig"},{"location":"appsettings.json/#targetconfig","text":"The TargetConfig section is a json object used by the Levridge Integration Framework to define the configuration for the Target of an integration interaction. See TargetConfig Settings","title":"TargetConfig"},{"location":"command-line-arguments/","text":"Introduction Brief introduction of the module, component or feature being documented. This document explains ... Overview Main Point 1 Sub Point 1.1","title":"Introduction"},{"location":"command-line-arguments/#introduction","text":"Brief introduction of the module, component or feature being documented. This document explains ...","title":"Introduction"},{"location":"command-line-arguments/#overview","text":"","title":"Overview"},{"location":"command-line-arguments/#main-point-1","text":"","title":"Main Point 1"},{"location":"command-line-arguments/#sub-point-11","text":"","title":"Sub Point 1.1"},{"location":"hostsettings.json/","text":"Introduction Brief introduction of the module, component or feature being documented. This document explains ... Overview Main Point 1 Sub Point 1.1","title":"Introduction"},{"location":"hostsettings.json/#introduction","text":"Brief introduction of the module, component or feature being documented. This document explains ...","title":"Introduction"},{"location":"hostsettings.json/#overview","text":"","title":"Overview"},{"location":"hostsettings.json/#main-point-1","text":"","title":"Main Point 1"},{"location":"hostsettings.json/#sub-point-11","text":"","title":"Sub Point 1.1"},{"location":"how-to-install-levridge-scale/","text":"How to Install Levridge Scale Prerequisites Windows user account needs to have local admin rights and a password PC needs to have an internet connection Disable any security add-ons (Bitdefender, etc.) Whenever prompted to use a browser, use Chrome Install SqlExpress using the 'Basic' installation SQL is located in the Master Scale App Install folder It was downloaded from https://go.microsoft.com/fwlink/?linkid=866658 The SQLEXPRESS connection string will default to Server=localhost\\SQLEXPRESS;Database=master;Trusted_Connection=True; If IIS is not enabled, enable IIS through Control panel>Programs and Features>Turn Windows feature on or off>Internet Information Services. Check the root box for Internet Information Services and let is select the defaults. Ensure everything under IIS>World Wide Web Services>Application Development Features is checked. If you receive an error, when downloading IIS you may need to make a change in the registry: Computer\\HKEY_LOCAL_MACHINE\\SOFTWARE\\Policies\\Microsoft\\Windows\\WindowsUpdate\\AU Set \"UseWUServer\" to 0 Install Levridge Scale Run the Levridge Scale House Install.exe Right click and Run as Administrator Agree to terms and click Install When the installer displays \"Installation Successfully Completed\", click Close Configure Settings reference the 2. Levridge Scale Appsettings document 1. Open file explorer and go to C:\\Program Files (x86)\\Levridge\\LevridgeScaleHouse\\Servers 2. In the API and Client folders, configure appsettings.json 3. Open file explorer and go to C:\\Program Files (x86)\\Levridge\\LevridgeScaleHouse\\Services 4. In AxToScaleIntegration and HardwareInterface folders, configure appsettings.json Log into Services Open IIS In Application Pools, right click DefaultAppPool and click Stop On the left under Sites, rick click Default Web Site>Manager Website>Stop On the left select Application Pools, on the right do the follow steps for BOTH LevScaleAPI and LevPrint Right click>Advanced settings In the Identity field select Custom account>Set and enter username and password. Use the username and password. Open (Windows) Services There should be 4 new services installed: LevAxToScaleService LevHardwareService LevPrinterService LevScaleClient Set the Startup Type to \"Automatic (Delayed Start)\" On the LevHardwareService and LevScaleClient services, change the Log on for EACH service: Right click on the service>Properties Select the Log On tab Select account and password Click Apply and OK Restart the machine. When it restarts, the services will start automatically and the database will be created. Access Levridge Scale In a web browser, go to http://localhost to access Levridge Scale Sync data from F&O In F&O, Create an Event Frameowrk endpoint definition for scale Service Bus endpoint: Endpoint:=sb://example.servicebus.windows.net/;SharedAccessKeyName=RootManageSharedAccessKey;SharedAccessKey=xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx= Enter the Topic name Endpoint type: Topic Session Required: Yes Enabled: Yes Create an Event Framework event and sync all scale entities, referenced in Scale Integration","title":"How to install levridge scale"},{"location":"how-to-install-levridge-scale/#how-to-install-levridge-scale","text":"","title":"How to Install Levridge Scale"},{"location":"how-to-install-levridge-scale/#prerequisites","text":"Windows user account needs to have local admin rights and a password PC needs to have an internet connection Disable any security add-ons (Bitdefender, etc.) Whenever prompted to use a browser, use Chrome Install SqlExpress using the 'Basic' installation SQL is located in the Master Scale App Install folder It was downloaded from https://go.microsoft.com/fwlink/?linkid=866658 The SQLEXPRESS connection string will default to Server=localhost\\SQLEXPRESS;Database=master;Trusted_Connection=True; If IIS is not enabled, enable IIS through Control panel>Programs and Features>Turn Windows feature on or off>Internet Information Services. Check the root box for Internet Information Services and let is select the defaults. Ensure everything under IIS>World Wide Web Services>Application Development Features is checked. If you receive an error, when downloading IIS you may need to make a change in the registry: Computer\\HKEY_LOCAL_MACHINE\\SOFTWARE\\Policies\\Microsoft\\Windows\\WindowsUpdate\\AU Set \"UseWUServer\" to 0","title":"Prerequisites"},{"location":"how-to-install-levridge-scale/#install-levridge-scale","text":"Run the Levridge Scale House Install.exe Right click and Run as Administrator Agree to terms and click Install When the installer displays \"Installation Successfully Completed\", click Close","title":"Install Levridge Scale"},{"location":"how-to-install-levridge-scale/#configure-settings","text":"reference the 2. Levridge Scale Appsettings document 1. Open file explorer and go to C:\\Program Files (x86)\\Levridge\\LevridgeScaleHouse\\Servers 2. In the API and Client folders, configure appsettings.json 3. Open file explorer and go to C:\\Program Files (x86)\\Levridge\\LevridgeScaleHouse\\Services 4. In AxToScaleIntegration and HardwareInterface folders, configure appsettings.json","title":"Configure Settings"},{"location":"how-to-install-levridge-scale/#log-into-services","text":"Open IIS In Application Pools, right click DefaultAppPool and click Stop On the left under Sites, rick click Default Web Site>Manager Website>Stop On the left select Application Pools, on the right do the follow steps for BOTH LevScaleAPI and LevPrint Right click>Advanced settings In the Identity field select Custom account>Set and enter username and password. Use the username and password. Open (Windows) Services There should be 4 new services installed: LevAxToScaleService LevHardwareService LevPrinterService LevScaleClient Set the Startup Type to \"Automatic (Delayed Start)\" On the LevHardwareService and LevScaleClient services, change the Log on for EACH service: Right click on the service>Properties Select the Log On tab Select account and password Click Apply and OK Restart the machine. When it restarts, the services will start automatically and the database will be created.","title":"Log into Services"},{"location":"how-to-install-levridge-scale/#access-levridge-scale","text":"In a web browser, go to http://localhost to access Levridge Scale","title":"Access Levridge Scale"},{"location":"how-to-install-levridge-scale/#sync-data-from-fo","text":"In F&O, Create an Event Frameowrk endpoint definition for scale Service Bus endpoint: Endpoint:=sb://example.servicebus.windows.net/;SharedAccessKeyName=RootManageSharedAccessKey;SharedAccessKey=xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx= Enter the Topic name Endpoint type: Topic Session Required: Yes Enabled: Yes Create an Event Framework event and sync all scale entities, referenced in Scale Integration","title":"Sync data from F&amp;O"},{"location":"oneWeigh/","text":"oneWeigh","title":"oneWeigh"},{"location":"oneWeigh/#oneweigh","text":"","title":"oneWeigh"},{"location":"print-service/","text":"Print Service Installing Print Service Application To first install print service application, open Visual Studio. In Visual Studio, the print service is checked-in at \"$/Levridge/CRM/Main/PrintService\". Double click this and map it to a path on your machine. In Visual Studio, click Build > Build Solution. Activate the service as detailed below in PowerShell: Right click on the windows icon and select \"Windows PowerShell (Admin)\" Enter the following command once. Be sure to substitute the local path! New-Service-Name\"PrintService\"-\"{Your Local path to project}\\PrintService\\bin\\Debug\\PrintService.exe-k netsvcs\" Locating Printer Network Name If a printer is a network device, its name will be more difficult to locate. Use these steps to find the name that the Print Service application will use to print: Go to control panel and locate the setting titled \"Devices and Printers\" (Note: Not all setting are in this screenshot. This view is set to \"Small icons\") Navigate to the list of print options on your computer and select the device of your choice. The full network name that is required for the code is listed once you select the printer. (Highlighted in yellow below) Using Print Service Application After initial setup, use one of the following two options to start the Application. PowerShell Following the first setup, you can enter the following command into PowerShell instead. Start-Service-Name\"PrintService\" Services App. Use Windows+R to select \"services.msc\" to open Services Management Console. Right click on PrintService and select \"start\" Setting Up a New Printer To set up a new printer, new parameters will need to be setup. To add the parameters, open the app.config file in the project and add the following: ServiceBusConnectionString ServiceBusTopicSubscription ServiceBusTopic PrinterName SiteID Two parameters will also need to be updated. These two need to be changed for the printer that is being printed to. The Site ID is the customer site code from CRM. For debugging the Service, we can use the Windows Event Viewer. Go to Administrative tools in the Control Panel and Open Event Viewer. Additional Information and Documentation In CRM on the sales ordered details header, there is an Auto Print Field. Set to Yes and click save. The sales order will then print.","title":"Print Service"},{"location":"print-service/#print-service","text":"","title":"Print Service"},{"location":"print-service/#installing-print-service-application","text":"To first install print service application, open Visual Studio. In Visual Studio, the print service is checked-in at \"$/Levridge/CRM/Main/PrintService\". Double click this and map it to a path on your machine. In Visual Studio, click Build > Build Solution. Activate the service as detailed below in PowerShell: Right click on the windows icon and select \"Windows PowerShell (Admin)\" Enter the following command once. Be sure to substitute the local path! New-Service-Name\"PrintService\"-\"{Your Local path to project}\\PrintService\\bin\\Debug\\PrintService.exe-k netsvcs\"","title":"Installing Print Service Application"},{"location":"print-service/#locating-printer-network-name","text":"If a printer is a network device, its name will be more difficult to locate. Use these steps to find the name that the Print Service application will use to print: Go to control panel and locate the setting titled \"Devices and Printers\" (Note: Not all setting are in this screenshot. This view is set to \"Small icons\") Navigate to the list of print options on your computer and select the device of your choice. The full network name that is required for the code is listed once you select the printer. (Highlighted in yellow below)","title":"Locating Printer Network Name"},{"location":"print-service/#using-print-service-application","text":"After initial setup, use one of the following two options to start the Application. PowerShell Following the first setup, you can enter the following command into PowerShell instead. Start-Service-Name\"PrintService\" Services App. Use Windows+R to select \"services.msc\" to open Services Management Console. Right click on PrintService and select \"start\"","title":"Using Print Service Application"},{"location":"print-service/#setting-up-a-new-printer","text":"To set up a new printer, new parameters will need to be setup. To add the parameters, open the app.config file in the project and add the following: ServiceBusConnectionString ServiceBusTopicSubscription ServiceBusTopic PrinterName SiteID Two parameters will also need to be updated. These two need to be changed for the printer that is being printed to. The Site ID is the customer site code from CRM. For debugging the Service, we can use the Windows Event Viewer. Go to Administrative tools in the Control Panel and Open Event Viewer.","title":"Setting Up a New Printer"},{"location":"print-service/#additional-information-and-documentation","text":"In CRM on the sales ordered details header, there is an Auto Print Field. Set to Yes and click save. The sales order will then print.","title":"Additional Information and Documentation"},{"location":"scale-appsettings-configuration/","text":"Scale Appsettings Configuration Configure Appsettings Files Configure the appsettings.json file in the following default locations: C:\\Program Files (x86)\\Levridge\\LevridgeScaleHouse\\Servers\\LevScaleClient C:\\Program Files (x86)\\Levridge\\LevridgeScaleHouse\\Servers\\LevScaleAPI C:\\Program Files (x86)\\Levridge\\LevridgeScaleHouse\\Services\\AxToScaleIntegration C:\\Program Files (x86)\\Levridge\\LevridgeScaleHouse\\Services\\HardwareInterface LevScaleClient { \"GeneratedDb\": \"C:\\\\LevridgeScaleHouse\\\\sql\", \"ConnectionStrings\": { \"DefaultConnection\": \"Data Source=.\\\\SQLEXPRESS;Initial Catalog=LevScale;Trusted_Connection=True;MultipleActiveResultSets=true;\" }, \"WipProcessInterval\": 10000, \"WipCleanupInterval\": 86400000, \"Logging\": { \"IncludeScopes\": false, \"LogLevel\": { \"Default\": \"Warning\" } }, \"ServiceBusSettings\": { \"ServiceBusConnectionString\": //Enter the endpoint for the service bus for integration from Levridge Scale to F&O \"Endpoint=sb://integration.servicebus.windows.net/;SharedAccessKeyName=RootManageSharedAccessKey;SharedAccessKey=PG5SCOfZHIKQ7rmRmSdKwGe2OW27hRhxxxxxx=\", //Enter the Topic name for the service bus \"TopicName\": \"scaletofo\" }, \"WebAPI\": { \"Port\": 80, \"URL\": \"http://localhost:8080\", }, \"Printer\": { \"DataSource\": \"Server=.;Initial Catalog=LevScale;Trusted_Connection=True;MultipleActiveResultSets=true;\", \"ReportPath\": \"./\" }, \"ScaleInterface\": { \"UriString\": \"http://localhost:44323/\", \"TLSVersion\": \"\" }, \"LogLocationCleanup\": { /// <summary> /// The location of the log folder. /// </summary> /// <remarks>The default value is 'C:\\Temp'.</remarks> \"LogFolder\": \"C:\\\\Temp\", /// <summary> /// The time to wait (in minutes) before clearing old logs from the reporting location. /// </summary> /// <remarks>The default is 1,440 mintues (24 hours).</remarks> \"RecheckInterval\": 2880, /// <summary> /// Log files may be deleted that are older than this many mintues.. /// </summary> /// <remarks>The default is 2,880 minutes (24 hours).</remarks> \"RemoveAfter\": 2880, /// <summary> /// The file filter used when clearing old report files. /// </summary> /// <remarks>The default is \"*\".</remarks> \"FileFilter\": \"*.txt\", /// <summary> /// The file search options. /// </summary> /// <remarks>The default is <see cref=\"SearchOption.TopDirectoryOnly\"/></remarks> \"FileSearchOptions\": \"TopDirectoryOnly\" }, \"Log4Net\": { \"xml\": \"log4net.xml\" }, \"DynamicsAX\": { //Enter the connection details for F&O \"UriString\": \"https://landusuat.sandbox.operations.dynamics.com/\", \"ActiveDirectoryResource\": \"https://landusuat.sandbox.operations.dynamics.com\", \"ActiveDirectoryTenant\": \"https://login.microsoftonline.com/bb671371-5d5c-4dca-9c53-9376bxxxx\", \"ActiveDirectoryClientAppId\": \"ef8c69c1-0d35-40eb-aec9-b393xxxx\", \"ActiveDirectoryClientAppSecret\": \"4ZpHiXn0k2Nx9vnsgmJq+uYbT4drRL1cJLcxxxx=\", \"TLSVersion\": \"\", \"ODataEntityPath\": \"https://landusuat.sandbox.operations.dynamics.com/data/\", \"DataAreaId\": \"100\" } } LevScaleAPI { \"AllowedHosts\": \"*\", \"GeneratedDb\": \"C:\\\\LevridgeScaleHouse\\\\sql\", \"ConnectionStrings\": { \"DefaultConnection\": \"Data Source=.\\\\SQLEXPRESS;Initial Catalog=LevScale;Trusted_Connection=True;MultipleActiveResultSets=true;\" }, \"WipProcessInterval\": 10000, \"WipCleanupInterval\": 86400000, \"Logging\": { \"IncludeScopes\": false, \"LogLevel\": { \"Default\": \"Warning\" } }, \"ServiceBusSettings\": { \"ServiceBusConnectionString\": //Enter the endpoint for the service bus for integration from Levridge Scale to F&O \"Endpoint=sb://integration.servicebus.windows.net/;SharedAccessKeyName=RootManageSharedAccessKey;SharedAccessKey=PG5SCOfZHIKQ7rmRmSdKwGe2OW27hRhxxxxxx=\", //Enter the Topic name for the service bus \"TopicName\": \"scaletofo\" }, \"WebAPI\": { \"Port\": 80, \"URL\": \"http://localhost:8080\", }, \"Printer\": { \"DataSource\": \"Server=.;Initial Catalog=LevScale;Trusted_Connection=True;MultipleActiveResultSets=true;\", \"ReportPath\": \"./\" }, \"ScaleInterface\": { \"UriString\": \"http://localhost:44323/\", \"TLSVersion\": \"\" }, \"Log4Net\": { \"xml\": \"log4net.xml\" } } AxToScaleIntegration { \"Logging\": { \"ApplicationInsights\": { \"LogLevel\": { \"Default\": \"Trace\" } }, \"Debug\": { \"LogLevel\": { \"Default\": \"Trace\" } }, \"Console\": { \"IncludeScopes\": false, \"LogLevel\": { \"Default\": \"Debug\" } }, \"LogLevel\": { \"Default\": \"Information\" } }, \"ApplicationInsights\": { \"InstrumentationKey\": \"\" }, \"InstanceConfig\": { \"AzureTableConfiguration\": \"AzureTableConfigurationAX\" }, \"SourceConfig\": { \"ServiceBusConfigName\": \"AxToScale\", \"ODataConfigName\": \"DynamicsAX\", \"SystemName\": \"DynamicsAX\", \"Direction\": \"Source\" }, \"TargetConfig\": { \"ODataConfigName\": \"ScaleOData\", \"SystemName\": \"ScaleHouse\", \"Direction\": \"Target\" }, //Enter the connection details for F&O \"DynamicsAX\": { \"UriString\": \"https://landusuat.sandbox.operations.dynamics.com/\", \"ActiveDirectoryResource\": \"https://landusuat.sandbox.operations.dynamics.com\", \"ActiveDirectoryTenant\": \"https://login.microsoftonline.com/bb671371-5d5c-4dca-9c53xxxxxxx\", \"ActiveDirectoryClientAppId\": \"ef8c69c1-0d35-40eb-aec9-b39381xxxxx\", \"ActiveDirectoryClientAppSecret\": \"4ZpHiXn0k2Nx9vnsgmJq+uYbT4drRL1xxxxxI=\", \"TLSVersion\": \"\", \"ODataEntityPath\": \"https://landusuat.sandbox.operations.dynamics.com/data/\", \"DataAreaId\": \"100\" }, //Enter the Active Directory details for F&O. Leave the default values for UriString and ODataEntityPath \"ScaleOData\": { \"UriString\": \"http://localhost:8080/odata\", \"ActiveDirectoryResource\": \"https://landusuat.sandbox.operations.dynamics.com\", \"ActiveDirectoryTenant\": \"https://login.microsoftonline.com/bb671371-5d5c-4dca-9c53-9376xxxxx\", \"ActiveDirectoryClientAppId\": \"ef8c69c1-0d35-40eb-aec9-b3xxxxx\", \"ActiveDirectoryClientAppSecret\": \"4ZpHiXn0k2Nx9vnsgmJq+uYbT4drRL1cJLcxxxxx=\", \"TLSVersion\": \"\", \"ODataEntityPath\": \"http://localhost:8080/odata\" }, \"AxToScale\": { \"ConnectionString\": //Enter the endpoint for the service bus for integration from F&O to Levridge Scale \"Endpoint=sb://integration.servicebus.windows.net/;SharedAccessKeyName=RootManageSharedAccessKey;SharedAccessKey=PG5SCOfZHIKQ7rmRmSdKwGe2OW27hRhxxxxxx=\", //Enter the Topic name for the service bus \"TopicName\": \"axtoscale\", //Enter the Subscription name for the service bus \"SubscriptionName\": \"000\", \"PrefetchCount\": 40, \"MaxConcurrentCount\": 20 } } HardwareInterface { \"ConnectionStrings\": { \"DefaultConnection\": \"Data Source=.\\\\SQLEXPRESS;Initial Catalog=LevScale;Trusted_Connection=True;\" }, \"Logging\": { \"LogLevel\": { \"Default\": \"Warning\" } }, \"AllowedHosts\": \"*\", \"Hardware\": { \"HubUrl\": \"http://localhost:80/scalehub\", \"WorkstationName\" : \"Ws1\" } }","title":"Scale Appsettings Configuration"},{"location":"scale-appsettings-configuration/#scale-appsettings-configuration","text":"","title":"Scale Appsettings Configuration"},{"location":"scale-appsettings-configuration/#configure-appsettings-files","text":"Configure the appsettings.json file in the following default locations: C:\\Program Files (x86)\\Levridge\\LevridgeScaleHouse\\Servers\\LevScaleClient C:\\Program Files (x86)\\Levridge\\LevridgeScaleHouse\\Servers\\LevScaleAPI C:\\Program Files (x86)\\Levridge\\LevridgeScaleHouse\\Services\\AxToScaleIntegration C:\\Program Files (x86)\\Levridge\\LevridgeScaleHouse\\Services\\HardwareInterface","title":"Configure Appsettings Files"},{"location":"scale-appsettings-configuration/#levscaleclient","text":"{ \"GeneratedDb\": \"C:\\\\LevridgeScaleHouse\\\\sql\", \"ConnectionStrings\": { \"DefaultConnection\": \"Data Source=.\\\\SQLEXPRESS;Initial Catalog=LevScale;Trusted_Connection=True;MultipleActiveResultSets=true;\" }, \"WipProcessInterval\": 10000, \"WipCleanupInterval\": 86400000, \"Logging\": { \"IncludeScopes\": false, \"LogLevel\": { \"Default\": \"Warning\" } }, \"ServiceBusSettings\": { \"ServiceBusConnectionString\": //Enter the endpoint for the service bus for integration from Levridge Scale to F&O \"Endpoint=sb://integration.servicebus.windows.net/;SharedAccessKeyName=RootManageSharedAccessKey;SharedAccessKey=PG5SCOfZHIKQ7rmRmSdKwGe2OW27hRhxxxxxx=\", //Enter the Topic name for the service bus \"TopicName\": \"scaletofo\" }, \"WebAPI\": { \"Port\": 80, \"URL\": \"http://localhost:8080\", }, \"Printer\": { \"DataSource\": \"Server=.;Initial Catalog=LevScale;Trusted_Connection=True;MultipleActiveResultSets=true;\", \"ReportPath\": \"./\" }, \"ScaleInterface\": { \"UriString\": \"http://localhost:44323/\", \"TLSVersion\": \"\" }, \"LogLocationCleanup\": { /// <summary> /// The location of the log folder. /// </summary> /// <remarks>The default value is 'C:\\Temp'.</remarks> \"LogFolder\": \"C:\\\\Temp\", /// <summary> /// The time to wait (in minutes) before clearing old logs from the reporting location. /// </summary> /// <remarks>The default is 1,440 mintues (24 hours).</remarks> \"RecheckInterval\": 2880, /// <summary> /// Log files may be deleted that are older than this many mintues.. /// </summary> /// <remarks>The default is 2,880 minutes (24 hours).</remarks> \"RemoveAfter\": 2880, /// <summary> /// The file filter used when clearing old report files. /// </summary> /// <remarks>The default is \"*\".</remarks> \"FileFilter\": \"*.txt\", /// <summary> /// The file search options. /// </summary> /// <remarks>The default is <see cref=\"SearchOption.TopDirectoryOnly\"/></remarks> \"FileSearchOptions\": \"TopDirectoryOnly\" }, \"Log4Net\": { \"xml\": \"log4net.xml\" }, \"DynamicsAX\": { //Enter the connection details for F&O \"UriString\": \"https://landusuat.sandbox.operations.dynamics.com/\", \"ActiveDirectoryResource\": \"https://landusuat.sandbox.operations.dynamics.com\", \"ActiveDirectoryTenant\": \"https://login.microsoftonline.com/bb671371-5d5c-4dca-9c53-9376bxxxx\", \"ActiveDirectoryClientAppId\": \"ef8c69c1-0d35-40eb-aec9-b393xxxx\", \"ActiveDirectoryClientAppSecret\": \"4ZpHiXn0k2Nx9vnsgmJq+uYbT4drRL1cJLcxxxx=\", \"TLSVersion\": \"\", \"ODataEntityPath\": \"https://landusuat.sandbox.operations.dynamics.com/data/\", \"DataAreaId\": \"100\" } }","title":"LevScaleClient"},{"location":"scale-appsettings-configuration/#levscaleapi","text":"{ \"AllowedHosts\": \"*\", \"GeneratedDb\": \"C:\\\\LevridgeScaleHouse\\\\sql\", \"ConnectionStrings\": { \"DefaultConnection\": \"Data Source=.\\\\SQLEXPRESS;Initial Catalog=LevScale;Trusted_Connection=True;MultipleActiveResultSets=true;\" }, \"WipProcessInterval\": 10000, \"WipCleanupInterval\": 86400000, \"Logging\": { \"IncludeScopes\": false, \"LogLevel\": { \"Default\": \"Warning\" } }, \"ServiceBusSettings\": { \"ServiceBusConnectionString\": //Enter the endpoint for the service bus for integration from Levridge Scale to F&O \"Endpoint=sb://integration.servicebus.windows.net/;SharedAccessKeyName=RootManageSharedAccessKey;SharedAccessKey=PG5SCOfZHIKQ7rmRmSdKwGe2OW27hRhxxxxxx=\", //Enter the Topic name for the service bus \"TopicName\": \"scaletofo\" }, \"WebAPI\": { \"Port\": 80, \"URL\": \"http://localhost:8080\", }, \"Printer\": { \"DataSource\": \"Server=.;Initial Catalog=LevScale;Trusted_Connection=True;MultipleActiveResultSets=true;\", \"ReportPath\": \"./\" }, \"ScaleInterface\": { \"UriString\": \"http://localhost:44323/\", \"TLSVersion\": \"\" }, \"Log4Net\": { \"xml\": \"log4net.xml\" } }","title":"LevScaleAPI"},{"location":"scale-appsettings-configuration/#axtoscaleintegration","text":"{ \"Logging\": { \"ApplicationInsights\": { \"LogLevel\": { \"Default\": \"Trace\" } }, \"Debug\": { \"LogLevel\": { \"Default\": \"Trace\" } }, \"Console\": { \"IncludeScopes\": false, \"LogLevel\": { \"Default\": \"Debug\" } }, \"LogLevel\": { \"Default\": \"Information\" } }, \"ApplicationInsights\": { \"InstrumentationKey\": \"\" }, \"InstanceConfig\": { \"AzureTableConfiguration\": \"AzureTableConfigurationAX\" }, \"SourceConfig\": { \"ServiceBusConfigName\": \"AxToScale\", \"ODataConfigName\": \"DynamicsAX\", \"SystemName\": \"DynamicsAX\", \"Direction\": \"Source\" }, \"TargetConfig\": { \"ODataConfigName\": \"ScaleOData\", \"SystemName\": \"ScaleHouse\", \"Direction\": \"Target\" }, //Enter the connection details for F&O \"DynamicsAX\": { \"UriString\": \"https://landusuat.sandbox.operations.dynamics.com/\", \"ActiveDirectoryResource\": \"https://landusuat.sandbox.operations.dynamics.com\", \"ActiveDirectoryTenant\": \"https://login.microsoftonline.com/bb671371-5d5c-4dca-9c53xxxxxxx\", \"ActiveDirectoryClientAppId\": \"ef8c69c1-0d35-40eb-aec9-b39381xxxxx\", \"ActiveDirectoryClientAppSecret\": \"4ZpHiXn0k2Nx9vnsgmJq+uYbT4drRL1xxxxxI=\", \"TLSVersion\": \"\", \"ODataEntityPath\": \"https://landusuat.sandbox.operations.dynamics.com/data/\", \"DataAreaId\": \"100\" }, //Enter the Active Directory details for F&O. Leave the default values for UriString and ODataEntityPath \"ScaleOData\": { \"UriString\": \"http://localhost:8080/odata\", \"ActiveDirectoryResource\": \"https://landusuat.sandbox.operations.dynamics.com\", \"ActiveDirectoryTenant\": \"https://login.microsoftonline.com/bb671371-5d5c-4dca-9c53-9376xxxxx\", \"ActiveDirectoryClientAppId\": \"ef8c69c1-0d35-40eb-aec9-b3xxxxx\", \"ActiveDirectoryClientAppSecret\": \"4ZpHiXn0k2Nx9vnsgmJq+uYbT4drRL1cJLcxxxxx=\", \"TLSVersion\": \"\", \"ODataEntityPath\": \"http://localhost:8080/odata\" }, \"AxToScale\": { \"ConnectionString\": //Enter the endpoint for the service bus for integration from F&O to Levridge Scale \"Endpoint=sb://integration.servicebus.windows.net/;SharedAccessKeyName=RootManageSharedAccessKey;SharedAccessKey=PG5SCOfZHIKQ7rmRmSdKwGe2OW27hRhxxxxxx=\", //Enter the Topic name for the service bus \"TopicName\": \"axtoscale\", //Enter the Subscription name for the service bus \"SubscriptionName\": \"000\", \"PrefetchCount\": 40, \"MaxConcurrentCount\": 20 } }","title":"AxToScaleIntegration"},{"location":"scale-appsettings-configuration/#hardwareinterface","text":"{ \"ConnectionStrings\": { \"DefaultConnection\": \"Data Source=.\\\\SQLEXPRESS;Initial Catalog=LevScale;Trusted_Connection=True;\" }, \"Logging\": { \"LogLevel\": { \"Default\": \"Warning\" } }, \"AllowedHosts\": \"*\", \"Hardware\": { \"HubUrl\": \"http://localhost:80/scalehub\", \"WorkstationName\" : \"Ws1\" } }","title":"HardwareInterface"},{"location":"scale-integration/","text":"Levridge Scale Integration Azure service bus explorer for Integration Down: F&O to scale Up: Scale to F&O Appsettings.json - configuration file, edit file to switch between up and down Integration F&O to scale Here is the Integration List which lists the entities needed when integrating F&O to scale through System administration > Setup > Event Framework > Event framework events. 1. System administration > Setup > Event framework > Event framework events 2. Run azure service bus Down 3. Run IntegrationConsole.cmd Integration scale to F&O Run azure service bus Up Run IntegrationConsole.cmd When Print is clicked on the scale app, the ticket will then sync to F&O. Scale Entities for Integration Agronomy Release LevUnitOfMeasureEntity HcmWorkerEntity LevScaleReleaseProductsEntity LevHazardousMaterialsTableEntity CustCustomerV3Entity LevSplitGroupEntity LevSplitGroupLinesEntity LevAccountToOperationRelationshipEntity LevCustomerOperationEntity LevCustomerSiteEntity LevRollingStockV2Entity LevFreightCarrierEntity LevInventSiteEntity LevWarehouseEntity LevScaleOperatorEntity LevScaleHouseSalesOrderEntity LevScaleHouseScaleTicketEntity: Add a filter for: Table Scale tickets; Field Ticket type; Criteria Transfer origin","title":"Scale"},{"location":"scale-integration/#levridge-scale-integration","text":"","title":"Levridge Scale Integration"},{"location":"scale-integration/#azure-service-bus-explorer-for-integration","text":"Down: F&O to scale Up: Scale to F&O Appsettings.json - configuration file, edit file to switch between up and down","title":"Azure service bus explorer for Integration"},{"location":"scale-integration/#integration-fo-to-scale","text":"Here is the Integration List which lists the entities needed when integrating F&O to scale through System administration > Setup > Event Framework > Event framework events. 1. System administration > Setup > Event framework > Event framework events 2. Run azure service bus Down 3. Run IntegrationConsole.cmd","title":"Integration F&amp;O to scale"},{"location":"scale-integration/#integration-scale-to-fo","text":"Run azure service bus Up Run IntegrationConsole.cmd When Print is clicked on the scale app, the ticket will then sync to F&O.","title":"Integration scale to F&amp;O"},{"location":"scale-integration/#scale-entities-for-integration","text":"","title":"Scale Entities for Integration"},{"location":"scale-integration/#agronomy-release","text":"LevUnitOfMeasureEntity HcmWorkerEntity LevScaleReleaseProductsEntity LevHazardousMaterialsTableEntity CustCustomerV3Entity LevSplitGroupEntity LevSplitGroupLinesEntity LevAccountToOperationRelationshipEntity LevCustomerOperationEntity LevCustomerSiteEntity LevRollingStockV2Entity LevFreightCarrierEntity LevInventSiteEntity LevWarehouseEntity LevScaleOperatorEntity LevScaleHouseSalesOrderEntity LevScaleHouseScaleTicketEntity: Add a filter for: Table Scale tickets; Field Ticket type; Criteria Transfer origin","title":"Agronomy Release"},{"location":"scale-settings-configuration/","text":"Scale Settings Configuration Settings Under Application Configuration>Settings - Site: Choose a site from the drop down to assign site ID to the scale app - Number Prefix: Enter in the number ID of the Site in FO - Scale: Leave as default value of 1 - Company Name: Enter company name - Company Location/Address: Enter in correct company address - Company Carrier: Use this to Select the transport that is set up in F&O to trasnport Transfer Tickets: Company Name - Logo: Click on this to choose a picture file: Sizing IS being updated for logo - Printer Server: Address of the Printer Server, Default value is http://localhost:8081 - Printer Service: Address of the Printer Service, Default value is: http://localhost:4343/printerservice - Report File Location: Location of where PDF versions of tickets are stored. Default value is C:\\Temp\\Reports . Will have to create this file folder on your local machine. - Organization: DataAreaID of F&O, enter in 100 - IsDriverOn: If this is toggled on, all ticket types will default to having the flag on unless manually changed by the scale operator. If set to no, \"Driver-on\" flag will not be displayed on the ticket window. - Enable Spot Pricing: Further Development of Grain Functionality required. Field does not need to be filled in for agronomy. - Enable Disposition: Further Development of Grain Functionality required. Field does not need to be filled in for agronomy. - Scale Unit: Use drop down list to select LB - Max Weight: Enter in the max weight of the scale head. - Incr. Wt: This is what increments the scale head will display weights, i.e. 20 lbs. Printer Settings Under Application Configuration>Printer Settings. This window is used to choose the correct printers for each of your scale heads at a location, and to choose a corresponding menu color to know which scale head you are working with in your window. The printer settings you choose in this window will then default in when you choose that scale on the Application start page. Note: This setup is required. The printer selection can be changed to any printer on the Start page, but default printer setup is required to be able to print scale tickets. To Set up the Printer Settings for a Scale: Choose the scale you want to set up from the drop down on the top of the page. Choose your printers for both your Tickets and BOL. Using the printer Type can change between a full page printout vs. a receipt printout. Choose the number of copies that you want to print with each completed ticket. Choose a background Color for that specific scale-head to make sure the correct scale is being used to complete and print tickets. Click save to complete these changes. Customer Short List Under Application Configuration>Customer Short list Shows a list of all customers in the scale database in alphabetical order. Checking the Box next to a customer will put that customer into the drop-down list on all the scale ticket types in order to reduce load times because not all customers will be loading in the customer account drop-down. To add a customer to the short list manually, check the box next to their name, and click submit on the bottom left corner of the page. Gross Quantity Settings Under Application Configuration>Gross Quantity Settings Manually set up in scale to show the gross quantity conversions of certain products, i.e. lbs to gallons. To create a new gross quantity for a product: 1. Click create new on the top left of the page under INDEX 2. Choose the item ID for the correct item you want to add a quantity to 3. In Factor, enter the correct number. If you were to say a gallon of product is equal to 8 pounds, you would enter 8. The conversion is lbs/Unit of measure. The units of measure come from your FO system. 4. Choose your Commodity Unit of Measure. 5. Click Create. To edit a previously created Gross quantity, click Edit next to the GQ that you want to change and when you have finished editing it, click the blue \"Edit\" button to save it. To delete a previously created GQ, click delete next to the GQ. Confirm the delete by clicking the red \"Delete\" button.","title":"Scale Settings Configuration"},{"location":"scale-settings-configuration/#scale-settings-configuration","text":"","title":"Scale Settings Configuration"},{"location":"scale-settings-configuration/#settings","text":"Under Application Configuration>Settings - Site: Choose a site from the drop down to assign site ID to the scale app - Number Prefix: Enter in the number ID of the Site in FO - Scale: Leave as default value of 1 - Company Name: Enter company name - Company Location/Address: Enter in correct company address - Company Carrier: Use this to Select the transport that is set up in F&O to trasnport Transfer Tickets: Company Name - Logo: Click on this to choose a picture file: Sizing IS being updated for logo - Printer Server: Address of the Printer Server, Default value is http://localhost:8081 - Printer Service: Address of the Printer Service, Default value is: http://localhost:4343/printerservice - Report File Location: Location of where PDF versions of tickets are stored. Default value is C:\\Temp\\Reports . Will have to create this file folder on your local machine. - Organization: DataAreaID of F&O, enter in 100 - IsDriverOn: If this is toggled on, all ticket types will default to having the flag on unless manually changed by the scale operator. If set to no, \"Driver-on\" flag will not be displayed on the ticket window. - Enable Spot Pricing: Further Development of Grain Functionality required. Field does not need to be filled in for agronomy. - Enable Disposition: Further Development of Grain Functionality required. Field does not need to be filled in for agronomy. - Scale Unit: Use drop down list to select LB - Max Weight: Enter in the max weight of the scale head. - Incr. Wt: This is what increments the scale head will display weights, i.e. 20 lbs.","title":"Settings"},{"location":"scale-settings-configuration/#printer-settings","text":"Under Application Configuration>Printer Settings. This window is used to choose the correct printers for each of your scale heads at a location, and to choose a corresponding menu color to know which scale head you are working with in your window. The printer settings you choose in this window will then default in when you choose that scale on the Application start page. Note: This setup is required. The printer selection can be changed to any printer on the Start page, but default printer setup is required to be able to print scale tickets.","title":"Printer Settings"},{"location":"scale-settings-configuration/#to-set-up-the-printer-settings-for-a-scale","text":"Choose the scale you want to set up from the drop down on the top of the page. Choose your printers for both your Tickets and BOL. Using the printer Type can change between a full page printout vs. a receipt printout. Choose the number of copies that you want to print with each completed ticket. Choose a background Color for that specific scale-head to make sure the correct scale is being used to complete and print tickets. Click save to complete these changes.","title":"To Set up the Printer Settings for a Scale:"},{"location":"scale-settings-configuration/#customer-short-list","text":"Under Application Configuration>Customer Short list Shows a list of all customers in the scale database in alphabetical order. Checking the Box next to a customer will put that customer into the drop-down list on all the scale ticket types in order to reduce load times because not all customers will be loading in the customer account drop-down. To add a customer to the short list manually, check the box next to their name, and click submit on the bottom left corner of the page.","title":"Customer Short List"},{"location":"scale-settings-configuration/#gross-quantity-settings","text":"Under Application Configuration>Gross Quantity Settings Manually set up in scale to show the gross quantity conversions of certain products, i.e. lbs to gallons. To create a new gross quantity for a product: 1. Click create new on the top left of the page under INDEX 2. Choose the item ID for the correct item you want to add a quantity to 3. In Factor, enter the correct number. If you were to say a gallon of product is equal to 8 pounds, you would enter 8. The conversion is lbs/Unit of measure. The units of measure come from your FO system. 4. Choose your Commodity Unit of Measure. 5. Click Create. To edit a previously created Gross quantity, click Edit next to the GQ that you want to change and when you have finished editing it, click the blue \"Edit\" button to save it. To delete a previously created GQ, click delete next to the GQ. Confirm the delete by clicking the red \"Delete\" button.","title":"Gross Quantity Settings"},{"location":"web.config/","text":"Web.config File Overview The web.config file only applies to Integration Framework instances that are hosted as a web application. Typically these are hosted in Azure but can also be hosted on-premise in IIS. Although Microsoft documentation claims that the web.config file has been replaced , Azure App Services still use it as the means to start the application service. It should not contain any application configuration settings. The web.config file is an XML file that contains a element. Web.Config Elements \\ \\ This is the main element that contains the other elements \\ \\<\\location> This element contains the relative path for the web server \\ \\<\\system.webserver> Contains the handlers and aspNetCore elements \\ \\<\\handlers> Defines the element for the web server. \\<aspNetCore\\> Defines the path to the web server assembly and standard output logging information. Set stdoutLogEnable to \"true\" to write standard out logging to files in the relative location pointed to by the stdoutLogFile attribute. To specify command-line attributes add them to the processPath attribuute on the aspNetCore element. For example, to specify the name for the InstanceConfig node change the processPath to look like this: processPath=\".\\Levridge.Integration.Host.exe -i=MyInstanceConfig\" Sample <?xml version=\"1.0\" encoding=\"utf-8\"?> <configuration> <location path=\".\" inheritInChildApplications=\"false\"> <system.webServer> <handlers> <add name=\"aspNetCore\" path=\"*\" verb=\"*\" modules=\"AspNetCoreModuleV2\" resourceType=\"Unspecified\" /> </handlers> <aspNetCore processPath=\".\\Levridge.Integration.Host.exe\" stdoutLogEnabled=\"false\" stdoutLogFile=\".\\logs\\stdout\" /> </system.webServer> </location> </configuration>","title":"Web.config File"},{"location":"web.config/#webconfig-file","text":"","title":"Web.config File"},{"location":"web.config/#overview","text":"The web.config file only applies to Integration Framework instances that are hosted as a web application. Typically these are hosted in Azure but can also be hosted on-premise in IIS. Although Microsoft documentation claims that the web.config file has been replaced , Azure App Services still use it as the means to start the application service. It should not contain any application configuration settings. The web.config file is an XML file that contains a element.","title":"Overview"},{"location":"web.config/#webconfig-elements","text":"","title":"Web.Config Elements"},{"location":"web.config/#_1","text":"This is the main element that contains the other elements","title":"\\\\"},{"location":"web.config/#location","text":"This element contains the relative path for the web server","title":"\\\\&lt;\\location&gt;"},{"location":"web.config/#systemwebserver","text":"Contains the handlers and aspNetCore elements","title":"\\\\&lt;\\system.webserver&gt;"},{"location":"web.config/#handlers","text":"Defines the element for the web server.","title":"\\\\&lt;\\handlers&gt;"},{"location":"web.config/#aspnetcore","text":"Defines the path to the web server assembly and standard output logging information. Set stdoutLogEnable to \"true\" to write standard out logging to files in the relative location pointed to by the stdoutLogFile attribute. To specify command-line attributes add them to the processPath attribuute on the aspNetCore element. For example, to specify the name for the InstanceConfig node change the processPath to look like this: processPath=\".\\Levridge.Integration.Host.exe -i=MyInstanceConfig\"","title":"\\&lt;aspNetCore\\>"},{"location":"web.config/#sample","text":"<?xml version=\"1.0\" encoding=\"utf-8\"?> <configuration> <location path=\".\" inheritInChildApplications=\"false\"> <system.webServer> <handlers> <add name=\"aspNetCore\" path=\"*\" verb=\"*\" modules=\"AspNetCoreModuleV2\" resourceType=\"Unspecified\" /> </handlers> <aspNetCore processPath=\".\\Levridge.Integration.Host.exe\" stdoutLogEnabled=\"false\" stdoutLogFile=\".\\logs\\stdout\" /> </system.webServer> </location> </configuration>","title":"Sample"}]}